{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, Input\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from collections import namedtuple\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import datetime, time\n",
    "import os, sys\n",
    "import tqdm\n",
    "import gc\n",
    "from random import gauss\n",
    "from multiprocessing import Process\n",
    "Point = namedtuple('Point', ('x', 'y'))\n",
    "Circle = namedtuple('Circle', ('r'))\n",
    "Square = namedtuple('Square', ('side'))\n",
    "Rectangle = namedtuple('Rectangle', ('length', 'width'))\n",
    "PointWithDistance = namedtuple('PointWithDistance', ('p', 'dist'))\n",
    "float_memory_used = 'float16'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INIT\n",
    "# PART 1\n",
    "# number_samples = [5] + list(range(10, 101, 10)) + [120, 150, 200, 250, 300, 400, 500, 700] + list(range(1000, 10001, 1000))\n",
    "number_samples = [8192] \n",
    "# number_samples = [4096, 4915, 5734, 6554, 7373, 8192]\n",
    "\n",
    "# cnn_type = \"classification\"  # {\"classification\", \"regression\"}\n",
    "validation_size, noise_floor = 0.33, -95.0#-110.0\n",
    "max_x, max_y, number_image_channels = 100, 100, 1\n",
    "pic_cell_size, cell_size = 1, 10\n",
    "ss_shape = 'point' # shape = {'circle', 'square', 'point'} \n",
    "ss_param = None #ss_param overrides ss_shape if exists. make it None if you want to use ss_shape\n",
    "style = \"raw_power_min_max_norm\"  # {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "intensity_degradation, slope, IS_SOLID = 'log', 5, False  # 'log', 'linear', slope 3 for 1000, 5 for 100\n",
    "max_txs_num = 4\n",
    "propagation_model = 'log' # 'splat', 'log', 'testbed'\n",
    "noise, std = True, 1 # False for splat\n",
    "    \n",
    "STATIC_SENSORS = True\n",
    "sensors_num = 900\n",
    "if STATIC_SENSORS:\n",
    "    sensors_file_path = \"../../java_workspace/research/commons/resources/sensors/square\" \\\n",
    "    + str(max(max_x, max_y)) + \"/\" + str(sensors_num) + \"/sensors.txt\"\n",
    "# num_pus = (data_reg.shape[1] - 3)//3\n",
    "\n",
    "# PART 2\n",
    "number_of_proccessors = 6\n",
    "memory_size_allowed = 4 # in Gigabyte\n",
    "float_size = 0\n",
    "if float_memory_used == \"float16\":\n",
    "    float_size = 16\n",
    "elif float_memory_used == \"float\" or \"float32\":\n",
    "    float_size = 32\n",
    "elif float_memory_used == \"float8\":\n",
    "    float_size = 8\n",
    "\n",
    "dtime = datetime.datetime.now().strftime('_%Y%m_%d%H_%M')\n",
    "color = \"color\" if number_image_channels > 1 else \"gray\"\n",
    "image_dir = 'models/pictures_' + str(max_x) + '_' + str(max_y) + '/' + propagation_model + (\n",
    "    \"/noisy_std_\" + str(std) if noise else \"\") + \"/\" + (\"static_\" if STATIC_SENSORS else \"dynamic_\"\n",
    "                                                       ) + ss_shape + '_sensors' + \"/\" + style + \"/\" + color + (\n",
    "    \"\" if ss_shape == 'point' else (\"/\" + intensity_degradation + '_' + str(slope))) + \"/\" + str(sensors_num) + \"sensors\" + \"/images\"\n",
    "\n",
    "if not os.path.exists(image_dir):\n",
    "        os.makedirs(image_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/images'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD DATA\n",
    "num_columns = (sensors_num if STATIC_SENSORS else sensors_num * 3 + 1) + max_txs_num * 3 + 1 + 3\n",
    "cols = [i for i in range(num_columns)]\n",
    "dataset_name = \"STRONGEST_FIXEDPOWERS_150000_min0_max4TXs_900sensor_square100grid_log_alpha3.5_noisy_std1.0_2020_11_21_12_40.txt\"\n",
    "with open('/'.join(image_dir.split('/')[:-1]) + '/datasets' + dtime + '.txt', 'w') as set_file:\n",
    "    set_file.write(dataset_name)\n",
    "\n",
    "dataframe = pd.read_csv('../../java_workspace/research/localization/resources/data/'\n",
    "                        + dataset_name, delimiter=',', header=None, names=cols, dtype=np.float64)\n",
    "\n",
    "dataframe.reset_index(drop=True, inplace=True)\n",
    "\n",
    "\n",
    "data_reg = dataframe.values\n",
    "# data_reg[data_reg == '-Infinity'] = -float('inf')\n",
    "data_reg[data_reg < noise_floor] = noise_floor\n",
    "# data_reg = np.concatenate((dataframe_tot.values[:, 0:dataframe_tot.shape[1]-3], \n",
    "#                            dataframe_tot.values[:, dataframe_tot.shape[1]-1:dataframe_tot.shape[1]]), axis=1)\n",
    "# data_class = dataframe_tot.values[:, 0:dataframe_tot.shape[1]-1]\n",
    "# y_class_power = dataframe_tot.values[:, -1]\n",
    "\n",
    "if STATIC_SENSORS:\n",
    "    sensors_location = []\n",
    "    with open(sensors_file_path, 'r') as f:\n",
    "        lines = f.readlines()\n",
    "        for line in lines:\n",
    "            line = line.split(',')\n",
    "            sensors_location.append(Point(int(float(line[0])), int(float(line[1]))))\n",
    "del dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 916)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidian_distance(p1: Point, p2: Point):\n",
    "    return ((p1.x - p2.x) ** 2 + (p1.y - p2.y) ** 2) ** 0.5\n",
    "\n",
    "def calculate_mu_sigma(data, num_pus):\n",
    "    sum_non_noise = 0\n",
    "    for pu_n in range(num_pus): # calculate mu\n",
    "        sum_non_noise += data[pu_n*3+2]\n",
    "    mu = ((max_x * max_y - num_pus) * noise_floor + sum_non_noise)/(max_x * max_y)\n",
    "    sum_square = 0\n",
    "    for pu_n in range(num_pus): # calculate sigma\n",
    "        sum_square += (data[pu_n*3+2]-mu)**2\n",
    "    sum_square += (max_x * max_y - num_pus) * (noise_floor - mu)**2\n",
    "    sigma = math.sqrt(sum_square/(max_x * max_y))\n",
    "    return mu, sigma\n",
    "\n",
    "def get_pu_param(pu_shape: str, intensity_degradation: str, pu_p: float,\n",
    "                 noise_floor: float, slope: float):\n",
    "    pu_param = None\n",
    "    if pu_shape == 'circle':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Circle(int((pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Circle(int(10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'square':\n",
    "        if intensity_degradation == \"linear\":\n",
    "            pu_param = Square(int(2 ** 0.5 * (pu_p - noise_floor) / slope)) # linear\n",
    "        elif intensity_degradation == \"log\":\n",
    "            pu_param = Square(int(2 ** 0.5 * 10 ** ((pu_p - noise_floor) / (10 *slope)))) # log_based\n",
    "    elif pu_shape == 'point':\n",
    "        pu_param = None\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported PU shape(create_image)! \", pu_shape)\n",
    "    return pu_param\n",
    "\n",
    "def create_image(data, slope, sensors_num, style=\"raw_power_z_score\", noise_floor=-90, \n",
    "                 ss_shape= 'circle', ss_param=None, intensity_degradation=\"log\", max_ss_power: float=0,\n",
    "                pic_cell_size: int = 1):  \n",
    "    # style = {\"raw_power_zscore_norm\", \"image_intensity\", \"raw_power_min_max_norm\"}\n",
    "    # intensity_degradation= {\"log\", \"linear\"}\n",
    "    # if param is None, it's automatically calculated. Highest brightness(or power value) (255 or 1.) would\n",
    "    # assigned to the center(PU location) and radius(side) would be calculated based on its power, slope, and noise floor.\n",
    "    # If it is given, intensity(power) of pixel beside center would be calculated in the same fashin with an exception that \n",
    "    # intensity below zero(noise_floor) would be replaced by zero(noise_floor)\n",
    "    if style == \"raw_power_min_max_norm\":\n",
    "        # In this way, PUs' location are replaced with their power(dBm) and the power would fade with \n",
    "        # slope till gets noise_floor(in circle shape)\n",
    "        \n",
    "        # creating pu matrix\n",
    "        image = np.zeros((1,number_image_channels, max_x * pic_cell_size, max_y * pic_cell_size), \n",
    "                         dtype=float_memory_used)\n",
    "#         if not STATIC_SENSORS:\n",
    "        sss_num = int(data[0]) if not STATIC_SENSORS else sensors_num\n",
    "#             print(pus_num)\n",
    "        for ss_i in range(sss_num):\n",
    "            ss_x = int(data[ss_i * 3 + 1]) if not STATIC_SENSORS else int(sensors_location[ss_i].x)\n",
    "            ss_x = max(0, min(max_x * pic_cell_size - 1, ss_x * pic_cell_size)) # limit to borders\n",
    "            ss_y = int(data[ss_i * 3 + 2]) if not STATIC_SENSORS else int(sensors_location[ss_i].y)\n",
    "            ss_y = max(0, min(max_y * pic_cell_size - 1, ss_y * pic_cell_size)) # limit to borders\n",
    "            ss_p = data[ss_i * 3 + 3] if not STATIC_SENSORS else data[ss_i]\n",
    "#                 print(pu_x, pu_y, pu_p)\n",
    "            if ss_param is None:\n",
    "                ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "            else:\n",
    "                ss_param_p = ss_param\n",
    "            points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p, \n",
    "                                        max_x = max_x * pic_cell_size, max_y = max_y * pic_cell_size)\n",
    "            ss_channel = 0\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x * pic_cell_size and 0 <= point.p.y < max_y * pic_cell_size: # TODO should pass image size\n",
    "                    if not IS_SOLID:\n",
    "                        if intensity_degradation == \"linear\":\n",
    "                            val = (ss_p - slope * point.dist - noise_floor)/(max_ss_power - noise_floor)\n",
    "                        elif intensity_degradation == \"log\":\n",
    "                            if point.dist < 1:\n",
    "                                val = (ss_p - noise_floor) / (max_ss_power - noise_floor)\n",
    "                            else:\n",
    "                                val = (ss_p - slope * 10 * math.log10(point.dist)- noise_floor)/(\n",
    "                                    max_ss_power - noise_floor)\n",
    "                    else:\n",
    "                        val = (ss_p - noise_floor) / (max_ss_power - noise_floor)\n",
    "                    image[0][ss_channel][point.p.x][point.p.y] += max(val, 0)\n",
    "#         else:\n",
    "#             for ss_i in range(sensors_num):\n",
    "#                 ss_x, ss_y, ss_p = max(0, min(max_x-1, int(sensors_location[ss_i].x))), max(0, min(max_x-1, int(\n",
    "#                     sensors_location[ss_i].y))), max(noise_floor, data[ss_i])\n",
    "#                 ss_channel = 0 \n",
    "#                 if ss_param is None:\n",
    "#                     ss_param_p = get_pu_param(ss_shape, intensity_degradation, ss_p, noise_floor, slope)\n",
    "#                 else:\n",
    "#                     ss_param_p = ss_param\n",
    "#                 points = points_inside_shape(center=Point(ss_x, ss_y), shape=ss_shape, param=ss_param_p)\n",
    "#                 for point in points:\n",
    "#                     if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "#                         if intensity_degradation == \"linear\":\n",
    "#                             image[0][ss_channel][point.p.x][point.p.y] += (ss_p - slope * point.dist - noise_floor)/(\n",
    "#                                 max_ss_power - noise_floor)\n",
    "#                         elif intensity_degradation == \"log\":\n",
    "#                             if point.dist < 1:\n",
    "#                                 image[0][ss_channel][point.p.x][point.p.y] += (ss_p - noise_floor\n",
    "#                                                                               ) / (max_ss_power - noise_floor)\n",
    "#                             else:\n",
    "#                                 image[0][ss_channel][point.p.x][point.p.y] += (ss_p - \n",
    "#                                                                                slope * 10*math.log10(point.dist) -\n",
    "#                                                                                noise_floor)/(\n",
    "#                                     max_ss_power - noise_floor)\n",
    "        return image\n",
    "        \n",
    "#         pu_image = [[(noise_floor - mu)/sigma] * max_y for _ in range(max_x)]\n",
    "    elif style == \"image_intensity\":\n",
    "        # creating PU image\n",
    "        image = np.zeros((1,number_image_channels,max_x, max_y), dtype=float_memory_used)\n",
    "        for pu_i in range(pus_num):\n",
    "            pu_x, pu_y, pu_p = max(0, min(max_x-1, int(data[pu_i*3]))), max(0, min(max_x-1, int(data[pu_i*3+1]))), data[pu_i*3+2]\n",
    "            if pu_param is None:\n",
    "                pu_param_p = get_pu_param(pu_shape, intensity_degradation, pu_p, noise_floor, slope)\n",
    "            else:\n",
    "                pu_param_p = pu_param\n",
    "            points = points_inside_shape(center=Point(pu_x, pu_y), shape=pu_shape, param=pu_param_p)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if intensity_degradation == \"linear\":\n",
    "                        image[0][0][point.p.x][point.p.y] += max((pu_p - slope * point.dist + abs(noise_floor))\n",
    "                                                              /(pu_p + abs(noise_floor)), 0)\n",
    "                    elif intensity_degradation == \"log\":\n",
    "                        if point.dist < 1:\n",
    "                            image[0][0][point.p.x][point.p.y] = 1\n",
    "                        else:\n",
    "                            image[0][0][point.p.x][point.p.y] += max((pu_p - slope * 10*math.log10(point.dist) + abs(noise_floor))\n",
    "                                                                 /(pu_p + abs(noise_floor)), 0)\n",
    "                    image[0][0][point.p.x][point.p.y] = min(image[0][0][point.p.x][point.p.y], 1.0)\n",
    "                        \n",
    "                        # creating SU image\n",
    "        su_num = (len(data) - pus_num * 3) // 2\n",
    "        if not (len(data) - pus_num * 3) % 2:\n",
    "            raise ValueError(\"Data provided is not correct; can't get SUs' information(create_image)\")\n",
    "#         su_imag= np.zeros((max_x, max_y), dtype=float_memory_used)\n",
    "        if su_param is None:\n",
    "            # if su_param is unavailable, a circle(square) with radius(side) 1 is created\n",
    "            if su_shape == 'circle':\n",
    "                su_param = Circle(1)\n",
    "            elif su_shape == 'square':\n",
    "                su_param = Square(1)\n",
    "            elif su_shape == 'point':\n",
    "                su_param = None\n",
    "            else:\n",
    "                raise ValueError(\"Unsupported SU shape(create_image)! \", su_shape)\n",
    "        su_intensity = 1.\n",
    "        for su_i in range(su_num):\n",
    "            su_x, su_y, su_p = max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) +su_i*2]))\n",
    "                                  ), max(0, min(max_x-1, int(data[pus_num * (3 if not sensors else 1) + su_i*2+1]))), su_intensity\n",
    "            points = points_inside_shape(center=Point(su_x, su_y), param=su_param, shape=su_shape)\n",
    "            for point in points:\n",
    "                if 0 <= point.p.x < max_x and 0 <= point.p.y < max_y: # TODO should pass image size\n",
    "                    if number_image_channels > 1:\n",
    "                        image[0][1][point.p.x][point.p.y] = su_intensity\n",
    "                    elif number_image_channels == 1:\n",
    "                        image[0][0][point.p.x][point.p.y] = su_intensity\n",
    "#         return np.array([pu_image, su_image, [[0.] * max_y for _ in range(max_x)]], dtype='float32') # return like this to be able to display as an RGB image with pyplot.imshow(imsave)\n",
    "#         return np.append(pu_image, su_image, axis=0)\n",
    "        return image\n",
    "        \n",
    "            \n",
    "    else:\n",
    "        raise ValueError(\"Unsupported style(create_image)! \", style)\n",
    "        \n",
    "def points_inside_shape(center: Point, shape: str, param, max_x: int, max_y: int)-> list:\n",
    "    # This function returns points+distance around center with defined shape\n",
    "    if param and type(param).__name__ == 'Circle':\n",
    "        # First creates points inside a square(around orgigin) with 2*r side and then remove those with distance > r.\n",
    "        # Shift all remaining around center. O(4r^2)\n",
    "        r, origin = param.r, Point(0, 0)\n",
    "        square_points = set((Point(x, y) for x in range(max(-r, -max_x), min(r, max_x) + 1) \n",
    "                             for y in range(max(-r, -max_y), min(r, max_y) + 1)))\n",
    "        points = []\n",
    "        while square_points:\n",
    "            p = square_points.pop()\n",
    "            dist = euclidian_distance(p, origin)\n",
    "            if dist <= r:\n",
    "                points.append(PointWithDistance(Point(p.x + center.x, p.y + center.y), dist))\n",
    "                if p.x != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, p.y))\n",
    "                if p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(p.x, -p.y))\n",
    "                if p.x != 0 and p.y != 0:\n",
    "                    points.append(PointWithDistance(Point(-p.x + center.x, -p.y + center.y), dist))\n",
    "                    square_points.remove(Point(-p.x, -p.y))\n",
    "        del square_points\n",
    "        return points\n",
    "    elif param and type(param).__name__ == 'Square':\n",
    "        half_side = param.side // 2\n",
    "        return [PointWithDistance(Point(x, y), euclidian_distance(Point(x, y), center))\n",
    "                for x in range(-half_side + center.x, half_side + center.x+1) \n",
    "                         for y in range(-half_side + center.y, half_side + center.y + 1)]\n",
    "    elif not param and shape == 'point':\n",
    "        return [PointWithDistance(center, 0)]\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported shape(points_inside_shape)! \", shape)\n",
    "        \n",
    "def read_image(image_num):\n",
    "    if style == \"image_intensity\":\n",
    "        image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "        image = np.swapaxes(image, 0, 2)\n",
    "        image = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "    elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "        suffix = 'npz'  # npy, npz\n",
    "        image = np.load(image_dir + '/image' + str(image_num) + '.' + suffix)  \n",
    "        if type(image) == np.lib.npyio.NpzFile:\n",
    "            image = image['a']\n",
    "    \n",
    "    return image\n",
    "    \n",
    "# TODO: Consider using min_max normalization becasue difference between values using\n",
    "# z-score is huge since most of the pixels have the same value, noise floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.hist(data_reg[:5,(0 if STATIC_SENSORS else 3):(sensors_num if STATIC_SENSORS else sensors_num*3+1)\n",
    "                  :(1 if STATIC_SENSORS else 3)], bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-95.0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.amin(data_reg[:, :sensors_num])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(data_sensor, bins='auto')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-0.3\n"
     ]
    }
   ],
   "source": [
    "max_ss_power = -float('inf')\n",
    "for i in range(data_reg.shape[0]):\n",
    "    ss_num = data_reg[i][0] if not STATIC_SENSORS else sensors_num\n",
    "    low, high = (0 if STATIC_SENSORS else 3), (sensors_num if STATIC_SENSORS else sensors_num * 3 + 1),\n",
    "    step = (1 if STATIC_SENSORS else 3)\n",
    "    max_ss_power = max(max_ss_power, max(data_reg[i, low:high:step]))\n",
    "print(max_ss_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_ss_power = -10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving images once to save time\n",
    "# run this cell just for creating images\n",
    "def creating_image(start, end):\n",
    "    # for image_num in range(115, data_reg.shape[0]):\n",
    "    # for image_num in range(1625, 5000):\n",
    "    for image_num in tqdm.tqdm(range(start, end+1)):  #4463, data_reg.shape[0]\n",
    "        image = create_image(data=data_reg[image_num], slope=slope, style=style, \n",
    "                             noise_floor=noise_floor,\n",
    "                             ss_shape=ss_shape, \n",
    "                             sensors_num=(sensors_num if STATIC_SENSORS else 0), \n",
    "                             intensity_degradation=intensity_degradation, \n",
    "                             max_ss_power=max_ss_power, ss_param=ss_param, \n",
    "                            pic_cell_size=pic_cell_size)\n",
    "        if style == \"image_intensity\":\n",
    "            if number_image_channels != 3:\n",
    "                image = np.append(np.array(image[0]), np.zeros((3-number_image_channels,max_x, max_y), \n",
    "                                                               dtype=float_memory_used), axis=0)\n",
    "            image_save = np.swapaxes(image, 0, 2)\n",
    "            plt.imsave(image_dir + '/image' + str(image_num)+'.png', image_save)\n",
    "        elif style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "    #         np.save(image_dir + '/image' + str(image_num), image)\n",
    "            np.savez_compressed(image_dir + '/image' + str(image_num), a=image)\n",
    "        del image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg_bak = data_reg\n",
    "data_reg = data_reg[:100000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150000, 919)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 25000/25000 [05:52<00:00, 71.02it/s]\n",
      "100%|██████████| 25000/25000 [05:52<00:00, 70.90it/s]\n",
      "100%|██████████| 25000/25000 [05:52<00:00, 70.89it/s]\n",
      "100%|██████████| 25000/25000 [05:52<00:00, 70.88it/s]\n",
      "100%|██████████| 25000/25000 [05:53<00:00, 70.76it/s] \n",
      "100%|██████████| 25000/25000 [05:54<00:00, 70.56it/s] \n"
     ]
    }
   ],
   "source": [
    "jobs = []\n",
    "proc_sizes = [data_reg.shape[0]//number_of_proccessors] * (number_of_proccessors)\n",
    "proc_sizes[-1] += data_reg.shape[0]%number_of_proccessors\n",
    "proc_idx = [(sum(proc_sizes[:i]), sum(proc_sizes[:i+1])-1) for i in range(number_of_proccessors)]\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    p = Process(target=creating_image, args=(proc_idx[i][0], proc_idx[i][1]))\n",
    "    jobs.append(p)\n",
    "    p.start()\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].join()\n",
    "\n",
    "for i in range(number_of_proccessors):\n",
    "    jobs[i].terminate()\n",
    "    jobs[i].close()\n",
    "del jobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Point(x=2, y=2)\n",
      "2 Point(x=2, y=7)\n",
      "3 Point(x=2, y=12)\n",
      "4 Point(x=2, y=17)\n",
      "5 Point(x=2, y=22)\n",
      "6 Point(x=2, y=27)\n",
      "7 Point(x=2, y=32)\n",
      "8 Point(x=2, y=37)\n",
      "9 Point(x=2, y=42)\n",
      "10 Point(x=2, y=47)\n",
      "11 Point(x=2, y=52)\n",
      "12 Point(x=2, y=57)\n",
      "13 Point(x=2, y=62)\n",
      "14 Point(x=2, y=67)\n",
      "15 Point(x=2, y=72)\n",
      "16 Point(x=2, y=77)\n",
      "17 Point(x=2, y=82)\n",
      "18 Point(x=2, y=87)\n",
      "19 Point(x=2, y=92)\n",
      "20 Point(x=2, y=97)\n",
      "21 Point(x=2, y=102)\n",
      "22 Point(x=2, y=107)\n",
      "23 Point(x=2, y=112)\n",
      "24 Point(x=2, y=117)\n",
      "25 Point(x=2, y=122)\n",
      "26 Point(x=2, y=127)\n",
      "27 Point(x=2, y=132)\n",
      "28 Point(x=2, y=137)\n",
      "29 Point(x=2, y=142)\n",
      "30 Point(x=2, y=147)\n",
      "31 Point(x=2, y=152)\n",
      "32 Point(x=2, y=157)\n",
      "33 Point(x=2, y=162)\n",
      "34 Point(x=2, y=167)\n",
      "35 Point(x=2, y=172)\n",
      "36 Point(x=2, y=177)\n",
      "37 Point(x=2, y=182)\n",
      "38 Point(x=2, y=187)\n",
      "39 Point(x=2, y=192)\n",
      "40 Point(x=2, y=197)\n",
      "41 Point(x=7, y=2)\n",
      "42 Point(x=7, y=7)\n",
      "43 Point(x=7, y=12)\n",
      "44 Point(x=7, y=17)\n",
      "45 Point(x=7, y=22)\n",
      "46 Point(x=7, y=27)\n",
      "47 Point(x=7, y=32)\n",
      "48 Point(x=7, y=37)\n",
      "49 Point(x=7, y=42)\n",
      "50 Point(x=7, y=47)\n",
      "51 Point(x=7, y=52)\n",
      "52 Point(x=7, y=57)\n",
      "53 Point(x=7, y=62)\n",
      "54 Point(x=7, y=67)\n",
      "55 Point(x=7, y=72)\n",
      "56 Point(x=7, y=77)\n",
      "57 Point(x=7, y=82)\n",
      "58 Point(x=7, y=87)\n",
      "59 Point(x=7, y=92)\n",
      "60 Point(x=7, y=97)\n",
      "61 Point(x=7, y=102)\n",
      "62 Point(x=7, y=107)\n",
      "63 Point(x=7, y=112)\n",
      "64 Point(x=7, y=117)\n",
      "65 Point(x=7, y=122)\n",
      "66 Point(x=7, y=127)\n",
      "67 Point(x=7, y=132)\n",
      "68 Point(x=7, y=137)\n",
      "69 Point(x=7, y=142)\n",
      "70 Point(x=7, y=147)\n",
      "71 Point(x=7, y=152)\n",
      "72 Point(x=7, y=157)\n",
      "73 Point(x=7, y=162)\n",
      "74 Point(x=7, y=167)\n",
      "75 Point(x=7, y=172)\n",
      "76 Point(x=7, y=177)\n",
      "77 Point(x=7, y=182)\n",
      "78 Point(x=7, y=187)\n",
      "79 Point(x=7, y=192)\n",
      "80 Point(x=7, y=197)\n",
      "81 Point(x=12, y=2)\n",
      "82 Point(x=12, y=7)\n",
      "83 Point(x=12, y=12)\n",
      "84 Point(x=12, y=17)\n",
      "85 Point(x=12, y=22)\n",
      "86 Point(x=12, y=27)\n",
      "87 Point(x=12, y=32)\n",
      "88 Point(x=12, y=37)\n",
      "89 Point(x=12, y=42)\n",
      "90 Point(x=12, y=47)\n",
      "91 Point(x=12, y=52)\n",
      "92 Point(x=12, y=57)\n",
      "93 Point(x=12, y=62)\n",
      "94 Point(x=12, y=67)\n",
      "95 Point(x=12, y=72)\n",
      "96 Point(x=12, y=77)\n",
      "97 Point(x=12, y=82)\n",
      "98 Point(x=12, y=87)\n",
      "99 Point(x=12, y=92)\n",
      "100 Point(x=12, y=97)\n",
      "101 Point(x=12, y=102)\n",
      "102 Point(x=12, y=107)\n",
      "103 Point(x=12, y=112)\n",
      "104 Point(x=12, y=117)\n",
      "105 Point(x=12, y=122)\n",
      "106 Point(x=12, y=127)\n",
      "107 Point(x=12, y=132)\n",
      "108 Point(x=12, y=137)\n",
      "109 Point(x=12, y=142)\n",
      "110 Point(x=12, y=147)\n",
      "111 Point(x=12, y=152)\n",
      "112 Point(x=12, y=157)\n",
      "113 Point(x=12, y=162)\n",
      "114 Point(x=12, y=167)\n",
      "115 Point(x=12, y=172)\n",
      "116 Point(x=12, y=177)\n",
      "117 Point(x=12, y=182)\n",
      "118 Point(x=12, y=187)\n",
      "119 Point(x=12, y=192)\n",
      "120 Point(x=12, y=197)\n",
      "121 Point(x=17, y=2)\n",
      "122 Point(x=17, y=7)\n",
      "123 Point(x=17, y=12)\n",
      "124 Point(x=17, y=17)\n",
      "125 Point(x=17, y=22)\n",
      "126 Point(x=17, y=27)\n",
      "127 Point(x=17, y=32)\n",
      "128 Point(x=17, y=37)\n",
      "129 Point(x=17, y=42)\n",
      "130 Point(x=17, y=47)\n",
      "131 Point(x=17, y=52)\n",
      "132 Point(x=17, y=57)\n",
      "133 Point(x=17, y=62)\n",
      "134 Point(x=17, y=67)\n",
      "135 Point(x=17, y=72)\n",
      "136 Point(x=17, y=77)\n",
      "137 Point(x=17, y=82)\n",
      "138 Point(x=17, y=87)\n",
      "139 Point(x=17, y=92)\n",
      "140 Point(x=17, y=97)\n",
      "141 Point(x=17, y=102)\n",
      "142 Point(x=17, y=107)\n",
      "143 Point(x=17, y=112)\n",
      "144 Point(x=17, y=117)\n",
      "145 Point(x=17, y=122)\n",
      "146 Point(x=17, y=127)\n",
      "147 Point(x=17, y=132)\n",
      "148 Point(x=17, y=137)\n",
      "149 Point(x=17, y=142)\n",
      "150 Point(x=17, y=147)\n",
      "151 Point(x=17, y=152)\n",
      "152 Point(x=17, y=157)\n",
      "153 Point(x=17, y=162)\n",
      "154 Point(x=17, y=167)\n",
      "155 Point(x=17, y=172)\n",
      "156 Point(x=17, y=177)\n",
      "157 Point(x=17, y=182)\n",
      "158 Point(x=17, y=187)\n",
      "159 Point(x=17, y=192)\n",
      "160 Point(x=17, y=197)\n",
      "161 Point(x=22, y=2)\n",
      "162 Point(x=22, y=7)\n",
      "163 Point(x=22, y=12)\n",
      "164 Point(x=22, y=17)\n",
      "165 Point(x=22, y=22)\n",
      "166 Point(x=22, y=27)\n",
      "167 Point(x=22, y=32)\n",
      "168 Point(x=22, y=37)\n",
      "169 Point(x=22, y=42)\n",
      "170 Point(x=22, y=47)\n",
      "171 Point(x=22, y=52)\n",
      "172 Point(x=22, y=57)\n",
      "173 Point(x=22, y=62)\n",
      "174 Point(x=22, y=67)\n",
      "175 Point(x=22, y=72)\n",
      "176 Point(x=22, y=77)\n",
      "177 Point(x=22, y=82)\n",
      "178 Point(x=22, y=87)\n",
      "179 Point(x=22, y=92)\n",
      "180 Point(x=22, y=97)\n",
      "181 Point(x=22, y=102)\n",
      "182 Point(x=22, y=107)\n",
      "183 Point(x=22, y=112)\n",
      "184 Point(x=22, y=117)\n",
      "185 Point(x=22, y=122)\n",
      "186 Point(x=22, y=127)\n",
      "187 Point(x=22, y=132)\n",
      "188 Point(x=22, y=137)\n",
      "189 Point(x=22, y=142)\n",
      "190 Point(x=22, y=147)\n",
      "191 Point(x=22, y=152)\n",
      "192 Point(x=22, y=157)\n",
      "193 Point(x=22, y=162)\n",
      "194 Point(x=22, y=167)\n",
      "195 Point(x=22, y=172)\n",
      "196 Point(x=22, y=177)\n",
      "197 Point(x=22, y=182)\n",
      "198 Point(x=22, y=187)\n",
      "199 Point(x=22, y=192)\n",
      "200 Point(x=22, y=197)\n",
      "201 Point(x=27, y=2)\n",
      "202 Point(x=27, y=7)\n",
      "203 Point(x=27, y=12)\n",
      "204 Point(x=27, y=17)\n",
      "205 Point(x=27, y=22)\n",
      "206 Point(x=27, y=27)\n",
      "207 Point(x=27, y=32)\n",
      "208 Point(x=27, y=37)\n",
      "209 Point(x=27, y=42)\n",
      "210 Point(x=27, y=47)\n",
      "211 Point(x=27, y=52)\n",
      "212 Point(x=27, y=57)\n",
      "213 Point(x=27, y=62)\n",
      "214 Point(x=27, y=67)\n",
      "215 Point(x=27, y=72)\n",
      "216 Point(x=27, y=77)\n",
      "217 Point(x=27, y=82)\n",
      "218 Point(x=27, y=87)\n",
      "219 Point(x=27, y=92)\n",
      "220 Point(x=27, y=97)\n",
      "221 Point(x=27, y=102)\n",
      "222 Point(x=27, y=107)\n",
      "223 Point(x=27, y=112)\n",
      "224 Point(x=27, y=117)\n",
      "225 Point(x=27, y=122)\n",
      "226 Point(x=27, y=127)\n",
      "227 Point(x=27, y=132)\n",
      "228 Point(x=27, y=137)\n",
      "229 Point(x=27, y=142)\n",
      "230 Point(x=27, y=147)\n",
      "231 Point(x=27, y=152)\n",
      "232 Point(x=27, y=157)\n",
      "233 Point(x=27, y=162)\n",
      "234 Point(x=27, y=167)\n",
      "235 Point(x=27, y=172)\n",
      "236 Point(x=27, y=177)\n",
      "237 Point(x=27, y=182)\n",
      "238 Point(x=27, y=187)\n",
      "239 Point(x=27, y=192)\n",
      "240 Point(x=27, y=197)\n",
      "241 Point(x=32, y=2)\n",
      "242 Point(x=32, y=7)\n",
      "243 Point(x=32, y=12)\n",
      "244 Point(x=32, y=17)\n",
      "245 Point(x=32, y=22)\n",
      "246 Point(x=32, y=27)\n",
      "247 Point(x=32, y=32)\n",
      "248 Point(x=32, y=37)\n",
      "249 Point(x=32, y=42)\n",
      "250 Point(x=32, y=47)\n",
      "251 Point(x=32, y=52)\n",
      "252 Point(x=32, y=57)\n",
      "253 Point(x=32, y=62)\n",
      "254 Point(x=32, y=67)\n",
      "255 Point(x=32, y=72)\n",
      "256 Point(x=32, y=77)\n",
      "257 Point(x=32, y=82)\n",
      "258 Point(x=32, y=87)\n",
      "259 Point(x=32, y=92)\n",
      "260 Point(x=32, y=97)\n",
      "261 Point(x=32, y=102)\n",
      "262 Point(x=32, y=107)\n",
      "263 Point(x=32, y=112)\n",
      "264 Point(x=32, y=117)\n",
      "265 Point(x=32, y=122)\n",
      "266 Point(x=32, y=127)\n",
      "267 Point(x=32, y=132)\n",
      "268 Point(x=32, y=137)\n",
      "269 Point(x=32, y=142)\n",
      "270 Point(x=32, y=147)\n",
      "271 Point(x=32, y=152)\n",
      "272 Point(x=32, y=157)\n",
      "273 Point(x=32, y=162)\n",
      "274 Point(x=32, y=167)\n",
      "275 Point(x=32, y=172)\n",
      "276 Point(x=32, y=177)\n",
      "277 Point(x=32, y=182)\n",
      "278 Point(x=32, y=187)\n",
      "279 Point(x=32, y=192)\n",
      "280 Point(x=32, y=197)\n",
      "281 Point(x=37, y=2)\n",
      "282 Point(x=37, y=7)\n",
      "283 Point(x=37, y=12)\n",
      "284 Point(x=37, y=17)\n",
      "285 Point(x=37, y=22)\n",
      "286 Point(x=37, y=27)\n",
      "287 Point(x=37, y=32)\n",
      "288 Point(x=37, y=37)\n",
      "289 Point(x=37, y=42)\n",
      "290 Point(x=37, y=47)\n",
      "291 Point(x=37, y=52)\n",
      "292 Point(x=37, y=57)\n",
      "293 Point(x=37, y=62)\n",
      "294 Point(x=37, y=67)\n",
      "295 Point(x=37, y=72)\n",
      "296 Point(x=37, y=77)\n",
      "297 Point(x=37, y=82)\n",
      "298 Point(x=37, y=87)\n",
      "299 Point(x=37, y=92)\n",
      "300 Point(x=37, y=97)\n",
      "301 Point(x=37, y=102)\n",
      "302 Point(x=37, y=107)\n",
      "303 Point(x=37, y=112)\n",
      "304 Point(x=37, y=117)\n",
      "305 Point(x=37, y=122)\n",
      "306 Point(x=37, y=127)\n",
      "307 Point(x=37, y=132)\n",
      "308 Point(x=37, y=137)\n",
      "309 Point(x=37, y=142)\n",
      "310 Point(x=37, y=147)\n",
      "311 Point(x=37, y=152)\n",
      "312 Point(x=37, y=157)\n",
      "313 Point(x=37, y=162)\n",
      "314 Point(x=37, y=167)\n",
      "315 Point(x=37, y=172)\n",
      "316 Point(x=37, y=177)\n",
      "317 Point(x=37, y=182)\n",
      "318 Point(x=37, y=187)\n",
      "319 Point(x=37, y=192)\n",
      "320 Point(x=37, y=197)\n",
      "321 Point(x=42, y=2)\n",
      "322 Point(x=42, y=7)\n",
      "323 Point(x=42, y=12)\n",
      "324 Point(x=42, y=17)\n",
      "325 Point(x=42, y=22)\n",
      "326 Point(x=42, y=27)\n",
      "327 Point(x=42, y=32)\n",
      "328 Point(x=42, y=37)\n",
      "329 Point(x=42, y=42)\n",
      "330 Point(x=42, y=47)\n",
      "331 Point(x=42, y=52)\n",
      "332 Point(x=42, y=57)\n",
      "333 Point(x=42, y=62)\n",
      "334 Point(x=42, y=67)\n",
      "335 Point(x=42, y=72)\n",
      "336 Point(x=42, y=77)\n",
      "337 Point(x=42, y=82)\n",
      "338 Point(x=42, y=87)\n",
      "339 Point(x=42, y=92)\n",
      "340 Point(x=42, y=97)\n",
      "341 Point(x=42, y=102)\n",
      "342 Point(x=42, y=107)\n",
      "343 Point(x=42, y=112)\n",
      "344 Point(x=42, y=117)\n",
      "345 Point(x=42, y=122)\n",
      "346 Point(x=42, y=127)\n",
      "347 Point(x=42, y=132)\n",
      "348 Point(x=42, y=137)\n",
      "349 Point(x=42, y=142)\n",
      "350 Point(x=42, y=147)\n",
      "351 Point(x=42, y=152)\n",
      "352 Point(x=42, y=157)\n",
      "353 Point(x=42, y=162)\n",
      "354 Point(x=42, y=167)\n",
      "355 Point(x=42, y=172)\n",
      "356 Point(x=42, y=177)\n",
      "357 Point(x=42, y=182)\n",
      "358 Point(x=42, y=187)\n",
      "359 Point(x=42, y=192)\n",
      "360 Point(x=42, y=197)\n",
      "361 Point(x=47, y=2)\n",
      "362 Point(x=47, y=7)\n",
      "363 Point(x=47, y=12)\n",
      "364 Point(x=47, y=17)\n",
      "365 Point(x=47, y=22)\n",
      "366 Point(x=47, y=27)\n",
      "367 Point(x=47, y=32)\n",
      "368 Point(x=47, y=37)\n",
      "369 Point(x=47, y=42)\n",
      "370 Point(x=47, y=47)\n",
      "371 Point(x=47, y=52)\n",
      "372 Point(x=47, y=57)\n",
      "373 Point(x=47, y=62)\n",
      "374 Point(x=47, y=67)\n",
      "375 Point(x=47, y=72)\n",
      "376 Point(x=47, y=77)\n",
      "377 Point(x=47, y=82)\n",
      "378 Point(x=47, y=87)\n",
      "379 Point(x=47, y=92)\n",
      "380 Point(x=47, y=97)\n",
      "381 Point(x=47, y=102)\n",
      "382 Point(x=47, y=107)\n",
      "383 Point(x=47, y=112)\n",
      "384 Point(x=47, y=117)\n",
      "385 Point(x=47, y=122)\n",
      "386 Point(x=47, y=127)\n",
      "387 Point(x=47, y=132)\n",
      "388 Point(x=47, y=137)\n",
      "389 Point(x=47, y=142)\n",
      "390 Point(x=47, y=147)\n",
      "391 Point(x=47, y=152)\n",
      "392 Point(x=47, y=157)\n",
      "393 Point(x=47, y=162)\n",
      "394 Point(x=47, y=167)\n",
      "395 Point(x=47, y=172)\n",
      "396 Point(x=47, y=177)\n",
      "397 Point(x=47, y=182)\n",
      "398 Point(x=47, y=187)\n",
      "399 Point(x=47, y=192)\n",
      "400 Point(x=47, y=197)\n",
      "401 Point(x=52, y=2)\n",
      "402 Point(x=52, y=7)\n",
      "403 Point(x=52, y=12)\n",
      "404 Point(x=52, y=17)\n",
      "405 Point(x=52, y=22)\n",
      "406 Point(x=52, y=27)\n",
      "407 Point(x=52, y=32)\n",
      "408 Point(x=52, y=37)\n",
      "409 Point(x=52, y=42)\n",
      "410 Point(x=52, y=47)\n",
      "411 Point(x=52, y=52)\n",
      "412 Point(x=52, y=57)\n",
      "413 Point(x=52, y=62)\n",
      "414 Point(x=52, y=67)\n",
      "415 Point(x=52, y=72)\n",
      "416 Point(x=52, y=77)\n",
      "417 Point(x=52, y=82)\n",
      "418 Point(x=52, y=87)\n",
      "419 Point(x=52, y=92)\n",
      "420 Point(x=52, y=97)\n",
      "421 Point(x=52, y=102)\n",
      "422 Point(x=52, y=107)\n",
      "423 Point(x=52, y=112)\n",
      "424 Point(x=52, y=117)\n",
      "425 Point(x=52, y=122)\n",
      "426 Point(x=52, y=127)\n",
      "427 Point(x=52, y=132)\n",
      "428 Point(x=52, y=137)\n",
      "429 Point(x=52, y=142)\n",
      "430 Point(x=52, y=147)\n",
      "431 Point(x=52, y=152)\n",
      "432 Point(x=52, y=157)\n",
      "433 Point(x=52, y=162)\n",
      "434 Point(x=52, y=167)\n",
      "435 Point(x=52, y=172)\n",
      "436 Point(x=52, y=177)\n",
      "437 Point(x=52, y=182)\n",
      "438 Point(x=52, y=187)\n",
      "439 Point(x=52, y=192)\n",
      "440 Point(x=52, y=197)\n",
      "441 Point(x=57, y=2)\n",
      "442 Point(x=57, y=7)\n",
      "443 Point(x=57, y=12)\n",
      "444 Point(x=57, y=17)\n",
      "445 Point(x=57, y=22)\n",
      "446 Point(x=57, y=27)\n",
      "447 Point(x=57, y=32)\n",
      "448 Point(x=57, y=37)\n",
      "449 Point(x=57, y=42)\n",
      "450 Point(x=57, y=47)\n",
      "451 Point(x=57, y=52)\n",
      "452 Point(x=57, y=57)\n",
      "453 Point(x=57, y=62)\n",
      "454 Point(x=57, y=67)\n",
      "455 Point(x=57, y=72)\n",
      "456 Point(x=57, y=77)\n",
      "457 Point(x=57, y=82)\n",
      "458 Point(x=57, y=87)\n",
      "459 Point(x=57, y=92)\n",
      "460 Point(x=57, y=97)\n",
      "461 Point(x=57, y=102)\n",
      "462 Point(x=57, y=107)\n",
      "463 Point(x=57, y=112)\n",
      "464 Point(x=57, y=117)\n",
      "465 Point(x=57, y=122)\n",
      "466 Point(x=57, y=127)\n",
      "467 Point(x=57, y=132)\n",
      "468 Point(x=57, y=137)\n",
      "469 Point(x=57, y=142)\n",
      "470 Point(x=57, y=147)\n",
      "471 Point(x=57, y=152)\n",
      "472 Point(x=57, y=157)\n",
      "473 Point(x=57, y=162)\n",
      "474 Point(x=57, y=167)\n",
      "475 Point(x=57, y=172)\n",
      "476 Point(x=57, y=177)\n",
      "477 Point(x=57, y=182)\n",
      "478 Point(x=57, y=187)\n",
      "479 Point(x=57, y=192)\n",
      "480 Point(x=57, y=197)\n",
      "481 Point(x=62, y=2)\n",
      "482 Point(x=62, y=7)\n",
      "483 Point(x=62, y=12)\n",
      "484 Point(x=62, y=17)\n",
      "485 Point(x=62, y=22)\n",
      "486 Point(x=62, y=27)\n",
      "487 Point(x=62, y=32)\n",
      "488 Point(x=62, y=37)\n",
      "489 Point(x=62, y=42)\n",
      "490 Point(x=62, y=47)\n",
      "491 Point(x=62, y=52)\n",
      "492 Point(x=62, y=57)\n",
      "493 Point(x=62, y=62)\n",
      "494 Point(x=62, y=67)\n",
      "495 Point(x=62, y=72)\n",
      "496 Point(x=62, y=77)\n",
      "497 Point(x=62, y=82)\n",
      "498 Point(x=62, y=87)\n",
      "499 Point(x=62, y=92)\n",
      "500 Point(x=62, y=97)\n",
      "501 Point(x=62, y=102)\n",
      "502 Point(x=62, y=107)\n",
      "503 Point(x=62, y=112)\n",
      "504 Point(x=62, y=117)\n",
      "505 Point(x=62, y=122)\n",
      "506 Point(x=62, y=127)\n",
      "507 Point(x=62, y=132)\n",
      "508 Point(x=62, y=137)\n",
      "509 Point(x=62, y=142)\n",
      "510 Point(x=62, y=147)\n",
      "511 Point(x=62, y=152)\n",
      "512 Point(x=62, y=157)\n",
      "513 Point(x=62, y=162)\n",
      "514 Point(x=62, y=167)\n",
      "515 Point(x=62, y=172)\n",
      "516 Point(x=62, y=177)\n",
      "517 Point(x=62, y=182)\n",
      "518 Point(x=62, y=187)\n",
      "519 Point(x=62, y=192)\n",
      "520 Point(x=62, y=197)\n",
      "521 Point(x=67, y=2)\n",
      "522 Point(x=67, y=7)\n",
      "523 Point(x=67, y=12)\n",
      "524 Point(x=67, y=17)\n",
      "525 Point(x=67, y=22)\n",
      "526 Point(x=67, y=27)\n",
      "527 Point(x=67, y=32)\n",
      "528 Point(x=67, y=37)\n",
      "529 Point(x=67, y=42)\n",
      "530 Point(x=67, y=47)\n",
      "531 Point(x=67, y=52)\n",
      "532 Point(x=67, y=57)\n",
      "533 Point(x=67, y=62)\n",
      "534 Point(x=67, y=67)\n",
      "535 Point(x=67, y=72)\n",
      "536 Point(x=67, y=77)\n",
      "537 Point(x=67, y=82)\n",
      "538 Point(x=67, y=87)\n",
      "539 Point(x=67, y=92)\n",
      "540 Point(x=67, y=97)\n",
      "541 Point(x=67, y=102)\n",
      "542 Point(x=67, y=107)\n",
      "543 Point(x=67, y=112)\n",
      "544 Point(x=67, y=117)\n",
      "545 Point(x=67, y=122)\n",
      "546 Point(x=67, y=127)\n",
      "547 Point(x=67, y=132)\n",
      "548 Point(x=67, y=137)\n",
      "549 Point(x=67, y=142)\n",
      "550 Point(x=67, y=147)\n",
      "551 Point(x=67, y=152)\n",
      "552 Point(x=67, y=157)\n",
      "553 Point(x=67, y=162)\n",
      "554 Point(x=67, y=167)\n",
      "555 Point(x=67, y=172)\n",
      "556 Point(x=67, y=177)\n",
      "557 Point(x=67, y=182)\n",
      "558 Point(x=67, y=187)\n",
      "559 Point(x=67, y=192)\n",
      "560 Point(x=67, y=197)\n",
      "561 Point(x=72, y=2)\n",
      "562 Point(x=72, y=7)\n",
      "563 Point(x=72, y=12)\n",
      "564 Point(x=72, y=17)\n",
      "565 Point(x=72, y=22)\n",
      "566 Point(x=72, y=27)\n",
      "567 Point(x=72, y=32)\n",
      "568 Point(x=72, y=37)\n",
      "569 Point(x=72, y=42)\n",
      "570 Point(x=72, y=47)\n",
      "571 Point(x=72, y=52)\n",
      "572 Point(x=72, y=57)\n",
      "573 Point(x=72, y=62)\n",
      "574 Point(x=72, y=67)\n",
      "575 Point(x=72, y=72)\n",
      "576 Point(x=72, y=77)\n",
      "577 Point(x=72, y=82)\n",
      "578 Point(x=72, y=87)\n",
      "579 Point(x=72, y=92)\n",
      "580 Point(x=72, y=97)\n",
      "581 Point(x=72, y=102)\n",
      "582 Point(x=72, y=107)\n",
      "583 Point(x=72, y=112)\n",
      "584 Point(x=72, y=117)\n",
      "585 Point(x=72, y=122)\n",
      "586 Point(x=72, y=127)\n",
      "587 Point(x=72, y=132)\n",
      "588 Point(x=72, y=137)\n",
      "589 Point(x=72, y=142)\n",
      "590 Point(x=72, y=147)\n",
      "591 Point(x=72, y=152)\n",
      "592 Point(x=72, y=157)\n",
      "593 Point(x=72, y=162)\n",
      "594 Point(x=72, y=167)\n",
      "595 Point(x=72, y=172)\n",
      "596 Point(x=72, y=177)\n",
      "597 Point(x=72, y=182)\n",
      "598 Point(x=72, y=187)\n",
      "599 Point(x=72, y=192)\n",
      "600 Point(x=72, y=197)\n",
      "601 Point(x=77, y=2)\n",
      "602 Point(x=77, y=7)\n",
      "603 Point(x=77, y=12)\n",
      "604 Point(x=77, y=17)\n",
      "605 Point(x=77, y=22)\n",
      "606 Point(x=77, y=27)\n",
      "607 Point(x=77, y=32)\n",
      "608 Point(x=77, y=37)\n",
      "609 Point(x=77, y=42)\n",
      "610 Point(x=77, y=47)\n",
      "611 Point(x=77, y=52)\n",
      "612 Point(x=77, y=57)\n",
      "613 Point(x=77, y=62)\n",
      "614 Point(x=77, y=67)\n",
      "615 Point(x=77, y=72)\n",
      "616 Point(x=77, y=77)\n",
      "617 Point(x=77, y=82)\n",
      "618 Point(x=77, y=87)\n",
      "619 Point(x=77, y=92)\n",
      "620 Point(x=77, y=97)\n",
      "621 Point(x=77, y=102)\n",
      "622 Point(x=77, y=107)\n",
      "623 Point(x=77, y=112)\n",
      "624 Point(x=77, y=117)\n",
      "625 Point(x=77, y=122)\n",
      "626 Point(x=77, y=127)\n",
      "627 Point(x=77, y=132)\n",
      "628 Point(x=77, y=137)\n",
      "629 Point(x=77, y=142)\n",
      "630 Point(x=77, y=147)\n",
      "631 Point(x=77, y=152)\n",
      "632 Point(x=77, y=157)\n",
      "633 Point(x=77, y=162)\n",
      "634 Point(x=77, y=167)\n",
      "635 Point(x=77, y=172)\n",
      "636 Point(x=77, y=177)\n",
      "637 Point(x=77, y=182)\n",
      "638 Point(x=77, y=187)\n",
      "639 Point(x=77, y=192)\n",
      "640 Point(x=77, y=197)\n",
      "641 Point(x=82, y=2)\n",
      "642 Point(x=82, y=7)\n",
      "643 Point(x=82, y=12)\n",
      "644 Point(x=82, y=17)\n",
      "645 Point(x=82, y=22)\n",
      "646 Point(x=82, y=27)\n",
      "647 Point(x=82, y=32)\n",
      "648 Point(x=82, y=37)\n",
      "649 Point(x=82, y=42)\n",
      "650 Point(x=82, y=47)\n",
      "651 Point(x=82, y=52)\n",
      "652 Point(x=82, y=57)\n",
      "653 Point(x=82, y=62)\n",
      "654 Point(x=82, y=67)\n",
      "655 Point(x=82, y=72)\n",
      "656 Point(x=82, y=77)\n",
      "657 Point(x=82, y=82)\n",
      "658 Point(x=82, y=87)\n",
      "659 Point(x=82, y=92)\n",
      "660 Point(x=82, y=97)\n",
      "661 Point(x=82, y=102)\n",
      "662 Point(x=82, y=107)\n",
      "663 Point(x=82, y=112)\n",
      "664 Point(x=82, y=117)\n",
      "665 Point(x=82, y=122)\n",
      "666 Point(x=82, y=127)\n",
      "667 Point(x=82, y=132)\n",
      "668 Point(x=82, y=137)\n",
      "669 Point(x=82, y=142)\n",
      "670 Point(x=82, y=147)\n",
      "671 Point(x=82, y=152)\n",
      "672 Point(x=82, y=157)\n",
      "673 Point(x=82, y=162)\n",
      "674 Point(x=82, y=167)\n",
      "675 Point(x=82, y=172)\n",
      "676 Point(x=82, y=177)\n",
      "677 Point(x=82, y=182)\n",
      "678 Point(x=82, y=187)\n",
      "679 Point(x=82, y=192)\n",
      "680 Point(x=82, y=197)\n",
      "681 Point(x=87, y=2)\n",
      "682 Point(x=87, y=7)\n",
      "683 Point(x=87, y=12)\n",
      "684 Point(x=87, y=17)\n",
      "685 Point(x=87, y=22)\n",
      "686 Point(x=87, y=27)\n",
      "687 Point(x=87, y=32)\n",
      "688 Point(x=87, y=37)\n",
      "689 Point(x=87, y=42)\n",
      "690 Point(x=87, y=47)\n",
      "691 Point(x=87, y=52)\n",
      "692 Point(x=87, y=57)\n",
      "693 Point(x=87, y=62)\n",
      "694 Point(x=87, y=67)\n",
      "695 Point(x=87, y=72)\n",
      "696 Point(x=87, y=77)\n",
      "697 Point(x=87, y=82)\n",
      "698 Point(x=87, y=87)\n",
      "699 Point(x=87, y=92)\n",
      "700 Point(x=87, y=97)\n",
      "701 Point(x=87, y=102)\n",
      "702 Point(x=87, y=107)\n",
      "703 Point(x=87, y=112)\n",
      "704 Point(x=87, y=117)\n",
      "705 Point(x=87, y=122)\n",
      "706 Point(x=87, y=127)\n",
      "707 Point(x=87, y=132)\n",
      "708 Point(x=87, y=137)\n",
      "709 Point(x=87, y=142)\n",
      "710 Point(x=87, y=147)\n",
      "711 Point(x=87, y=152)\n",
      "712 Point(x=87, y=157)\n",
      "713 Point(x=87, y=162)\n",
      "714 Point(x=87, y=167)\n",
      "715 Point(x=87, y=172)\n",
      "716 Point(x=87, y=177)\n",
      "717 Point(x=87, y=182)\n",
      "718 Point(x=87, y=187)\n",
      "719 Point(x=87, y=192)\n",
      "720 Point(x=87, y=197)\n",
      "721 Point(x=92, y=2)\n",
      "722 Point(x=92, y=7)\n",
      "723 Point(x=92, y=12)\n",
      "724 Point(x=92, y=17)\n",
      "725 Point(x=92, y=22)\n",
      "726 Point(x=92, y=27)\n",
      "727 Point(x=92, y=32)\n",
      "728 Point(x=92, y=37)\n",
      "729 Point(x=92, y=42)\n",
      "730 Point(x=92, y=47)\n",
      "731 Point(x=92, y=52)\n",
      "732 Point(x=92, y=57)\n",
      "733 Point(x=92, y=62)\n",
      "734 Point(x=92, y=67)\n",
      "735 Point(x=92, y=72)\n",
      "736 Point(x=92, y=77)\n",
      "737 Point(x=92, y=82)\n",
      "738 Point(x=92, y=87)\n",
      "739 Point(x=92, y=92)\n",
      "740 Point(x=92, y=97)\n",
      "741 Point(x=92, y=102)\n",
      "742 Point(x=92, y=107)\n",
      "743 Point(x=92, y=112)\n",
      "744 Point(x=92, y=117)\n",
      "745 Point(x=92, y=122)\n",
      "746 Point(x=92, y=127)\n",
      "747 Point(x=92, y=132)\n",
      "748 Point(x=92, y=137)\n",
      "749 Point(x=92, y=142)\n",
      "750 Point(x=92, y=147)\n",
      "751 Point(x=92, y=152)\n",
      "752 Point(x=92, y=157)\n",
      "753 Point(x=92, y=162)\n",
      "754 Point(x=92, y=167)\n",
      "755 Point(x=92, y=172)\n",
      "756 Point(x=92, y=177)\n",
      "757 Point(x=92, y=182)\n",
      "758 Point(x=92, y=187)\n",
      "759 Point(x=92, y=192)\n",
      "760 Point(x=92, y=197)\n",
      "761 Point(x=97, y=2)\n",
      "762 Point(x=97, y=7)\n",
      "763 Point(x=97, y=12)\n",
      "764 Point(x=97, y=17)\n",
      "765 Point(x=97, y=22)\n",
      "766 Point(x=97, y=27)\n",
      "767 Point(x=97, y=32)\n",
      "768 Point(x=97, y=37)\n",
      "769 Point(x=97, y=42)\n",
      "770 Point(x=97, y=47)\n",
      "771 Point(x=97, y=52)\n",
      "772 Point(x=97, y=57)\n",
      "773 Point(x=97, y=62)\n",
      "774 Point(x=97, y=67)\n",
      "775 Point(x=97, y=72)\n",
      "776 Point(x=97, y=77)\n",
      "777 Point(x=97, y=82)\n",
      "778 Point(x=97, y=87)\n",
      "779 Point(x=97, y=92)\n",
      "780 Point(x=97, y=97)\n",
      "781 Point(x=97, y=102)\n",
      "782 Point(x=97, y=107)\n",
      "783 Point(x=97, y=112)\n",
      "784 Point(x=97, y=117)\n",
      "785 Point(x=97, y=122)\n",
      "786 Point(x=97, y=127)\n",
      "787 Point(x=97, y=132)\n",
      "788 Point(x=97, y=137)\n",
      "789 Point(x=97, y=142)\n",
      "790 Point(x=97, y=147)\n",
      "791 Point(x=97, y=152)\n",
      "792 Point(x=97, y=157)\n",
      "793 Point(x=97, y=162)\n",
      "794 Point(x=97, y=167)\n",
      "795 Point(x=97, y=172)\n",
      "796 Point(x=97, y=177)\n",
      "797 Point(x=97, y=182)\n",
      "798 Point(x=97, y=187)\n",
      "799 Point(x=97, y=192)\n",
      "800 Point(x=97, y=197)\n",
      "801 Point(x=102, y=2)\n",
      "802 Point(x=102, y=7)\n",
      "803 Point(x=102, y=12)\n",
      "804 Point(x=102, y=17)\n",
      "805 Point(x=102, y=22)\n",
      "806 Point(x=102, y=27)\n",
      "807 Point(x=102, y=32)\n",
      "808 Point(x=102, y=37)\n",
      "809 Point(x=102, y=42)\n",
      "810 Point(x=102, y=47)\n",
      "811 Point(x=102, y=52)\n",
      "812 Point(x=102, y=57)\n",
      "813 Point(x=102, y=62)\n",
      "814 Point(x=102, y=67)\n",
      "815 Point(x=102, y=72)\n",
      "816 Point(x=102, y=77)\n",
      "817 Point(x=102, y=82)\n",
      "818 Point(x=102, y=87)\n",
      "819 Point(x=102, y=92)\n",
      "820 Point(x=102, y=97)\n",
      "821 Point(x=102, y=102)\n",
      "822 Point(x=102, y=107)\n",
      "823 Point(x=102, y=112)\n",
      "824 Point(x=102, y=117)\n",
      "825 Point(x=102, y=122)\n",
      "826 Point(x=102, y=127)\n",
      "827 Point(x=102, y=132)\n",
      "828 Point(x=102, y=137)\n",
      "829 Point(x=102, y=142)\n",
      "830 Point(x=102, y=147)\n",
      "831 Point(x=102, y=152)\n",
      "832 Point(x=102, y=157)\n",
      "833 Point(x=102, y=162)\n",
      "834 Point(x=102, y=167)\n",
      "835 Point(x=102, y=172)\n",
      "836 Point(x=102, y=177)\n",
      "837 Point(x=102, y=182)\n",
      "838 Point(x=102, y=187)\n",
      "839 Point(x=102, y=192)\n",
      "840 Point(x=102, y=197)\n",
      "841 Point(x=107, y=2)\n",
      "842 Point(x=107, y=7)\n",
      "843 Point(x=107, y=12)\n",
      "844 Point(x=107, y=17)\n",
      "845 Point(x=107, y=22)\n",
      "846 Point(x=107, y=27)\n",
      "847 Point(x=107, y=32)\n",
      "848 Point(x=107, y=37)\n",
      "849 Point(x=107, y=42)\n",
      "850 Point(x=107, y=47)\n",
      "851 Point(x=107, y=52)\n",
      "852 Point(x=107, y=57)\n",
      "853 Point(x=107, y=62)\n",
      "854 Point(x=107, y=67)\n",
      "855 Point(x=107, y=72)\n",
      "856 Point(x=107, y=77)\n",
      "857 Point(x=107, y=82)\n",
      "858 Point(x=107, y=87)\n",
      "859 Point(x=107, y=92)\n",
      "860 Point(x=107, y=97)\n",
      "861 Point(x=107, y=102)\n",
      "862 Point(x=107, y=107)\n",
      "863 Point(x=107, y=112)\n",
      "864 Point(x=107, y=117)\n",
      "865 Point(x=107, y=122)\n",
      "866 Point(x=107, y=127)\n",
      "867 Point(x=107, y=132)\n",
      "868 Point(x=107, y=137)\n",
      "869 Point(x=107, y=142)\n",
      "870 Point(x=107, y=147)\n",
      "871 Point(x=107, y=152)\n",
      "872 Point(x=107, y=157)\n",
      "873 Point(x=107, y=162)\n",
      "874 Point(x=107, y=167)\n",
      "875 Point(x=107, y=172)\n",
      "876 Point(x=107, y=177)\n",
      "877 Point(x=107, y=182)\n",
      "878 Point(x=107, y=187)\n",
      "879 Point(x=107, y=192)\n",
      "880 Point(x=107, y=197)\n",
      "881 Point(x=112, y=2)\n",
      "882 Point(x=112, y=7)\n",
      "883 Point(x=112, y=12)\n",
      "884 Point(x=112, y=17)\n",
      "885 Point(x=112, y=22)\n",
      "886 Point(x=112, y=27)\n",
      "887 Point(x=112, y=32)\n",
      "888 Point(x=112, y=37)\n",
      "889 Point(x=112, y=42)\n",
      "890 Point(x=112, y=47)\n",
      "891 Point(x=112, y=52)\n",
      "892 Point(x=112, y=57)\n",
      "893 Point(x=112, y=62)\n",
      "894 Point(x=112, y=67)\n",
      "895 Point(x=112, y=72)\n",
      "896 Point(x=112, y=77)\n",
      "897 Point(x=112, y=82)\n",
      "898 Point(x=112, y=87)\n",
      "899 Point(x=112, y=92)\n",
      "900 Point(x=112, y=97)\n",
      "901 Point(x=112, y=102)\n",
      "902 Point(x=112, y=107)\n",
      "903 Point(x=112, y=112)\n",
      "904 Point(x=112, y=117)\n",
      "905 Point(x=112, y=122)\n",
      "906 Point(x=112, y=127)\n",
      "907 Point(x=112, y=132)\n",
      "908 Point(x=112, y=137)\n",
      "909 Point(x=112, y=142)\n",
      "910 Point(x=112, y=147)\n",
      "911 Point(x=112, y=152)\n",
      "912 Point(x=112, y=157)\n",
      "913 Point(x=112, y=162)\n",
      "914 Point(x=112, y=167)\n",
      "915 Point(x=112, y=172)\n",
      "916 Point(x=112, y=177)\n",
      "917 Point(x=112, y=182)\n",
      "918 Point(x=112, y=187)\n",
      "919 Point(x=112, y=192)\n",
      "920 Point(x=112, y=197)\n",
      "921 Point(x=117, y=2)\n",
      "922 Point(x=117, y=7)\n",
      "923 Point(x=117, y=12)\n",
      "924 Point(x=117, y=17)\n",
      "925 Point(x=117, y=22)\n",
      "926 Point(x=117, y=27)\n",
      "927 Point(x=117, y=32)\n",
      "928 Point(x=117, y=37)\n",
      "929 Point(x=117, y=42)\n",
      "930 Point(x=117, y=47)\n",
      "931 Point(x=117, y=52)\n",
      "932 Point(x=117, y=57)\n",
      "933 Point(x=117, y=62)\n",
      "934 Point(x=117, y=67)\n",
      "935 Point(x=117, y=72)\n",
      "936 Point(x=117, y=77)\n",
      "937 Point(x=117, y=82)\n",
      "938 Point(x=117, y=87)\n",
      "939 Point(x=117, y=92)\n",
      "940 Point(x=117, y=97)\n",
      "941 Point(x=117, y=102)\n",
      "942 Point(x=117, y=107)\n",
      "943 Point(x=117, y=112)\n",
      "944 Point(x=117, y=117)\n",
      "945 Point(x=117, y=122)\n",
      "946 Point(x=117, y=127)\n",
      "947 Point(x=117, y=132)\n",
      "948 Point(x=117, y=137)\n",
      "949 Point(x=117, y=142)\n",
      "950 Point(x=117, y=147)\n",
      "951 Point(x=117, y=152)\n",
      "952 Point(x=117, y=157)\n",
      "953 Point(x=117, y=162)\n",
      "954 Point(x=117, y=167)\n",
      "955 Point(x=117, y=172)\n",
      "956 Point(x=117, y=177)\n",
      "957 Point(x=117, y=182)\n",
      "958 Point(x=117, y=187)\n",
      "959 Point(x=117, y=192)\n",
      "960 Point(x=117, y=197)\n",
      "961 Point(x=122, y=2)\n",
      "962 Point(x=122, y=7)\n",
      "963 Point(x=122, y=12)\n",
      "964 Point(x=122, y=17)\n",
      "965 Point(x=122, y=22)\n",
      "966 Point(x=122, y=27)\n",
      "967 Point(x=122, y=32)\n",
      "968 Point(x=122, y=37)\n",
      "969 Point(x=122, y=42)\n",
      "970 Point(x=122, y=47)\n",
      "971 Point(x=122, y=52)\n",
      "972 Point(x=122, y=57)\n",
      "973 Point(x=122, y=62)\n",
      "974 Point(x=122, y=67)\n",
      "975 Point(x=122, y=72)\n",
      "976 Point(x=122, y=77)\n",
      "977 Point(x=122, y=82)\n",
      "978 Point(x=122, y=87)\n",
      "979 Point(x=122, y=92)\n",
      "980 Point(x=122, y=97)\n",
      "981 Point(x=122, y=102)\n",
      "982 Point(x=122, y=107)\n",
      "983 Point(x=122, y=112)\n",
      "984 Point(x=122, y=117)\n",
      "985 Point(x=122, y=122)\n",
      "986 Point(x=122, y=127)\n",
      "987 Point(x=122, y=132)\n",
      "988 Point(x=122, y=137)\n",
      "989 Point(x=122, y=142)\n",
      "990 Point(x=122, y=147)\n",
      "991 Point(x=122, y=152)\n",
      "992 Point(x=122, y=157)\n",
      "993 Point(x=122, y=162)\n",
      "994 Point(x=122, y=167)\n",
      "995 Point(x=122, y=172)\n",
      "996 Point(x=122, y=177)\n",
      "997 Point(x=122, y=182)\n",
      "998 Point(x=122, y=187)\n",
      "999 Point(x=122, y=192)\n",
      "1000 Point(x=122, y=197)\n",
      "1001 Point(x=127, y=2)\n",
      "1002 Point(x=127, y=7)\n",
      "1003 Point(x=127, y=12)\n",
      "1004 Point(x=127, y=17)\n",
      "1005 Point(x=127, y=22)\n",
      "1006 Point(x=127, y=27)\n",
      "1007 Point(x=127, y=32)\n",
      "1008 Point(x=127, y=37)\n",
      "1009 Point(x=127, y=42)\n",
      "1010 Point(x=127, y=47)\n",
      "1011 Point(x=127, y=52)\n",
      "1012 Point(x=127, y=57)\n",
      "1013 Point(x=127, y=62)\n",
      "1014 Point(x=127, y=67)\n",
      "1015 Point(x=127, y=72)\n",
      "1016 Point(x=127, y=77)\n",
      "1017 Point(x=127, y=82)\n",
      "1018 Point(x=127, y=87)\n",
      "1019 Point(x=127, y=92)\n",
      "1020 Point(x=127, y=97)\n",
      "1021 Point(x=127, y=102)\n",
      "1022 Point(x=127, y=107)\n",
      "1023 Point(x=127, y=112)\n",
      "1024 Point(x=127, y=117)\n",
      "1025 Point(x=127, y=122)\n",
      "1026 Point(x=127, y=127)\n",
      "1027 Point(x=127, y=132)\n",
      "1028 Point(x=127, y=137)\n",
      "1029 Point(x=127, y=142)\n",
      "1030 Point(x=127, y=147)\n",
      "1031 Point(x=127, y=152)\n",
      "1032 Point(x=127, y=157)\n",
      "1033 Point(x=127, y=162)\n",
      "1034 Point(x=127, y=167)\n",
      "1035 Point(x=127, y=172)\n",
      "1036 Point(x=127, y=177)\n",
      "1037 Point(x=127, y=182)\n",
      "1038 Point(x=127, y=187)\n",
      "1039 Point(x=127, y=192)\n",
      "1040 Point(x=127, y=197)\n",
      "1041 Point(x=132, y=2)\n",
      "1042 Point(x=132, y=7)\n",
      "1043 Point(x=132, y=12)\n",
      "1044 Point(x=132, y=17)\n",
      "1045 Point(x=132, y=22)\n",
      "1046 Point(x=132, y=27)\n",
      "1047 Point(x=132, y=32)\n",
      "1048 Point(x=132, y=37)\n",
      "1049 Point(x=132, y=42)\n",
      "1050 Point(x=132, y=47)\n",
      "1051 Point(x=132, y=52)\n",
      "1052 Point(x=132, y=57)\n",
      "1053 Point(x=132, y=62)\n",
      "1054 Point(x=132, y=67)\n",
      "1055 Point(x=132, y=72)\n",
      "1056 Point(x=132, y=77)\n",
      "1057 Point(x=132, y=82)\n",
      "1058 Point(x=132, y=87)\n",
      "1059 Point(x=132, y=92)\n",
      "1060 Point(x=132, y=97)\n",
      "1061 Point(x=132, y=102)\n",
      "1062 Point(x=132, y=107)\n",
      "1063 Point(x=132, y=112)\n",
      "1064 Point(x=132, y=117)\n",
      "1065 Point(x=132, y=122)\n",
      "1066 Point(x=132, y=127)\n",
      "1067 Point(x=132, y=132)\n",
      "1068 Point(x=132, y=137)\n",
      "1069 Point(x=132, y=142)\n",
      "1070 Point(x=132, y=147)\n",
      "1071 Point(x=132, y=152)\n",
      "1072 Point(x=132, y=157)\n",
      "1073 Point(x=132, y=162)\n",
      "1074 Point(x=132, y=167)\n",
      "1075 Point(x=132, y=172)\n",
      "1076 Point(x=132, y=177)\n",
      "1077 Point(x=132, y=182)\n",
      "1078 Point(x=132, y=187)\n",
      "1079 Point(x=132, y=192)\n",
      "1080 Point(x=132, y=197)\n",
      "1081 Point(x=137, y=2)\n",
      "1082 Point(x=137, y=7)\n",
      "1083 Point(x=137, y=12)\n",
      "1084 Point(x=137, y=17)\n",
      "1085 Point(x=137, y=22)\n",
      "1086 Point(x=137, y=27)\n",
      "1087 Point(x=137, y=32)\n",
      "1088 Point(x=137, y=37)\n",
      "1089 Point(x=137, y=42)\n",
      "1090 Point(x=137, y=47)\n",
      "1091 Point(x=137, y=52)\n",
      "1092 Point(x=137, y=57)\n",
      "1093 Point(x=137, y=62)\n",
      "1094 Point(x=137, y=67)\n",
      "1095 Point(x=137, y=72)\n",
      "1096 Point(x=137, y=77)\n",
      "1097 Point(x=137, y=82)\n",
      "1098 Point(x=137, y=87)\n",
      "1099 Point(x=137, y=92)\n",
      "1100 Point(x=137, y=97)\n",
      "1101 Point(x=137, y=102)\n",
      "1102 Point(x=137, y=107)\n",
      "1103 Point(x=137, y=112)\n",
      "1104 Point(x=137, y=117)\n",
      "1105 Point(x=137, y=122)\n",
      "1106 Point(x=137, y=127)\n",
      "1107 Point(x=137, y=132)\n",
      "1108 Point(x=137, y=137)\n",
      "1109 Point(x=137, y=142)\n",
      "1110 Point(x=137, y=147)\n",
      "1111 Point(x=137, y=152)\n",
      "1112 Point(x=137, y=157)\n",
      "1113 Point(x=137, y=162)\n",
      "1114 Point(x=137, y=167)\n",
      "1115 Point(x=137, y=172)\n",
      "1116 Point(x=137, y=177)\n",
      "1117 Point(x=137, y=182)\n",
      "1118 Point(x=137, y=187)\n",
      "1119 Point(x=137, y=192)\n",
      "1120 Point(x=137, y=197)\n",
      "1121 Point(x=142, y=2)\n",
      "1122 Point(x=142, y=7)\n",
      "1123 Point(x=142, y=12)\n",
      "1124 Point(x=142, y=17)\n",
      "1125 Point(x=142, y=22)\n",
      "1126 Point(x=142, y=27)\n",
      "1127 Point(x=142, y=32)\n",
      "1128 Point(x=142, y=37)\n",
      "1129 Point(x=142, y=42)\n",
      "1130 Point(x=142, y=47)\n",
      "1131 Point(x=142, y=52)\n",
      "1132 Point(x=142, y=57)\n",
      "1133 Point(x=142, y=62)\n",
      "1134 Point(x=142, y=67)\n",
      "1135 Point(x=142, y=72)\n",
      "1136 Point(x=142, y=77)\n",
      "1137 Point(x=142, y=82)\n",
      "1138 Point(x=142, y=87)\n",
      "1139 Point(x=142, y=92)\n",
      "1140 Point(x=142, y=97)\n",
      "1141 Point(x=142, y=102)\n",
      "1142 Point(x=142, y=107)\n",
      "1143 Point(x=142, y=112)\n",
      "1144 Point(x=142, y=117)\n",
      "1145 Point(x=142, y=122)\n",
      "1146 Point(x=142, y=127)\n",
      "1147 Point(x=142, y=132)\n",
      "1148 Point(x=142, y=137)\n",
      "1149 Point(x=142, y=142)\n",
      "1150 Point(x=142, y=147)\n",
      "1151 Point(x=142, y=152)\n",
      "1152 Point(x=142, y=157)\n",
      "1153 Point(x=142, y=162)\n",
      "1154 Point(x=142, y=167)\n",
      "1155 Point(x=142, y=172)\n",
      "1156 Point(x=142, y=177)\n",
      "1157 Point(x=142, y=182)\n",
      "1158 Point(x=142, y=187)\n",
      "1159 Point(x=142, y=192)\n",
      "1160 Point(x=142, y=197)\n",
      "1161 Point(x=147, y=2)\n",
      "1162 Point(x=147, y=7)\n",
      "1163 Point(x=147, y=12)\n",
      "1164 Point(x=147, y=17)\n",
      "1165 Point(x=147, y=22)\n",
      "1166 Point(x=147, y=27)\n",
      "1167 Point(x=147, y=32)\n",
      "1168 Point(x=147, y=37)\n",
      "1169 Point(x=147, y=42)\n",
      "1170 Point(x=147, y=47)\n",
      "1171 Point(x=147, y=52)\n",
      "1172 Point(x=147, y=57)\n",
      "1173 Point(x=147, y=62)\n",
      "1174 Point(x=147, y=67)\n",
      "1175 Point(x=147, y=72)\n",
      "1176 Point(x=147, y=77)\n",
      "1177 Point(x=147, y=82)\n",
      "1178 Point(x=147, y=87)\n",
      "1179 Point(x=147, y=92)\n",
      "1180 Point(x=147, y=97)\n",
      "1181 Point(x=147, y=102)\n",
      "1182 Point(x=147, y=107)\n",
      "1183 Point(x=147, y=112)\n",
      "1184 Point(x=147, y=117)\n",
      "1185 Point(x=147, y=122)\n",
      "1186 Point(x=147, y=127)\n",
      "1187 Point(x=147, y=132)\n",
      "1188 Point(x=147, y=137)\n",
      "1189 Point(x=147, y=142)\n",
      "1190 Point(x=147, y=147)\n",
      "1191 Point(x=147, y=152)\n",
      "1192 Point(x=147, y=157)\n",
      "1193 Point(x=147, y=162)\n",
      "1194 Point(x=147, y=167)\n",
      "1195 Point(x=147, y=172)\n",
      "1196 Point(x=147, y=177)\n",
      "1197 Point(x=147, y=182)\n",
      "1198 Point(x=147, y=187)\n",
      "1199 Point(x=147, y=192)\n",
      "1200 Point(x=147, y=197)\n",
      "1201 Point(x=152, y=2)\n",
      "1202 Point(x=152, y=7)\n",
      "1203 Point(x=152, y=12)\n",
      "1204 Point(x=152, y=17)\n",
      "1205 Point(x=152, y=22)\n",
      "1206 Point(x=152, y=27)\n",
      "1207 Point(x=152, y=32)\n",
      "1208 Point(x=152, y=37)\n",
      "1209 Point(x=152, y=42)\n",
      "1210 Point(x=152, y=47)\n",
      "1211 Point(x=152, y=52)\n",
      "1212 Point(x=152, y=57)\n",
      "1213 Point(x=152, y=62)\n",
      "1214 Point(x=152, y=67)\n",
      "1215 Point(x=152, y=72)\n",
      "1216 Point(x=152, y=77)\n",
      "1217 Point(x=152, y=82)\n",
      "1218 Point(x=152, y=87)\n",
      "1219 Point(x=152, y=92)\n",
      "1220 Point(x=152, y=97)\n",
      "1221 Point(x=152, y=102)\n",
      "1222 Point(x=152, y=107)\n",
      "1223 Point(x=152, y=112)\n",
      "1224 Point(x=152, y=117)\n",
      "1225 Point(x=152, y=122)\n",
      "1226 Point(x=152, y=127)\n",
      "1227 Point(x=152, y=132)\n",
      "1228 Point(x=152, y=137)\n",
      "1229 Point(x=152, y=142)\n",
      "1230 Point(x=152, y=147)\n",
      "1231 Point(x=152, y=152)\n",
      "1232 Point(x=152, y=157)\n",
      "1233 Point(x=152, y=162)\n",
      "1234 Point(x=152, y=167)\n",
      "1235 Point(x=152, y=172)\n",
      "1236 Point(x=152, y=177)\n",
      "1237 Point(x=152, y=182)\n",
      "1238 Point(x=152, y=187)\n",
      "1239 Point(x=152, y=192)\n",
      "1240 Point(x=152, y=197)\n",
      "1241 Point(x=157, y=2)\n",
      "1242 Point(x=157, y=7)\n",
      "1243 Point(x=157, y=12)\n",
      "1244 Point(x=157, y=17)\n",
      "1245 Point(x=157, y=22)\n",
      "1246 Point(x=157, y=27)\n",
      "1247 Point(x=157, y=32)\n",
      "1248 Point(x=157, y=37)\n",
      "1249 Point(x=157, y=42)\n",
      "1250 Point(x=157, y=47)\n",
      "1251 Point(x=157, y=52)\n",
      "1252 Point(x=157, y=57)\n",
      "1253 Point(x=157, y=62)\n",
      "1254 Point(x=157, y=67)\n",
      "1255 Point(x=157, y=72)\n",
      "1256 Point(x=157, y=77)\n",
      "1257 Point(x=157, y=82)\n",
      "1258 Point(x=157, y=87)\n",
      "1259 Point(x=157, y=92)\n",
      "1260 Point(x=157, y=97)\n",
      "1261 Point(x=157, y=102)\n",
      "1262 Point(x=157, y=107)\n",
      "1263 Point(x=157, y=112)\n",
      "1264 Point(x=157, y=117)\n",
      "1265 Point(x=157, y=122)\n",
      "1266 Point(x=157, y=127)\n",
      "1267 Point(x=157, y=132)\n",
      "1268 Point(x=157, y=137)\n",
      "1269 Point(x=157, y=142)\n",
      "1270 Point(x=157, y=147)\n",
      "1271 Point(x=157, y=152)\n",
      "1272 Point(x=157, y=157)\n",
      "1273 Point(x=157, y=162)\n",
      "1274 Point(x=157, y=167)\n",
      "1275 Point(x=157, y=172)\n",
      "1276 Point(x=157, y=177)\n",
      "1277 Point(x=157, y=182)\n",
      "1278 Point(x=157, y=187)\n",
      "1279 Point(x=157, y=192)\n",
      "1280 Point(x=157, y=197)\n",
      "1281 Point(x=162, y=2)\n",
      "1282 Point(x=162, y=7)\n",
      "1283 Point(x=162, y=12)\n",
      "1284 Point(x=162, y=17)\n",
      "1285 Point(x=162, y=22)\n",
      "1286 Point(x=162, y=27)\n",
      "1287 Point(x=162, y=32)\n",
      "1288 Point(x=162, y=37)\n",
      "1289 Point(x=162, y=42)\n",
      "1290 Point(x=162, y=47)\n",
      "1291 Point(x=162, y=52)\n",
      "1292 Point(x=162, y=57)\n",
      "1293 Point(x=162, y=62)\n",
      "1294 Point(x=162, y=67)\n",
      "1295 Point(x=162, y=72)\n",
      "1296 Point(x=162, y=77)\n",
      "1297 Point(x=162, y=82)\n",
      "1298 Point(x=162, y=87)\n",
      "1299 Point(x=162, y=92)\n",
      "1300 Point(x=162, y=97)\n",
      "1301 Point(x=162, y=102)\n",
      "1302 Point(x=162, y=107)\n",
      "1303 Point(x=162, y=112)\n",
      "1304 Point(x=162, y=117)\n",
      "1305 Point(x=162, y=122)\n",
      "1306 Point(x=162, y=127)\n",
      "1307 Point(x=162, y=132)\n",
      "1308 Point(x=162, y=137)\n",
      "1309 Point(x=162, y=142)\n",
      "1310 Point(x=162, y=147)\n",
      "1311 Point(x=162, y=152)\n",
      "1312 Point(x=162, y=157)\n",
      "1313 Point(x=162, y=162)\n",
      "1314 Point(x=162, y=167)\n",
      "1315 Point(x=162, y=172)\n",
      "1316 Point(x=162, y=177)\n",
      "1317 Point(x=162, y=182)\n",
      "1318 Point(x=162, y=187)\n",
      "1319 Point(x=162, y=192)\n",
      "1320 Point(x=162, y=197)\n",
      "1321 Point(x=167, y=2)\n",
      "1322 Point(x=167, y=7)\n",
      "1323 Point(x=167, y=12)\n",
      "1324 Point(x=167, y=17)\n",
      "1325 Point(x=167, y=22)\n",
      "1326 Point(x=167, y=27)\n",
      "1327 Point(x=167, y=32)\n",
      "1328 Point(x=167, y=37)\n",
      "1329 Point(x=167, y=42)\n",
      "1330 Point(x=167, y=47)\n",
      "1331 Point(x=167, y=52)\n",
      "1332 Point(x=167, y=57)\n",
      "1333 Point(x=167, y=62)\n",
      "1334 Point(x=167, y=67)\n",
      "1335 Point(x=167, y=72)\n",
      "1336 Point(x=167, y=77)\n",
      "1337 Point(x=167, y=82)\n",
      "1338 Point(x=167, y=87)\n",
      "1339 Point(x=167, y=92)\n",
      "1340 Point(x=167, y=97)\n",
      "1341 Point(x=167, y=102)\n",
      "1342 Point(x=167, y=107)\n",
      "1343 Point(x=167, y=112)\n",
      "1344 Point(x=167, y=117)\n",
      "1345 Point(x=167, y=122)\n",
      "1346 Point(x=167, y=127)\n",
      "1347 Point(x=167, y=132)\n",
      "1348 Point(x=167, y=137)\n",
      "1349 Point(x=167, y=142)\n",
      "1350 Point(x=167, y=147)\n",
      "1351 Point(x=167, y=152)\n",
      "1352 Point(x=167, y=157)\n",
      "1353 Point(x=167, y=162)\n",
      "1354 Point(x=167, y=167)\n",
      "1355 Point(x=167, y=172)\n",
      "1356 Point(x=167, y=177)\n",
      "1357 Point(x=167, y=182)\n",
      "1358 Point(x=167, y=187)\n",
      "1359 Point(x=167, y=192)\n",
      "1360 Point(x=167, y=197)\n",
      "1361 Point(x=172, y=2)\n",
      "1362 Point(x=172, y=7)\n",
      "1363 Point(x=172, y=12)\n",
      "1364 Point(x=172, y=17)\n",
      "1365 Point(x=172, y=22)\n",
      "1366 Point(x=172, y=27)\n",
      "1367 Point(x=172, y=32)\n",
      "1368 Point(x=172, y=37)\n",
      "1369 Point(x=172, y=42)\n",
      "1370 Point(x=172, y=47)\n",
      "1371 Point(x=172, y=52)\n",
      "1372 Point(x=172, y=57)\n",
      "1373 Point(x=172, y=62)\n",
      "1374 Point(x=172, y=67)\n",
      "1375 Point(x=172, y=72)\n",
      "1376 Point(x=172, y=77)\n",
      "1377 Point(x=172, y=82)\n",
      "1378 Point(x=172, y=87)\n",
      "1379 Point(x=172, y=92)\n",
      "1380 Point(x=172, y=97)\n",
      "1381 Point(x=172, y=102)\n",
      "1382 Point(x=172, y=107)\n",
      "1383 Point(x=172, y=112)\n",
      "1384 Point(x=172, y=117)\n",
      "1385 Point(x=172, y=122)\n",
      "1386 Point(x=172, y=127)\n",
      "1387 Point(x=172, y=132)\n",
      "1388 Point(x=172, y=137)\n",
      "1389 Point(x=172, y=142)\n",
      "1390 Point(x=172, y=147)\n",
      "1391 Point(x=172, y=152)\n",
      "1392 Point(x=172, y=157)\n",
      "1393 Point(x=172, y=162)\n",
      "1394 Point(x=172, y=167)\n",
      "1395 Point(x=172, y=172)\n",
      "1396 Point(x=172, y=177)\n",
      "1397 Point(x=172, y=182)\n",
      "1398 Point(x=172, y=187)\n",
      "1399 Point(x=172, y=192)\n",
      "1400 Point(x=172, y=197)\n",
      "1401 Point(x=177, y=2)\n",
      "1402 Point(x=177, y=7)\n",
      "1403 Point(x=177, y=12)\n",
      "1404 Point(x=177, y=17)\n",
      "1405 Point(x=177, y=22)\n",
      "1406 Point(x=177, y=27)\n",
      "1407 Point(x=177, y=32)\n",
      "1408 Point(x=177, y=37)\n",
      "1409 Point(x=177, y=42)\n",
      "1410 Point(x=177, y=47)\n",
      "1411 Point(x=177, y=52)\n",
      "1412 Point(x=177, y=57)\n",
      "1413 Point(x=177, y=62)\n",
      "1414 Point(x=177, y=67)\n",
      "1415 Point(x=177, y=72)\n",
      "1416 Point(x=177, y=77)\n",
      "1417 Point(x=177, y=82)\n",
      "1418 Point(x=177, y=87)\n",
      "1419 Point(x=177, y=92)\n",
      "1420 Point(x=177, y=97)\n",
      "1421 Point(x=177, y=102)\n",
      "1422 Point(x=177, y=107)\n",
      "1423 Point(x=177, y=112)\n",
      "1424 Point(x=177, y=117)\n",
      "1425 Point(x=177, y=122)\n",
      "1426 Point(x=177, y=127)\n",
      "1427 Point(x=177, y=132)\n",
      "1428 Point(x=177, y=137)\n",
      "1429 Point(x=177, y=142)\n",
      "1430 Point(x=177, y=147)\n",
      "1431 Point(x=177, y=152)\n",
      "1432 Point(x=177, y=157)\n",
      "1433 Point(x=177, y=162)\n",
      "1434 Point(x=177, y=167)\n",
      "1435 Point(x=177, y=172)\n",
      "1436 Point(x=177, y=177)\n",
      "1437 Point(x=177, y=182)\n",
      "1438 Point(x=177, y=187)\n",
      "1439 Point(x=177, y=192)\n",
      "1440 Point(x=177, y=197)\n",
      "1441 Point(x=182, y=2)\n",
      "1442 Point(x=182, y=7)\n",
      "1443 Point(x=182, y=12)\n",
      "1444 Point(x=182, y=17)\n",
      "1445 Point(x=182, y=22)\n",
      "1446 Point(x=182, y=27)\n",
      "1447 Point(x=182, y=32)\n",
      "1448 Point(x=182, y=37)\n",
      "1449 Point(x=182, y=42)\n",
      "1450 Point(x=182, y=47)\n",
      "1451 Point(x=182, y=52)\n",
      "1452 Point(x=182, y=57)\n",
      "1453 Point(x=182, y=62)\n",
      "1454 Point(x=182, y=67)\n",
      "1455 Point(x=182, y=72)\n",
      "1456 Point(x=182, y=77)\n",
      "1457 Point(x=182, y=82)\n",
      "1458 Point(x=182, y=87)\n",
      "1459 Point(x=182, y=92)\n",
      "1460 Point(x=182, y=97)\n",
      "1461 Point(x=182, y=102)\n",
      "1462 Point(x=182, y=107)\n",
      "1463 Point(x=182, y=112)\n",
      "1464 Point(x=182, y=117)\n",
      "1465 Point(x=182, y=122)\n",
      "1466 Point(x=182, y=127)\n",
      "1467 Point(x=182, y=132)\n",
      "1468 Point(x=182, y=137)\n",
      "1469 Point(x=182, y=142)\n",
      "1470 Point(x=182, y=147)\n",
      "1471 Point(x=182, y=152)\n",
      "1472 Point(x=182, y=157)\n",
      "1473 Point(x=182, y=162)\n",
      "1474 Point(x=182, y=167)\n",
      "1475 Point(x=182, y=172)\n",
      "1476 Point(x=182, y=177)\n",
      "1477 Point(x=182, y=182)\n",
      "1478 Point(x=182, y=187)\n",
      "1479 Point(x=182, y=192)\n",
      "1480 Point(x=182, y=197)\n",
      "1481 Point(x=187, y=2)\n",
      "1482 Point(x=187, y=7)\n",
      "1483 Point(x=187, y=12)\n",
      "1484 Point(x=187, y=17)\n",
      "1485 Point(x=187, y=22)\n",
      "1486 Point(x=187, y=27)\n",
      "1487 Point(x=187, y=32)\n",
      "1488 Point(x=187, y=37)\n",
      "1489 Point(x=187, y=42)\n",
      "1490 Point(x=187, y=47)\n",
      "1491 Point(x=187, y=52)\n",
      "1492 Point(x=187, y=57)\n",
      "1493 Point(x=187, y=62)\n",
      "1494 Point(x=187, y=67)\n",
      "1495 Point(x=187, y=72)\n",
      "1496 Point(x=187, y=77)\n",
      "1497 Point(x=187, y=82)\n",
      "1498 Point(x=187, y=87)\n",
      "1499 Point(x=187, y=92)\n",
      "1500 Point(x=187, y=97)\n",
      "1501 Point(x=187, y=102)\n",
      "1502 Point(x=187, y=107)\n",
      "1503 Point(x=187, y=112)\n",
      "1504 Point(x=187, y=117)\n",
      "1505 Point(x=187, y=122)\n",
      "1506 Point(x=187, y=127)\n",
      "1507 Point(x=187, y=132)\n",
      "1508 Point(x=187, y=137)\n",
      "1509 Point(x=187, y=142)\n",
      "1510 Point(x=187, y=147)\n",
      "1511 Point(x=187, y=152)\n",
      "1512 Point(x=187, y=157)\n",
      "1513 Point(x=187, y=162)\n",
      "1514 Point(x=187, y=167)\n",
      "1515 Point(x=187, y=172)\n",
      "1516 Point(x=187, y=177)\n",
      "1517 Point(x=187, y=182)\n",
      "1518 Point(x=187, y=187)\n",
      "1519 Point(x=187, y=192)\n",
      "1520 Point(x=187, y=197)\n",
      "1521 Point(x=192, y=2)\n",
      "1522 Point(x=192, y=7)\n",
      "1523 Point(x=192, y=12)\n",
      "1524 Point(x=192, y=17)\n",
      "1525 Point(x=192, y=22)\n",
      "1526 Point(x=192, y=27)\n",
      "1527 Point(x=192, y=32)\n",
      "1528 Point(x=192, y=37)\n",
      "1529 Point(x=192, y=42)\n",
      "1530 Point(x=192, y=47)\n",
      "1531 Point(x=192, y=52)\n",
      "1532 Point(x=192, y=57)\n",
      "1533 Point(x=192, y=62)\n",
      "1534 Point(x=192, y=67)\n",
      "1535 Point(x=192, y=72)\n",
      "1536 Point(x=192, y=77)\n",
      "1537 Point(x=192, y=82)\n",
      "1538 Point(x=192, y=87)\n",
      "1539 Point(x=192, y=92)\n",
      "1540 Point(x=192, y=97)\n",
      "1541 Point(x=192, y=102)\n",
      "1542 Point(x=192, y=107)\n",
      "1543 Point(x=192, y=112)\n",
      "1544 Point(x=192, y=117)\n",
      "1545 Point(x=192, y=122)\n",
      "1546 Point(x=192, y=127)\n",
      "1547 Point(x=192, y=132)\n",
      "1548 Point(x=192, y=137)\n",
      "1549 Point(x=192, y=142)\n",
      "1550 Point(x=192, y=147)\n",
      "1551 Point(x=192, y=152)\n",
      "1552 Point(x=192, y=157)\n",
      "1553 Point(x=192, y=162)\n",
      "1554 Point(x=192, y=167)\n",
      "1555 Point(x=192, y=172)\n",
      "1556 Point(x=192, y=177)\n",
      "1557 Point(x=192, y=182)\n",
      "1558 Point(x=192, y=187)\n",
      "1559 Point(x=192, y=192)\n",
      "1560 Point(x=192, y=197)\n",
      "1561 Point(x=197, y=2)\n",
      "1562 Point(x=197, y=7)\n",
      "1563 Point(x=197, y=12)\n",
      "1564 Point(x=197, y=17)\n",
      "1565 Point(x=197, y=22)\n",
      "1566 Point(x=197, y=27)\n",
      "1567 Point(x=197, y=32)\n",
      "1568 Point(x=197, y=37)\n",
      "1569 Point(x=197, y=42)\n",
      "1570 Point(x=197, y=47)\n",
      "1571 Point(x=197, y=52)\n",
      "1572 Point(x=197, y=57)\n",
      "1573 Point(x=197, y=62)\n",
      "1574 Point(x=197, y=67)\n",
      "1575 Point(x=197, y=72)\n",
      "1576 Point(x=197, y=77)\n",
      "1577 Point(x=197, y=82)\n",
      "1578 Point(x=197, y=87)\n",
      "1579 Point(x=197, y=92)\n",
      "1580 Point(x=197, y=97)\n",
      "1581 Point(x=197, y=102)\n",
      "1582 Point(x=197, y=107)\n",
      "1583 Point(x=197, y=112)\n",
      "1584 Point(x=197, y=117)\n",
      "1585 Point(x=197, y=122)\n",
      "1586 Point(x=197, y=127)\n",
      "1587 Point(x=197, y=132)\n",
      "1588 Point(x=197, y=137)\n",
      "1589 Point(x=197, y=142)\n",
      "1590 Point(x=197, y=147)\n",
      "1591 Point(x=197, y=152)\n",
      "1592 Point(x=197, y=157)\n",
      "1593 Point(x=197, y=162)\n",
      "1594 Point(x=197, y=167)\n",
      "1595 Point(x=197, y=172)\n",
      "1596 Point(x=197, y=177)\n",
      "1597 Point(x=197, y=182)\n",
      "1598 Point(x=197, y=187)\n",
      "1599 Point(x=197, y=192)\n",
      "1600 Point(x=197, y=197)\n"
     ]
    }
   ],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, point in enumerate(sensors_location):\n",
    "    print(idx+1, point,\"close\") if math.sqrt((point.x-917)**2+(point.y-415)**2)<=1.5 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = [0, 0, 0, 0]\n",
    "idxx = [[],[],[],[]]\n",
    "for i in range(data_reg.shape[0]):\n",
    "    pus_c = int(data_reg[i][0]) * 3 + 1\n",
    "    idx = int(data_reg[i][pus_c]) - 1\n",
    "    count[idx] += 1\n",
    "    idxx[idx].append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(count)\n",
    "print(idxx[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "imm = read_image(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 200, 200)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imm[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reg[:,-1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 54.,  81.,   0.,   4.,  14.,  90., -15.,  16.,  85., -10.,  11.,\n",
       "        16.,  -5.,  54.,  81.,   0.])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[3][sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f0420366f50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 4320x4320 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(60,60))         # Sample figsize in inches\n",
    "sns.heatmap(imm[0][0], ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (3, 3), (2,2)\n",
    "    cnn = models.Sequential()\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "                          input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(2*num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                         kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                         kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer='lecun_normal'))\n",
    "    cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "    cnn.add(BatchNormalization(axis=1))\n",
    "    \n",
    "    if max(max_x, max_y) == 200:\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "#     cnn.add(layers.Dropout(0.25))\n",
    "# from here for 1000\n",
    "    if max(max_x, max_y) == 1000:\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "\n",
    "        cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "                             kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init))\n",
    "    #     cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "    #                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "    #                          kernel_initializer='lecun_normal'))\n",
    "        cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "        cnn.add(BatchNormalization(axis=1))\n",
    "    #     cnn.add(layers.Dropout(0.25))\n",
    "    \n",
    "    cnn.add(layers.Flatten())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization())\n",
    "    cnn.add(layers.Dense(3, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                         bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init))\n",
    "    return cnn\n",
    "\n",
    "\n",
    "class DataBatchGenerator(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, SENSOR_NUM:int, STATIC:bool=False):\n",
    "#         print(dataset.shape[0])\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.STATIC = STATIC\n",
    "        self.SENSOR_NUM = SENSOR_NUM\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.number_image_channels, self.max_x, self.max_y), dtype=self.float_memory_used)\n",
    "        batch_y = np.empty((size, 3), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            y_offset = (self.SENSOR_NUM if self.STATIC else int(self.dataset[idx * self.batch_size + i][0])*3+1)\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i)\n",
    "            batch_y[i][0] = self.dataset[idx * self.batch_size + i][y_offset] #x\n",
    "            batch_y[i][1] = self.dataset[idx * self.batch_size + i][y_offset + 1] #y\n",
    "            batch_y[i][2] = self.dataset[idx * self.batch_size + i][y_offset + 2] #p\n",
    "        return batch_x, batch_y\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def location_mae(y_true, y_pred):\n",
    "    res = y_pred[:,:2] - y_true[:,:2]\n",
    "    return K.mean(K.sqrt(K.square(res)))\n",
    "\n",
    "def power_mae(y_true, y_pred):\n",
    "    res = y_pred[:,-1] - y_true[:,-1]\n",
    "    return K.mean(K.abs(res))\n",
    "\n",
    "def total_mae(y_true, y_pred):\n",
    "    return location_mae(y_true, y_pred) + power_mae(y_true, y_pred)\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 200, 200)      100       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 10, 100, 100)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 10, 100, 100)      40        \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 20, 100, 100)      1820      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 20, 50, 50)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 20, 50, 50)        80        \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 30, 50, 50)        5430      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 30, 25, 25)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 30, 25, 25)        120       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 40, 25, 25)        10840     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 40, 12, 12)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 40, 12, 12)        160       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 40, 12, 12)        14440     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 40, 6, 6)          0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 40, 6, 6)          160       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1440)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 20)                28820     \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 20)                80        \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 20)                420       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 63        \n",
      "=================================================================\n",
      "Total params: 62,573\n",
      "Trainable params: 62,253\n",
      "Non-trainable params: 320\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-108-17bb7203622b>:1: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [8192*2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 16384 , New samples: 16384\n",
      "Validation size: 5407 , starts: 16384 , ends: 21790\n",
      "\n",
      "Epoch 00001: val_total_mae improved from inf to 109.08794, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_total_mae did not improve from 109.08794\n",
      "\n",
      "Epoch 00003: val_total_mae improved from 109.08794 to 108.58304, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_total_mae improved from 108.58304 to 101.49221, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00005: val_total_mae improved from 101.49221 to 64.72574, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00006: val_total_mae improved from 64.72574 to 48.35625, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_total_mae did not improve from 48.35625\n",
      "\n",
      "Epoch 00008: val_total_mae did not improve from 48.35625\n",
      "\n",
      "Epoch 00009: val_total_mae did not improve from 48.35625\n",
      "\n",
      "Epoch 00010: val_total_mae improved from 48.35625 to 40.53482, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00011: val_total_mae improved from 40.53482 to 36.90274, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_total_mae improved from 36.90274 to 31.79640, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_total_mae did not improve from 31.79640\n",
      "\n",
      "Epoch 00014: val_total_mae improved from 31.79640 to 28.48242, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00015: val_total_mae did not improve from 28.48242\n",
      "\n",
      "Epoch 00016: val_total_mae improved from 28.48242 to 28.33077, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00017: val_total_mae improved from 28.33077 to 27.40422, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00018: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00019: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00020: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00021: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00022: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00023: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00024: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00025: val_total_mae did not improve from 27.40422\n",
      "\n",
      "Epoch 00026: val_total_mae improved from 27.40422 to 21.13284, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00027: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00028: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00029: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00030: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00031: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00032: val_total_mae did not improve from 21.13284\n",
      "\n",
      "Epoch 00033: val_total_mae improved from 21.13284 to 20.32266, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_total_mae did not improve from 20.32266\n",
      "\n",
      "Epoch 00035: val_total_mae did not improve from 20.32266\n",
      "\n",
      "Epoch 00036: val_total_mae did not improve from 20.32266\n",
      "\n",
      "Epoch 00037: val_total_mae did not improve from 20.32266\n",
      "\n",
      "Epoch 00038: val_total_mae did not improve from 20.32266\n",
      "\n",
      "Epoch 00039: val_total_mae improved from 20.32266 to 20.15009, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00041: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00042: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00043: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00044: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00045: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00046: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00047: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00048: val_total_mae did not improve from 20.15009\n",
      "\n",
      "Epoch 00049: val_total_mae improved from 20.15009 to 19.97276, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00050: val_total_mae did not improve from 19.97276\n",
      "\n",
      "Epoch 00051: val_total_mae did not improve from 19.97276\n",
      "\n",
      "Epoch 00052: val_total_mae improved from 19.97276 to 19.21922, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_total_mae did not improve from 19.21922\n",
      "\n",
      "Epoch 00054: val_total_mae improved from 19.21922 to 19.04231, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00055: val_total_mae did not improve from 19.04231\n",
      "\n",
      "Epoch 00056: val_total_mae did not improve from 19.04231\n",
      "\n",
      "Epoch 00057: val_total_mae did not improve from 19.04231\n",
      "\n",
      "Epoch 00058: val_total_mae did not improve from 19.04231\n",
      "\n",
      "Epoch 00059: val_total_mae did not improve from 19.04231\n",
      "\n",
      "Epoch 00060: val_total_mae improved from 19.04231 to 18.76959, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00061: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00062: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00063: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00064: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00065: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00066: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00067: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00068: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00069: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00070: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00071: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00072: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00073: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00074: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00075: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00076: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00077: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00078: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00079: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00080: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00081: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00082: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00083: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00084: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00085: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00086: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00087: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00088: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00089: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00090: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00091: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00092: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00093: val_total_mae did not improve from 18.76959\n",
      "\n",
      "Epoch 00094: val_total_mae did not improve from 18.76959\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_total_mae improved from 18.76959 to 18.47756, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00096: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00097: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00098: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00099: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00100: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00101: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00102: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00103: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00104: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00105: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00106: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00107: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00108: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00109: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00110: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00111: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00112: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00113: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00114: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00115: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00116: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00117: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00118: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00119: val_total_mae did not improve from 18.47756\n",
      "\n",
      "Epoch 00120: val_total_mae improved from 18.47756 to 17.98044, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00121: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00122: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00123: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00124: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00125: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00126: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00127: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00128: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00129: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00130: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00131: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00132: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00133: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00134: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00135: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00136: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00137: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00138: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00139: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00140: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00141: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00142: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00143: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00144: val_total_mae did not improve from 17.98044\n",
      "\n",
      "Epoch 00145: val_total_mae improved from 17.98044 to 17.93298, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00146: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00147: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00148: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00149: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00150: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00151: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00152: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00153: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00154: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00155: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00156: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00157: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00158: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00159: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00160: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00161: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00162: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00163: val_total_mae did not improve from 17.93298\n",
      "\n",
      "Epoch 00164: val_total_mae improved from 17.93298 to 17.67235, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00165: val_total_mae did not improve from 17.67235\n",
      "\n",
      "Epoch 00166: val_total_mae did not improve from 17.67235\n",
      "\n",
      "Epoch 00167: val_total_mae improved from 17.67235 to 17.66802, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00168: val_total_mae did not improve from 17.66802\n",
      "\n",
      "Epoch 00169: val_total_mae did not improve from 17.66802\n",
      "\n",
      "Epoch 00170: val_total_mae did not improve from 17.66802\n",
      "\n",
      "Epoch 00171: val_total_mae did not improve from 17.66802\n",
      "\n",
      "Epoch 00172: val_total_mae did not improve from 17.66802\n",
      "\n",
      "Epoch 00173: val_total_mae improved from 17.66802 to 17.33575, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00174: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00175: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00176: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00177: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00178: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00179: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00180: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00181: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00182: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00183: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00184: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00185: val_total_mae did not improve from 17.33575\n",
      "\n",
      "Epoch 00186: val_total_mae improved from 17.33575 to 17.19712, saving model to models/pictures_200_200/log/static_point_sensors/raw_power_min_max_norm/gray/1600sensors/models/16384/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00187: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00188: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00189: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00190: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00191: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00192: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00193: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00194: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00195: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00196: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00197: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00198: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00199: val_total_mae did not improve from 17.19712\n",
      "\n",
      "Epoch 00200: val_total_mae did not improve from 17.19712\n",
      "\n",
      "\n",
      "Lambda: 0.1 , Time: 0:55:24\n",
      "\n",
      "Train Error(all epochs): 3.873552083969116 \n",
      " [69.406, 68.158, 64.465, 56.861, 46.203, 36.878, 31.117, 27.604, 24.908, 22.38, 20.115, 18.242, 16.788, 15.772, 14.868, 14.138, 13.657, 13.105, 12.72, 12.251, 11.938, 11.709, 11.39, 11.042, 10.906, 10.587, 10.442, 10.178, 9.945, 9.801, 9.51, 9.453, 9.275, 9.139, 8.913, 8.862, 8.709, 8.473, 8.435, 8.392, 8.209, 8.078, 7.93, 7.746, 7.606, 7.556, 7.365, 7.373, 7.292, 7.186, 7.099, 6.98, 7.039, 6.868, 6.764, 6.68, 6.598, 6.587, 6.528, 6.464, 6.461, 6.315, 6.293, 6.211, 6.109, 6.002, 6.116, 6.022, 6.145, 5.946, 5.896, 5.825, 5.809, 5.826, 5.71, 5.667, 5.663, 5.606, 5.557, 5.496, 5.497, 5.394, 5.509, 5.487, 5.438, 5.368, 5.31, 5.339, 5.184, 5.082, 5.178, 5.232, 5.194, 5.129, 5.136, 4.932, 5.098, 5.091, 5.151, 5.191, 4.994, 4.968, 4.888, 4.953, 4.943, 4.808, 4.864, 4.817, 4.839, 4.739, 4.623, 4.65, 4.723, 4.799, 4.746, 4.784, 4.681, 4.749, 4.57, 4.612, 4.647, 4.657, 4.583, 4.54, 4.606, 4.675, 4.665, 4.52, 4.61, 4.561, 4.461, 4.39, 4.53, 4.439, 4.438, 4.555, 4.542, 4.396, 4.409, 4.327, 4.335, 4.42, 4.411, 4.512, 4.485, 4.345, 4.232, 4.173, 4.18, 4.184, 4.262, 4.225, 4.239, 4.231, 4.297, 4.337, 4.28, 4.22, 4.14, 4.149, 4.159, 4.255, 4.374, 4.254, 4.269, 4.247, 4.138, 4.255, 4.303, 4.201, 4.055, 4.091, 4.117, 4.265, 4.293, 4.217, 4.101, 3.995, 3.994, 4.025, 3.941, 3.972, 3.951, 4.012, 3.974, 4.071, 4.03, 4.194, 4.126, 4.174, 4.098, 4.09, 4.031, 4.035, 4.032, 4.02, 4.034, 4.035, 3.924, 3.874]\n",
      "\n",
      "Train Total Error(all epochs): 6.646631717681885 \n",
      " [108.407, 106.266, 100.426, 88.688, 72.42, 58.352, 49.61, 44.245, 40.194, 36.353, 32.833, 29.894, 27.634, 26.057, 24.561, 23.373, 22.607, 21.732, 21.093, 20.312, 19.773, 19.342, 18.785, 18.232, 17.963, 17.445, 17.198, 16.758, 16.386, 16.121, 15.666, 15.554, 15.258, 15.027, 14.667, 14.562, 14.326, 13.945, 13.878, 13.797, 13.505, 13.3, 13.058, 12.78, 12.562, 12.475, 12.184, 12.174, 12.058, 11.889, 11.762, 11.56, 11.658, 11.384, 11.231, 11.106, 10.981, 10.958, 10.862, 10.762, 10.757, 10.54, 10.494, 10.377, 10.219, 10.052, 10.227, 10.077, 10.264, 9.967, 9.892, 9.775, 9.76, 9.774, 9.601, 9.537, 9.524, 9.441, 9.36, 9.262, 9.27, 9.112, 9.283, 9.244, 9.163, 9.059, 8.964, 9.009, 8.781, 8.624, 8.766, 8.843, 8.79, 8.684, 8.705, 8.384, 8.639, 8.617, 8.707, 8.765, 8.474, 8.428, 8.308, 8.399, 8.382, 8.18, 8.261, 8.188, 8.222, 8.067, 7.886, 7.923, 8.031, 8.154, 8.071, 8.124, 7.964, 8.061, 7.792, 7.853, 7.901, 7.925, 7.808, 7.738, 7.84, 7.934, 7.926, 7.707, 7.84, 7.759, 7.609, 7.503, 7.71, 7.577, 7.571, 7.748, 7.723, 7.501, 7.52, 7.388, 7.405, 7.536, 7.519, 7.664, 7.632, 7.415, 7.242, 7.159, 7.163, 7.169, 7.273, 7.231, 7.244, 7.237, 7.321, 7.387, 7.295, 7.205, 7.084, 7.088, 7.111, 7.253, 7.434, 7.255, 7.267, 7.23, 7.068, 7.246, 7.315, 7.163, 6.937, 6.996, 7.033, 7.255, 7.304, 7.181, 7.008, 6.844, 6.843, 6.883, 6.762, 6.8, 6.763, 6.861, 6.8, 6.954, 6.888, 7.13, 7.028, 7.106, 6.988, 6.978, 6.89, 6.887, 6.879, 6.864, 6.875, 6.894, 6.726, 6.647]\n",
      "\n",
      "Train Location Error(all epochs): 4.974024295806885 \n",
      " [99.81, 98.209, 92.968, 81.895, 66.188, 52.283, 43.739, 38.568, 34.531, 30.786, 27.511, 24.833, 22.731, 21.259, 20.043, 19.04, 18.363, 17.582, 17.066, 16.442, 16.041, 15.784, 15.385, 14.893, 14.755, 14.316, 14.129, 13.776, 13.45, 13.282, 12.865, 12.805, 12.566, 12.391, 12.071, 12.024, 11.802, 11.476, 11.427, 11.38, 11.122, 10.933, 10.733, 10.457, 10.256, 10.193, 9.91, 9.944, 9.818, 9.668, 9.534, 9.381, 9.46, 9.219, 9.062, 8.932, 8.812, 8.804, 8.722, 8.631, 8.625, 8.404, 8.385, 8.256, 8.107, 7.955, 8.122, 7.988, 8.172, 7.872, 7.796, 7.701, 7.669, 7.706, 7.529, 7.466, 7.464, 7.377, 7.311, 7.224, 7.22, 7.071, 7.245, 7.218, 7.151, 7.045, 6.966, 7.008, 6.771, 6.624, 6.768, 6.852, 6.792, 6.702, 6.702, 6.414, 6.653, 6.657, 6.747, 6.809, 6.51, 6.477, 6.357, 6.459, 6.447, 6.244, 6.332, 6.262, 6.294, 6.149, 5.983, 6.026, 6.139, 6.244, 6.168, 6.229, 6.078, 6.185, 5.918, 5.983, 6.04, 6.047, 5.941, 5.882, 5.978, 6.09, 6.069, 5.852, 5.99, 5.925, 5.775, 5.666, 5.879, 5.738, 5.742, 5.917, 5.902, 5.687, 5.709, 5.593, 5.601, 5.723, 5.713, 5.872, 5.823, 5.62, 5.455, 5.36, 5.376, 5.384, 5.514, 5.445, 5.472, 5.457, 5.569, 5.623, 5.545, 5.456, 5.337, 5.359, 5.366, 5.512, 5.687, 5.508, 5.54, 5.511, 5.347, 5.518, 5.594, 5.438, 5.229, 5.277, 5.319, 5.541, 5.575, 5.471, 5.295, 5.14, 5.137, 5.191, 5.062, 5.116, 5.09, 5.176, 5.121, 5.26, 5.202, 5.451, 5.349, 5.415, 5.307, 5.293, 5.202, 5.218, 5.216, 5.197, 5.226, 5.212, 5.045, 4.974]\n",
      "\n",
      "Train Power Error(all epochs): 1.648928165435791 \n",
      " [8.597, 8.057, 7.457, 6.794, 6.232, 6.068, 5.871, 5.678, 5.663, 5.567, 5.322, 5.061, 4.903, 4.798, 4.519, 4.333, 4.244, 4.151, 4.027, 3.869, 3.732, 3.558, 3.4, 3.34, 3.208, 3.129, 3.07, 2.981, 2.936, 2.84, 2.802, 2.748, 2.691, 2.636, 2.596, 2.538, 2.524, 2.469, 2.451, 2.417, 2.383, 2.367, 2.324, 2.323, 2.306, 2.282, 2.274, 2.23, 2.24, 2.221, 2.228, 2.179, 2.199, 2.165, 2.168, 2.174, 2.169, 2.154, 2.14, 2.131, 2.133, 2.136, 2.109, 2.121, 2.111, 2.096, 2.105, 2.088, 2.092, 2.095, 2.096, 2.075, 2.091, 2.068, 2.072, 2.071, 2.06, 2.064, 2.049, 2.038, 2.05, 2.041, 2.038, 2.026, 2.013, 2.014, 1.998, 2.001, 2.01, 2.0, 1.998, 1.99, 1.998, 1.982, 2.004, 1.97, 1.986, 1.959, 1.961, 1.956, 1.964, 1.951, 1.951, 1.941, 1.936, 1.936, 1.929, 1.926, 1.928, 1.918, 1.904, 1.897, 1.892, 1.91, 1.903, 1.895, 1.886, 1.876, 1.874, 1.87, 1.861, 1.878, 1.867, 1.856, 1.862, 1.844, 1.857, 1.855, 1.85, 1.834, 1.834, 1.837, 1.832, 1.839, 1.828, 1.831, 1.82, 1.813, 1.811, 1.795, 1.805, 1.813, 1.807, 1.792, 1.809, 1.795, 1.788, 1.799, 1.788, 1.785, 1.76, 1.786, 1.773, 1.78, 1.752, 1.764, 1.75, 1.749, 1.747, 1.73, 1.745, 1.74, 1.747, 1.747, 1.727, 1.72, 1.721, 1.728, 1.722, 1.725, 1.707, 1.719, 1.714, 1.714, 1.729, 1.71, 1.714, 1.703, 1.706, 1.692, 1.699, 1.684, 1.673, 1.685, 1.679, 1.693, 1.686, 1.68, 1.679, 1.69, 1.68, 1.686, 1.689, 1.669, 1.662, 1.666, 1.649, 1.682, 1.681, 1.673]\n",
      "\n",
      "Val Error(all epochs): 10.921257019042969 \n",
      " [69.981, 71.04, 69.869, 65.595, 41.48, 30.302, 51.471, 48.525, 42.226, 25.045, 22.884, 19.575, 22.4, 17.417, 17.832, 17.207, 16.679, 19.751, 18.925, 18.098, 18.775, 17.952, 23.381, 24.622, 19.346, 12.968, 14.349, 14.42, 13.626, 22.477, 13.675, 26.554, 12.656, 20.571, 14.053, 13.092, 15.238, 16.63, 12.644, 12.865, 14.351, 14.077, 20.076, 15.761, 13.254, 13.933, 13.607, 21.222, 12.594, 14.914, 12.858, 12.163, 13.293, 12.056, 15.532, 19.633, 13.161, 17.37, 20.268, 11.826, 17.765, 13.193, 15.37, 16.279, 15.062, 19.231, 14.787, 13.625, 15.958, 12.206, 13.789, 16.149, 14.766, 12.162, 15.025, 11.854, 13.756, 12.581, 12.52, 13.386, 12.247, 11.945, 13.32, 12.156, 13.568, 18.689, 13.458, 12.609, 20.38, 12.896, 16.213, 13.149, 12.943, 25.272, 11.708, 15.292, 14.267, 17.05, 13.098, 12.451, 13.215, 13.413, 14.428, 18.163, 14.402, 12.197, 13.217, 12.153, 21.11, 12.371, 13.385, 11.858, 12.477, 12.611, 12.095, 11.751, 13.441, 15.015, 19.17, 11.402, 11.545, 11.436, 15.822, 12.838, 13.92, 14.746, 12.79, 12.004, 11.721, 17.061, 13.242, 14.167, 12.272, 11.81, 11.895, 13.009, 13.189, 11.684, 15.722, 14.44, 16.116, 11.383, 11.971, 12.448, 11.348, 12.265, 14.198, 13.549, 12.01, 12.292, 14.128, 12.155, 11.538, 12.76, 12.033, 11.433, 11.483, 12.808, 13.721, 12.532, 12.134, 17.128, 11.69, 11.242, 13.184, 12.765, 11.227, 12.9, 11.644, 11.978, 11.503, 14.51, 11.004, 16.799, 11.447, 12.015, 12.095, 11.682, 11.649, 12.615, 17.596, 13.565, 11.272, 13.054, 19.602, 10.921, 15.809, 11.292, 14.899, 11.384, 13.017, 11.931, 11.667, 12.464, 11.953, 15.958, 14.978, 13.77, 14.112, 10.927]\n",
      "\n",
      "Val Total Error(all epochs): 17.19711685180664 \n",
      " [109.088, 110.962, 108.583, 101.492, 64.726, 48.356, 80.62, 75.954, 66.736, 40.535, 36.903, 31.796, 35.946, 28.482, 29.246, 28.331, 27.404, 31.578, 30.884, 28.987, 30.005, 28.718, 37.158, 39.22, 30.753, 21.133, 23.053, 23.259, 22.028, 35.435, 22.17, 41.697, 20.323, 32.781, 22.314, 21.042, 24.387, 26.271, 20.15, 20.625, 22.88, 22.258, 31.66, 25.093, 21.255, 22.108, 21.551, 33.033, 19.973, 23.449, 20.349, 19.219, 21.015, 19.042, 24.418, 30.696, 20.804, 27.131, 31.909, 18.77, 28.054, 20.801, 24.175, 25.89, 23.763, 30.034, 23.33, 21.682, 25.059, 19.326, 21.747, 25.694, 23.368, 19.252, 23.479, 18.78, 21.672, 19.847, 19.995, 21.098, 19.352, 18.931, 21.064, 19.563, 21.303, 29.171, 21.173, 19.932, 31.739, 20.251, 25.269, 20.763, 20.567, 39.548, 18.478, 23.991, 22.27, 26.638, 20.821, 19.76, 20.815, 21.062, 22.685, 28.355, 22.811, 19.423, 20.909, 19.126, 33.392, 19.511, 21.048, 18.809, 19.676, 19.957, 19.053, 18.498, 21.266, 23.889, 30.308, 17.98, 18.274, 18.022, 25.164, 20.149, 21.803, 22.999, 20.315, 18.843, 18.474, 26.968, 20.794, 22.312, 19.525, 18.693, 18.802, 20.695, 20.736, 18.445, 24.49, 22.785, 25.452, 17.986, 18.919, 19.551, 17.933, 19.402, 22.318, 21.254, 18.923, 19.31, 22.269, 19.165, 18.184, 19.995, 19.093, 17.968, 18.071, 20.214, 21.644, 20.055, 19.12, 27.131, 18.428, 17.672, 20.7, 20.001, 17.668, 20.261, 18.331, 18.91, 18.149, 22.993, 17.336, 26.48, 17.973, 18.881, 19.173, 18.34, 18.383, 19.82, 27.745, 21.206, 17.837, 20.828, 30.936, 17.197, 24.905, 17.795, 23.531, 17.843, 20.536, 18.747, 18.306, 19.535, 18.83, 25.531, 23.607, 21.868, 22.161, 17.2]\n",
      "\n",
      "Val Location Error(all epochs): 15.454054832458496 \n",
      " [100.518, 101.839, 100.773, 95.066, 59.362, 42.487, 74.193, 69.825, 60.573, 35.125, 32.021, 26.734, 31.527, 24.008, 24.356, 23.177, 22.821, 27.564, 25.948, 25.258, 26.099, 24.94, 33.123, 34.804, 26.925, 17.719, 19.856, 20.205, 18.963, 31.857, 18.925, 37.692, 17.606, 29.17, 19.725, 18.341, 21.57, 23.459, 17.751, 17.934, 20.154, 19.97, 28.58, 22.384, 18.652, 19.672, 19.261, 30.232, 17.652, 21.173, 18.191, 17.058, 18.771, 16.941, 22.011, 27.994, 18.585, 24.734, 28.894, 16.707, 25.178, 18.726, 21.846, 23.108, 21.318, 27.348, 20.834, 19.086, 22.635, 17.199, 19.516, 22.945, 20.856, 17.113, 21.329, 16.736, 19.527, 17.846, 17.505, 19.031, 17.3, 16.845, 18.817, 16.994, 19.295, 26.619, 19.019, 17.707, 29.056, 18.224, 23.008, 18.665, 18.11, 35.964, 16.518, 21.78, 20.243, 24.3, 18.452, 17.469, 18.678, 18.986, 20.555, 25.942, 20.345, 17.16, 18.662, 17.212, 30.196, 17.488, 19.019, 16.752, 17.572, 17.817, 17.098, 16.556, 18.974, 21.116, 27.388, 16.091, 16.299, 16.176, 22.35, 18.215, 19.795, 21.036, 17.924, 16.991, 16.595, 24.298, 18.706, 20.058, 17.199, 16.7, 16.736, 18.306, 18.715, 16.54, 22.464, 20.528, 22.771, 16.124, 16.874, 17.682, 16.036, 17.329, 20.148, 19.348, 17.007, 17.483, 20.043, 17.131, 16.332, 18.13, 16.893, 16.267, 16.282, 18.118, 19.438, 17.335, 17.137, 24.285, 16.49, 15.86, 18.792, 18.166, 15.861, 18.206, 16.435, 16.798, 16.26, 20.659, 15.571, 23.931, 16.212, 17.023, 16.974, 16.51, 16.408, 18.003, 25.118, 19.317, 15.917, 18.32, 28.003, 15.454, 22.605, 16.032, 21.164, 16.084, 18.396, 16.912, 16.57, 17.803, 16.727, 22.43, 21.352, 19.418, 20.099, 15.475]\n",
      "\n",
      "Val Power Error(all epochs): 1.7010880708694458 \n",
      " [8.57, 9.123, 7.81, 6.427, 5.363, 5.869, 6.427, 6.129, 6.163, 5.409, 4.881, 5.062, 4.419, 4.474, 4.89, 5.154, 4.583, 4.014, 4.936, 3.728, 3.907, 3.778, 4.034, 4.415, 3.828, 3.413, 3.197, 3.054, 3.065, 3.578, 3.245, 4.005, 2.717, 3.611, 2.589, 2.701, 2.817, 2.811, 2.399, 2.691, 2.725, 2.288, 3.08, 2.71, 2.603, 2.436, 2.29, 2.801, 2.321, 2.276, 2.158, 2.162, 2.244, 2.101, 2.407, 2.702, 2.22, 2.397, 3.016, 2.062, 2.876, 2.075, 2.329, 2.781, 2.444, 2.687, 2.497, 2.596, 2.424, 2.127, 2.231, 2.749, 2.512, 2.139, 2.15, 2.043, 2.146, 2.0, 2.49, 2.067, 2.053, 2.086, 2.247, 2.569, 2.008, 2.552, 2.154, 2.225, 2.684, 2.027, 2.261, 2.098, 2.456, 3.583, 1.959, 2.211, 2.027, 2.338, 2.369, 2.291, 2.136, 2.076, 2.13, 2.413, 2.466, 2.263, 2.247, 1.914, 3.196, 2.023, 2.029, 2.057, 2.104, 2.14, 1.955, 1.941, 2.292, 2.773, 2.92, 1.889, 1.975, 1.846, 2.814, 1.934, 2.008, 1.964, 2.391, 1.852, 1.879, 2.671, 2.088, 2.254, 2.326, 1.993, 2.066, 2.388, 2.02, 1.905, 2.026, 2.258, 2.681, 1.862, 2.045, 1.869, 1.897, 2.073, 2.169, 1.906, 1.916, 1.826, 2.227, 2.034, 1.852, 1.865, 2.2, 1.701, 1.789, 2.096, 2.207, 2.721, 1.983, 2.846, 1.938, 1.812, 1.907, 1.836, 1.807, 2.055, 1.896, 2.112, 1.89, 2.334, 1.765, 2.548, 1.761, 1.858, 2.199, 1.829, 1.975, 1.817, 2.628, 1.889, 1.919, 2.508, 2.934, 1.743, 2.3, 1.763, 2.366, 1.76, 2.14, 1.835, 1.736, 1.732, 2.103, 3.101, 2.255, 2.449, 2.062, 1.724]\n",
      "\n",
      "Trainig set size: 16384 , Time: 0:55:24 , best_lambda: 0.1 , min_ val_total_mae error: 17.197\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST = False\n",
    "mini_batch = 16 if max(max_x, max_y) == 1000 else 256\n",
    "epochs = 200 if max(max_x, max_y) == 1000 else 200\n",
    "MAX_QUEUE_SIZE, WORKERS = 6, 1\n",
    "fp_penalty_coef, fn_penalty_coef = 1, 1\n",
    "hyper_metric, mode = \"val_total_mae\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0.1]#[0.001, 0.01, 0.1, 1]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\n",
    "# lambda_vec = [0.01, 0.1, 1]\n",
    "# lambda_vec = [10]\n",
    "# MODEL_PATH = 'models/'\n",
    "average_diff_power, average_location_error = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "all_cnns = []\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "#     if num_sample_idx < 3:\n",
    "#         continue\n",
    "#     if num_sample_idx == 0:\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss='mse',#custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                        optimizer='adam', \n",
    "                        metrics=['mse', 'mae', location_mae, power_mae, total_mae])\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb_idx) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'location_mae': location_mae,\n",
    "                                                 'power_mae': power_mae, 'total_mae': total_mae}) \n",
    "                for lamb_idx in range(len(lambda_vec))]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used, \n",
    "                                         STATIC=STATIC_SENSORS, SENSOR_NUM=sensors_num)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                      STATIC=STATIC_SENSORS, SENSOR_NUM=sensors_num)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=0,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\n\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"\\nTrain Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"\\nTrain Total Error(all epochs):\", min(cnns[lamb_idx].history.history['total_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['total_mae']])\n",
    "        print(\"\\nTrain Location Error(all epochs):\", min(cnns[lamb_idx].history.history['location_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['location_mae']])\n",
    "        print(\"\\nTrain Power Error(all epochs):\", min(cnns[lamb_idx].history.history['power_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['power_mae']])\n",
    "        \n",
    "        \n",
    "        print(\"\\nVal Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"\\nVal Total Error(all epochs):\", min(cnns[lamb_idx].history.history['val_total_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_total_mae']])\n",
    "        print(\"\\nVal Location Error(all epochs):\", min(cnns[lamb_idx].history.history['val_location_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_location_mae']])\n",
    "        print(\"\\nVal Power Error(all epochs):\", min(cnns[lamb_idx].history.history['val_power_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_power_mae']])\n",
    "#     if num_sample_idx == 3:    \n",
    "#         models_min_mae = [8.27781, 8.23545, 8.20838, 7.74743]\n",
    "#         models_min_mae += [min(cnns[lamb_idx].history.history[hyper_metric]) for lamb_idx in range(4,lamb_idx+1)]\n",
    "#     else:\n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , hyper_metric,\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator, checkpointers\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[434.40948486328125,\n",
       "  374.4504699707031,\n",
       "  320.1500244140625,\n",
       "  287.4156799316406,\n",
       "  566.0879516601562,\n",
       "  346.3506164550781,\n",
       "  352.1589660644531,\n",
       "  364.9403076171875,\n",
       "  289.760498046875,\n",
       "  338.2485046386719,\n",
       "  314.6058654785156,\n",
       "  1554.814453125,\n",
       "  337.03448486328125,\n",
       "  322.7658386230469,\n",
       "  2638.966064453125,\n",
       "  257.04168701171875,\n",
       "  1433.9849853515625,\n",
       "  309.13665771484375,\n",
       "  2551.390380859375,\n",
       "  615.9176025390625,\n",
       "  1420.5355224609375,\n",
       "  344.8804931640625,\n",
       "  315.6465148925781,\n",
       "  665.6366577148438,\n",
       "  3229.704833984375,\n",
       "  667.3214721679688,\n",
       "  3083.19287109375,\n",
       "  1022.021728515625,\n",
       "  304.1854248046875,\n",
       "  1277.291259765625,\n",
       "  915.7808837890625,\n",
       "  327.5958251953125,\n",
       "  347.187255859375,\n",
       "  811.7786254882812,\n",
       "  657.817626953125]]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "models_min_mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Lambda: 0.001 , Time: 0:24:39\n",
      "Train Error(all epochs): 48.49994659423828 \n",
      " [282.558, 273.271, 245.181, 204.392, 168.033, 143.991, 129.84, 122.935, 119.77, 117.446, 116.165, 115.712, 114.147, 113.436, 112.757, 112.438, 112.405, 111.254, 110.112, 109.728, 109.161, 108.551, 108.838, 107.63, 106.896, 105.074, 102.867, 97.924, 90.565, 81.064, 69.135, 60.424, 53.126, 48.983, 48.5]\n",
      "Train Total Error(all epochs): 77.52651977539062 \n",
      " [438.263, 423.265, 378.942, 317.533, 262.796, 226.746, 205.471, 195.021, 190.158, 186.738, 184.743, 182.945, 180.211, 178.766, 177.187, 176.445, 175.858, 173.91, 171.532, 170.45, 169.329, 168.239, 168.604, 166.321, 165.269, 162.513, 159.128, 151.877, 140.703, 126.699, 108.816, 95.482, 84.398, 78.159, 77.527]\n",
      "Train Location Error(all epochs): 67.97329711914062 \n",
      " [409.412, 396.548, 356.602, 295.644, 241.304, 205.227, 184.05, 173.784, 169.152, 165.6, 163.752, 164.189, 162.229, 161.542, 161.085, 160.87, 161.358, 159.852, 158.806, 158.734, 158.154, 157.414, 157.909, 156.57, 155.419, 152.708, 149.474, 141.893, 130.992, 116.494, 98.59, 85.791, 74.981, 68.79, 67.973]\n",
      "Train Power Error(all epochs): 9.369025230407715 \n",
      " [28.851, 26.717, 22.34, 21.889, 21.492, 21.519, 21.421, 21.237, 21.005, 21.138, 20.991, 18.756, 17.982, 17.224, 16.101, 15.575, 14.5, 14.058, 12.726, 11.716, 11.175, 10.826, 10.695, 9.751, 9.851, 9.806, 9.654, 9.984, 9.712, 10.205, 10.226, 9.691, 9.416, 9.369, 9.553]\n",
      "Val Error(all epochs): 163.4048309326172 \n",
      " [277.109, 242.875, 206.405, 187.376, 358.029, 220.503, 222.895, 233.123, 186.135, 216.067, 201.074, 796.266, 215.103, 206.307, 1285.253, 163.405, 747.435, 198.862, 1234.004, 365.496, 732.544, 221.091, 189.596, 395.909, 1546.258, 343.901, 1419.096, 499.068, 196.866, 585.975, 458.524, 212.39, 224.582, 413.395, 365.844]\n",
      "Val Total Error(all epochs): 257.04168701171875 \n",
      " [434.409, 374.45, 320.15, 287.416, 566.088, 346.351, 352.159, 364.94, 289.76, 338.249, 314.606, 1554.814, 337.034, 322.766, 2638.966, 257.042, 1433.985, 309.137, 2551.39, 615.918, 1420.536, 344.88, 315.647, 665.637, 3229.705, 667.321, 3083.193, 1022.022, 304.185, 1277.291, 915.781, 327.596, 347.187, 811.779, 657.818]\n",
      "Val Location Error(all epochs): 231.180419921875 \n",
      " [397.292, 354.274, 298.249, 272.885, 509.083, 314.154, 315.779, 334.348, 266.24, 310.053, 287.918, 837.32, 307.214, 295.868, 1216.329, 231.18, 809.101, 286.419, 1154.376, 479.156, 784.881, 319.262, 251.425, 519.009, 1411.713, 369.419, 1175.567, 482.23, 285.232, 482.643, 461.84, 307.883, 326.162, 429.76, 441.414]\n",
      "Val Power Error(all epochs): 14.531084060668945 \n",
      " [37.117, 20.177, 21.901, 14.531, 57.005, 32.197, 36.38, 30.593, 23.52, 28.196, 26.687, 717.494, 29.82, 26.898, 1422.637, 25.861, 624.884, 22.717, 1397.015, 136.762, 635.654, 25.618, 64.221, 146.628, 1817.991, 297.903, 1907.625, 539.792, 18.954, 794.648, 453.94, 19.713, 21.025, 382.018, 216.403]\n",
      "\n",
      "Trainig set size: 2048 , Time: 0:24:39 , best_lambda: 0.001 , min_ val_total_mae error: 257.042\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    if True:\n",
    "        print(\"\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"Train Error(all epochs):\", min(cnns[lamb_idx].history.history['mae']), '\\n', \n",
    "              [round(val, 3) for val in cnns[lamb_idx].history.history['mae']])\n",
    "        print(\"Train Total Error(all epochs):\", min(cnns[lamb_idx].history.history['total_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['total_mae']])\n",
    "        print(\"Train Location Error(all epochs):\", min(cnns[lamb_idx].history.history['location_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['location_mae']])\n",
    "        print(\"Train Power Error(all epochs):\", min(cnns[lamb_idx].history.history['power_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['power_mae']])\n",
    "        \n",
    "        \n",
    "        print(\"Val Error(all epochs):\", min(cnns[lamb_idx].history.history['val_mae']), '\\n', \n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_mae']])\n",
    "        print(\"Val Total Error(all epochs):\", min(cnns[lamb_idx].history.history['val_total_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_total_mae']])\n",
    "        print(\"Val Location Error(all epochs):\", min(cnns[lamb_idx].history.history['val_location_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_location_mae']])\n",
    "        print(\"Val Power Error(all epochs):\", min(cnns[lamb_idx].history.history['val_power_mae']), '\\n',\n",
    "              [round(val,3) for val in cnns[lamb_idx].history.history['val_power_mae']])\n",
    "    models_min_mae = [min(cnns[0].history.history[hyper_metric])]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , hyper_metric,\n",
    "          \"error:\", round(min(models_min_mae), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cnn_model2(num_filters, kernel_lam, bias_lam):\n",
    "#     num_filters, lam = 5, 5\n",
    "    data_format = 'channels_first'\n",
    "    convolution_init, dense_init = \"lecun_normal\", \"RandomNormal\"\n",
    "    convolution_filter, dense_filter = 'selu', 'linear' #softsign, sigmoid; relu, linear\n",
    "    filter_shape, pool_size = (5, 5), (2,2)\n",
    "    leak_coeff = 0.0\n",
    "    cnn_input = tf.keras.Input(shape=(number_image_channels, max_x * pic_cell_size, max_y * pic_cell_size)\n",
    "                               , name=\"CNN_INPUT\")\n",
    "    \n",
    "    cnn = layers.Conv2D(8, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn_input)\n",
    "    cnn = layers.Conv2D(32, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    cnn = layers.Conv2D(1, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    \n",
    "    \n",
    "    cnn = layers.Conv2D(num_filters, filter_shape, padding='same', #activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    cnn = layers.LeakyReLU(alpha=leak_coeff)(cnn)\n",
    "    cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "    cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "#     cnn.add(layers.Conv2D(num_filters, filter_shape, padding='same', activation=convolution_filter, \n",
    "#                           input_shape=(number_image_channels, max_x, max_y), data_format=data_format,\n",
    "#                           kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                           kernel_initializer=convolution_init))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "    cnn = layers.Conv2D(2*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    \n",
    "    cnn = layers.Conv2D(2*num_filters, filter_shape, padding='same', #activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    cnn = layers.LeakyReLU(alpha=leak_coeff)(cnn)\n",
    "    cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "    cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(2*num_filters, filter_shape,padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size=pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "    cnn = layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    \n",
    "    cnn = layers.Conv2D(3*num_filters, filter_shape, padding='same', #activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    cnn = layers.LeakyReLU(alpha=leak_coeff)(cnn)\n",
    "    cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "    cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "#     cnn.add(layers.Conv2D(3*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "    cnn = layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    \n",
    "    cnn = layers.Conv2D(4*num_filters, \n",
    "                        filter_shape, padding='same', #activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "    cnn = layers.LeakyReLU(alpha=leak_coeff)(cnn)\n",
    "    cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "    cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "    \n",
    "#     cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                          kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                          kernel_initializer=convolution_init))\n",
    "#     cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#     cnn.add(BatchNormalization(axis=1))\n",
    "    \n",
    "    if max(max_x, max_y) * pic_cell_size == 200:\n",
    "        cnn = layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "#         cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                              kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                              kernel_initializer=convolution_init))\n",
    "#         cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#         cnn.add(BatchNormalization(axis=1))\n",
    "        \n",
    "# from here for 1000\n",
    "    if max(max_x, max_y) * pic_cell_size == 1000:\n",
    "        cnn = layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "#         cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                              kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam),\n",
    "#                              kernel_initializer=convolution_init))\n",
    "#         cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#         cnn.add(BatchNormalization(axis=1))\n",
    "\n",
    "        cnn = layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "#         cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                              kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "#                              kernel_initializer=convolution_init))\n",
    "#         cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#         cnn.add(BatchNormalization(axis=1))\n",
    "        \n",
    "        cnn = layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter,\n",
    "                        data_format=data_format,  kernel_regularizer=regularizers.l2(kernel_lam), \n",
    "                        bias_regularizer=regularizers.l2(bias_lam),\n",
    "                        kernel_initializer=convolution_init)(cnn)\n",
    "        cnn = layers.MaxPooling2D(pool_size=pool_size, data_format=data_format)(cnn)\n",
    "        cnn = layers.BatchNormalization(axis=1)(cnn)\n",
    "    \n",
    "#         cnn.add(layers.Conv2D(4*num_filters, filter_shape, padding='same', activation=convolution_filter, data_format=data_format, \n",
    "#                              kernel_regularizer=regularizers.l2(kernel_lam), bias_regularizer=regularizers.l2(bias_lam), \n",
    "#                              kernel_initializer=convolution_init))\n",
    "#         cnn.add(layers.MaxPooling2D(pool_size, data_format=data_format))\n",
    "#         cnn.add(BatchNormalization(axis=1))\n",
    "    \n",
    "    cnn = layers.Flatten()(cnn)\n",
    "#     cnn.add(layers.Flatten())\n",
    "    cnn_power = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init)(cnn)\n",
    "#     cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "#                          bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "    cnn_power = layers.BatchNormalization()(cnn_power)\n",
    "#     cnn.add(BatchNormalization())\n",
    "    cnn_power = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init)(cnn_power)\n",
    "#     cnn.add(layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "#                          bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=convolution_init))\n",
    "#     cnn.add(BatchNormalization())\n",
    "    cnn_power = layers.Dense(1, activation=dense_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init,\n",
    "                             name=\"power\")(cnn_power)\n",
    "    \n",
    "    \n",
    "    cnn_location = layers.Dense(300, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init)(cnn)\n",
    "    cnn_location = layers.BatchNormalization()(cnn_location)\n",
    "    cnn_location = layers.Dense(300, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init)(cnn_location)\n",
    "    cnn_location = layers.Dense(2, activation='relu', kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init,\n",
    "                             name=\"location\")(cnn_location)\n",
    "    \n",
    "    # output if a tx exists or not\n",
    "    cnn_tx_exist = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam),\n",
    "                             kernel_initializer=convolution_init)(cnn)\n",
    "    cnn_tx_exist = layers.BatchNormalization()(cnn_tx_exist)\n",
    "    cnn_tx_exist = layers.Dense(20, activation=convolution_filter, kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), \n",
    "                             kernel_initializer=convolution_init)(cnn_tx_exist)\n",
    "    cnn_tx_exist = layers.Dense(1, activation=\"sigmoid\", kernel_regularizer=regularizers.l2(kernel_lam),\n",
    "                             bias_regularizer=regularizers.l2(bias_lam), kernel_initializer=dense_init,\n",
    "                             name=\"tx_exist\")(cnn_tx_exist)\n",
    "    model = tf.keras.Model(inputs=cnn_input, outputs=[cnn_location, cnn_power, cnn_tx_exist])\n",
    "    return model\n",
    "\n",
    "\n",
    "class DataBatchGenerator2(Sequence):\n",
    "    def __init__(self, dataset:np.ndarray, batch_size:int, start_idx:int,\n",
    "                 number_image_channels:int,\n",
    "                 max_x, max_y, float_memory_used, SENSOR_NUM:int, STATIC:bool=False, \n",
    "                pic_cell_size: int=1):\n",
    "#         print(dataset.shape[0])\n",
    "        self.dataset, self.batch_size, self.start_idx = dataset, batch_size, start_idx\n",
    "        self.number_image_channels, self.max_x, self.max_y = number_image_channels, max_x, max_y\n",
    "        self.float_memory_used = float_memory_used\n",
    "        self.STATIC = STATIC\n",
    "        self.SENSOR_NUM = SENSOR_NUM\n",
    "        self.pic_cell_size = pic_cell_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return np.ceil(self.dataset.shape[0] / self.batch_size).astype(np.int)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        size = min(self.dataset.shape[0] - idx * self.batch_size, self.batch_size)\n",
    "        batch_x = np.empty((size, self.number_image_channels, self.max_x * self.pic_cell_size,\n",
    "                            self.max_y * self.pic_cell_size),\n",
    "                          dtype=self.float_memory_used)\n",
    "        batch_y_p = np.empty((size, 1), dtype=self.float_memory_used)\n",
    "        batch_y_loc = np.empty((size, 2), dtype=self.float_memory_used)\n",
    "        batch_y_exist = np.empty((size, 1), dtype=self.float_memory_used)\n",
    "        for i in range(size):\n",
    "            y_offset = (self.SENSOR_NUM if self.STATIC else int(self.dataset[idx * self.batch_size + i][0])*3+1)\n",
    "            batch_x[i] = read_image(self.start_idx + idx * self.batch_size + i)\n",
    "            batch_y_loc[i][0] = self.dataset[idx * self.batch_size + i][y_offset] * self.pic_cell_size #x\n",
    "            batch_y_loc[i][1] = self.dataset[idx * self.batch_size + i][y_offset + 1] * self.pic_cell_size #y\n",
    "            batch_y_p[i][0] = self.dataset[idx * self.batch_size + i][y_offset + 2] #power\n",
    "            batch_y_exist[i][0] = 1 if int(self.dataset[idx * self.batch_size + i][y_offset + 3]) > 0 else 0\n",
    "        return batch_x, {'location': batch_y_loc, 'power': batch_y_p, 'tx_exist': batch_y_exist}\n",
    "    \n",
    "def custom_loss(fp_penalty_coef, fn_penalty_coef):\n",
    "    # custom loss function that penalize false positive and negative differently\n",
    "    def loss(y_true, y_pred):\n",
    "        res = y_pred - y_true\n",
    "        res = tf.where(res > 0, res * fp_penalty_coef, res * fn_penalty_coef)\n",
    "        return K.mean(K.square(res))\n",
    "    return loss\n",
    "\n",
    "def location_mae2(y_true, y_pred):\n",
    "    return K.mean(K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1)))\n",
    "\n",
    "def location_loss(y_true, y_pred):\n",
    "    return K.mean(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "#     return K.sqrt(K.sum(K.square(y_pred - y_true), axis=-1))\n",
    "\n",
    "def power_mae2(y_true, y_pred):\n",
    "    res = y_pred - y_true\n",
    "    return K.mean(K.abs(res))\n",
    "\n",
    "def total_mae2(y_true, y_pred):\n",
    "    return location_mae(y_true, y_pred) + power_mae(y_true, y_pred)\n",
    "\n",
    "def fp_mae(y_true, y_pred):\n",
    "    # custom metric that replace false negative with zero and return the mean of new vector\n",
    "    res = y_pred - y_true\n",
    "    res = tf.nn.relu(res)\n",
    "#     res = tf.where(res <= 0, 0, res)\n",
    "    return K.mean(res)\n",
    "         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "CNN_INPUT (InputLayer)          [(None, 1, 100, 100) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 8, 100, 100)  208         CNN_INPUT[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 100, 100) 6432        conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 1, 100, 100)  801         conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 10, 100, 100) 260         conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 10, 100, 100) 0           conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 10, 50, 50)   0           leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 10, 50, 50)   40          max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 20, 50, 50)   5020        batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 20, 50, 50)   10020       conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 20, 50, 50)   0           conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2D)  (None, 20, 25, 25)   0           leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 20, 25, 25)   80          max_pooling2d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 30, 25, 25)   15030       batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 30, 25, 25)   22530       conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 30, 25, 25)   0           conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 30, 12, 12)   0           leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 30, 12, 12)   120         max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 40, 12, 12)   30040       batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 40, 12, 12)   40040       conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 40, 12, 12)   0           conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2D)  (None, 40, 6, 6)     0           leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 40, 6, 6)     160         max_pooling2d_3[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 1440)         0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 300)          432300      flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 20)           28820       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 20)           28820       flatten[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 300)          1200        dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 20)           80          dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 20)           80          dense_4[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense_3 (Dense)                 (None, 300)          90300       batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 20)           420         batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_5 (Dense)                 (None, 20)           420         batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "location (Dense)                (None, 2)            602         dense_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "power (Dense)                   (None, 1)            21          dense_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tx_exist (Dense)                (None, 1)            21          dense_5[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 713,865\n",
      "Trainable params: 712,985\n",
      "Non-trainable params: 880\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = cnn_model2(10, 0, 0)\n",
    "model.summary()\n",
    "# tf.keras.utils.plot_model(model, \"Concatenated_model.png\", show_shapes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_samples = [75000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 75000 , New samples: 75000\n",
      "Validation size: 24750 , starts: 75000 , ends: 99749\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1207.98645, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1207.98645 to 1049.10999, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 1049.10999 to 500.78287, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 500.78287\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 500.78287\n",
      "\n",
      "Epoch 00006: val_loss improved from 500.78287 to 9.23006, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 9.23006\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 9.23006\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 9.23006\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 9.23006\n",
      "\n",
      "Epoch 00011: val_loss improved from 9.23006 to 3.33028, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00012: val_loss improved from 3.33028 to 3.10363, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 3.10363\n",
      "\n",
      "Epoch 00020: val_loss improved from 3.10363 to 1.87500, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 1.87500\n",
      "\n",
      "Epoch 00030: val_loss improved from 1.87500 to 1.50922, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 1.50922 to 1.45123, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 1.45123\n",
      "\n",
      "Epoch 00033: val_loss improved from 1.45123 to 1.17870, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 1.17870\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 1.17870\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 1.17870\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 1.17870\n",
      "\n",
      "Epoch 00038: val_loss improved from 1.17870 to 1.12975, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00039: val_loss improved from 1.12975 to 1.07340, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 1.07340\n",
      "\n",
      "Epoch 00046: val_loss improved from 1.07340 to 0.91984, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.91984\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 0.91984\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.91984\n",
      "\n",
      "Epoch 00050: val_loss improved from 0.91984 to 0.84685, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.84685\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.84685 to 0.66197, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.66197\n",
      "\n",
      "Epoch 00065: val_loss improved from 0.66197 to 0.62353, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.62353\n",
      "\n",
      "Epoch 00067: val_loss improved from 0.62353 to 0.60169, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.60169\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.60169\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 0.60169\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.60169 to 0.59521, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.59521\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.59521\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.59521\n",
      "\n",
      "Epoch 00075: val_loss improved from 0.59521 to 0.56324, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.56324\n",
      "\n",
      "Epoch 00092: val_loss improved from 0.56324 to 0.52641, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.52641\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.52641 to 0.49946, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_0.h5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00095: val_loss did not improve from 0.49946\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.49946\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 0.49946\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49946\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49946\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49946\n",
      "\n",
      "\n",
      "Lambda: 0 , Time: 1:28:47\n",
      "\n",
      "*****Train Info*****\n",
      "\n",
      "Train loss (all epochs): 0.3104208707809448 \n",
      " [450.47, 36.223, 21.843, 15.043, 12.677, 10.662, 9.209, 8.299, 7.347, 7.055, 5.745, 4.724, 4.502, 5.869, 11.816, 8.102, 4.556, 3.417, 3.405, 2.879, 2.532, 2.405, 2.444, 2.135, 6.327, 15.023, 3.852, 3.179, 2.43, 2.171, 2.144, 1.806, 1.73, 1.563, 1.386, 1.285, 1.277, 1.356, 1.202, 1.248, 1.078, 10.197, 2.666, 1.582, 1.311, 1.103, 0.952, 0.967, 0.911, 0.862, 0.764, 0.803, 0.867, 0.763, 0.676, 0.68, 0.733, 4.6, 2.479, 0.925, 0.794, 0.664, 0.639, 0.606, 0.546, 0.523, 0.494, 0.492, 0.51, 0.483, 0.477, 0.434, 0.489, 0.451, 0.423, 0.441, 0.483, 0.441, 0.453, 0.45, 0.384, 0.957, 9.378, 1.321, 0.906, 0.76, 0.604, 0.528, 0.48, 0.453, 0.404, 0.387, 0.385, 0.366, 0.342, 0.343, 0.338, 0.348, 0.322, 0.31]\n",
      "\n",
      "Train location_loss (all epochs): 0.3352299630641937 \n",
      " [436.62, 39.414, 23.958, 16.683, 14.027, 11.894, 10.223, 9.24, 8.142, 7.853, 6.353, 5.164, 4.956, 6.511, 13.298, 9.148, 5.083, 3.751, 3.731, 3.165, 2.762, 2.622, 2.68, 2.329, 7.101, 16.913, 4.242, 3.466, 2.601, 2.319, 2.307, 1.931, 1.847, 1.67, 1.468, 1.362, 1.361, 1.449, 1.269, 1.338, 1.148, 11.739, 2.945, 1.709, 1.398, 1.179, 1.014, 1.035, 0.975, 0.916, 0.811, 0.862, 0.932, 0.818, 0.713, 0.721, 0.79, 5.263, 2.787, 1.005, 0.861, 0.709, 0.681, 0.649, 0.587, 0.557, 0.527, 0.525, 0.546, 0.518, 0.514, 0.461, 0.523, 0.483, 0.453, 0.475, 0.525, 0.467, 0.494, 0.488, 0.407, 1.079, 10.761, 1.457, 0.991, 0.833, 0.66, 0.576, 0.521, 0.49, 0.437, 0.42, 0.413, 0.394, 0.369, 0.368, 0.365, 0.376, 0.346, 0.335]\n",
      "\n",
      "Train power_loss (all epochs): 0.25475358963012695 \n",
      " [793.379, 27.209, 14.793, 8.626, 7.538, 5.518, 5.196, 4.456, 4.271, 3.798, 3.45, 3.351, 2.893, 3.342, 5.129, 3.26, 2.349, 2.292, 2.336, 1.891, 1.838, 1.765, 1.667, 1.552, 2.905, 6.467, 2.462, 2.329, 2.188, 2.0, 1.833, 1.65, 1.598, 1.437, 1.386, 1.271, 1.194, 1.243, 1.238, 1.107, 1.027, 2.193, 1.624, 1.29, 1.222, 1.006, 0.896, 0.871, 0.82, 0.827, 0.743, 0.7, 0.752, 0.678, 0.703, 0.676, 0.618, 1.272, 1.106, 0.707, 0.624, 0.612, 0.601, 0.549, 0.475, 0.496, 0.46, 0.455, 0.453, 0.423, 0.401, 0.426, 0.441, 0.403, 0.381, 0.371, 0.369, 0.44, 0.333, 0.349, 0.377, 0.397, 2.303, 0.821, 0.63, 0.525, 0.431, 0.389, 0.368, 0.373, 0.326, 0.305, 0.336, 0.307, 0.282, 0.3, 0.277, 0.283, 0.285, 0.255]\n",
      "\n",
      "Train tx_exist_loss (all epochs): 1.5766728367339056e-08 \n",
      " [0.097, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.002, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Train location_location_mae2 (all epochs): 0.43872493505477905 \n",
      " [12.924, 4.623, 3.662, 3.065, 2.791, 2.593, 2.385, 2.29, 2.132, 2.064, 1.902, 1.747, 1.7, 1.838, 2.184, 2.186, 1.706, 1.467, 1.465, 1.354, 1.268, 1.234, 1.251, 1.167, 1.439, 2.55, 1.541, 1.399, 1.22, 1.145, 1.145, 1.046, 1.028, 0.978, 0.916, 0.883, 0.884, 0.916, 0.85, 0.878, 0.81, 1.786, 1.26, 0.972, 0.883, 0.813, 0.749, 0.764, 0.74, 0.716, 0.675, 0.701, 0.726, 0.683, 0.637, 0.641, 0.675, 1.183, 1.096, 0.742, 0.689, 0.623, 0.617, 0.603, 0.571, 0.56, 0.544, 0.545, 0.556, 0.543, 0.542, 0.511, 0.548, 0.526, 0.51, 0.521, 0.55, 0.52, 0.533, 0.529, 0.482, 0.512, 1.762, 0.87, 0.721, 0.668, 0.596, 0.558, 0.534, 0.52, 0.491, 0.483, 0.482, 0.472, 0.456, 0.456, 0.456, 0.464, 0.445, 0.439]\n",
      "\n",
      "Train power_mae (all epochs): 0.38595446944236755 \n",
      " [17.376, 4.117, 2.928, 2.154, 1.947, 1.707, 1.654, 1.552, 1.535, 1.464, 1.39, 1.396, 1.285, 1.357, 1.469, 1.374, 1.171, 1.16, 1.133, 1.044, 1.037, 1.015, 0.984, 0.934, 1.098, 1.607, 1.188, 1.153, 1.132, 1.082, 1.033, 0.97, 0.961, 0.921, 0.894, 0.86, 0.832, 0.85, 0.851, 0.799, 0.764, 1.054, 0.961, 0.857, 0.835, 0.768, 0.722, 0.716, 0.695, 0.697, 0.658, 0.641, 0.665, 0.629, 0.647, 0.632, 0.602, 0.794, 0.791, 0.648, 0.603, 0.602, 0.593, 0.562, 0.525, 0.543, 0.515, 0.512, 0.514, 0.498, 0.487, 0.496, 0.505, 0.483, 0.467, 0.467, 0.465, 0.504, 0.441, 0.455, 0.467, 0.472, 1.053, 0.675, 0.599, 0.548, 0.496, 0.473, 0.46, 0.466, 0.432, 0.419, 0.443, 0.421, 0.404, 0.418, 0.4, 0.406, 0.404, 0.386]\n",
      "\n",
      "Train tx_exist_accuracy (all epochs): 0.9740933179855347 \n",
      " [0.974, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "*****Validation Info*****\n",
      "\n",
      "Validation loss (all epochs): 0.499455064535141 \n",
      " [1207.986, 1049.11, 500.783, 622.717, 1137.068, 9.23, 23.638, 488.061, 22.367, 18.77, 3.33, 3.104, 1009.595, 3.312, 1310.84, 6.082, 5.97, 4.928, 4.496, 1.875, 2.172, 4.192, 2.107, 2.438, 1453.867, 117.29, 3.009, 2.445, 3.845, 1.509, 1.451, 2.102, 1.179, 2.307, 1.55, 2.168, 1.564, 1.13, 1.073, 1.298, 1.295, 93.886, 1.498, 1.518, 1.13, 0.92, 1.13, 1.267, 1.068, 0.847, 0.94, 0.662, 0.757, 0.792, 0.733, 0.8, 0.842, 1091.752, 1.1, 1.088, 0.848, 0.679, 0.817, 0.71, 0.624, 0.67, 0.602, 0.69, 0.64, 0.806, 0.595, 0.62, 0.606, 0.65, 0.563, 0.746, 0.588, 0.629, 0.767, 0.615, 0.628, 1665.714, 1.543, 1.055, 0.967, 0.744, 0.666, 0.62, 0.628, 0.606, 0.594, 0.526, 0.566, 0.499, 0.57, 0.503, 0.504, 0.593, 0.537, 0.537]\n",
      "\n",
      "Validation location_loss (all epochs): 0.557917594909668 \n",
      " [1181.944, 1187.435, 575.43, 723.77, 1306.334, 10.054, 26.818, 552.397, 15.28, 21.836, 3.09, 3.261, 1130.739, 3.663, 1348.578, 6.097, 5.265, 5.453, 4.931, 2.021, 2.502, 2.283, 2.4, 2.762, 1540.948, 133.692, 3.433, 2.797, 4.055, 1.739, 1.631, 2.298, 1.354, 2.686, 1.664, 2.383, 1.802, 1.274, 1.213, 1.484, 1.436, 100.905, 1.681, 1.752, 1.295, 1.056, 1.285, 1.423, 1.174, 0.949, 1.075, 0.759, 0.856, 0.853, 0.808, 0.894, 0.957, 1086.714, 1.265, 1.25, 0.959, 0.739, 0.887, 0.802, 0.706, 0.72, 0.635, 0.768, 0.705, 0.902, 0.669, 0.715, 0.655, 0.746, 0.643, 0.84, 0.642, 0.713, 0.843, 0.67, 0.601, 1942.487, 1.755, 1.205, 1.036, 0.83, 0.758, 0.698, 0.698, 0.679, 0.674, 0.599, 0.637, 0.558, 0.64, 0.56, 0.575, 0.677, 0.615, 0.609]\n",
      "\n",
      "Validation power_loss (all epochs): 0.12485980242490768 \n",
      " [2032.816, 397.35, 116.598, 74.969, 266.305, 6.845, 8.429, 184.883, 93.676, 2.093, 7.036, 3.319, 483.557, 1.978, 1643.519, 8.995, 14.947, 2.932, 3.04, 1.575, 0.451, 22.512, 0.671, 0.898, 1439.367, 36.514, 0.916, 0.674, 3.984, 0.308, 0.647, 1.484, 0.275, 0.235, 1.351, 1.422, 0.322, 0.468, 0.427, 0.367, 0.74, 80.982, 0.692, 0.288, 0.301, 0.22, 0.372, 0.58, 0.694, 0.403, 0.262, 0.17, 0.294, 0.668, 0.466, 0.394, 0.289, 1678.393, 0.247, 0.252, 0.327, 0.51, 0.63, 0.276, 0.232, 0.578, 0.617, 0.368, 0.399, 0.398, 0.268, 0.125, 0.495, 0.165, 0.167, 0.322, 0.421, 0.227, 0.499, 0.459, 1.175, 111.098, 0.511, 0.312, 0.864, 0.388, 0.21, 0.267, 0.349, 0.288, 0.209, 0.168, 0.247, 0.252, 0.263, 0.269, 0.148, 0.167, 0.146, 0.192]\n",
      "\n",
      "Validation tx_exist_loss (all epochs): 1.2364272938825138e-09 \n",
      " [1.04, 1.106, 0.144, 0.326, 1.085, 0.0, 0.0, 0.711, 0.23, 0.002, 0.0, 0.0, 2.219, 0.0, 3.937, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.019, 0.0, 0.0, 2.484, 0.001, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.378, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 4.12, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 69.814, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "Validation location_location_mae2 (all epochs): 0.44650715589523315 \n",
      " [28.788, 20.042, 13.704, 14.003, 18.033, 2.31, 3.822, 11.636, 2.658, 2.565, 1.169, 1.225, 16.969, 1.361, 25.486, 1.546, 1.606, 1.78, 1.517, 0.864, 1.044, 0.941, 1.005, 1.123, 26.572, 6.005, 1.317, 1.171, 1.096, 0.871, 0.82, 1.083, 0.717, 1.227, 0.85, 1.119, 0.944, 0.695, 0.711, 0.757, 0.785, 3.99, 0.856, 0.951, 0.787, 0.67, 0.773, 0.821, 0.715, 0.619, 0.726, 0.547, 0.592, 0.612, 0.56, 0.602, 0.652, 21.036, 0.737, 0.775, 0.672, 0.558, 0.629, 0.587, 0.544, 0.551, 0.526, 0.592, 0.549, 0.672, 0.537, 0.565, 0.512, 0.557, 0.508, 0.603, 0.507, 0.567, 0.647, 0.491, 0.479, 29.513, 0.88, 0.717, 0.665, 0.58, 0.54, 0.515, 0.533, 0.524, 0.514, 0.475, 0.478, 0.459, 0.495, 0.447, 0.477, 0.545, 0.458, 0.484]\n",
      "\n",
      "Validation power_mae (all epochs): 0.27260491251945496 \n",
      " [30.318, 11.356, 6.429, 4.839, 11.452, 2.02, 2.339, 6.552, 4.991, 1.006, 1.923, 1.203, 11.22, 1.199, 23.428, 2.066, 2.456, 1.033, 1.099, 0.881, 0.551, 2.367, 0.665, 0.777, 22.483, 3.322, 0.772, 0.686, 1.236, 0.424, 0.674, 0.835, 0.402, 0.366, 0.789, 1.007, 0.45, 0.469, 0.502, 0.454, 0.651, 4.646, 0.721, 0.427, 0.433, 0.368, 0.485, 0.581, 0.677, 0.533, 0.416, 0.311, 0.446, 0.67, 0.533, 0.496, 0.4, 24.183, 0.378, 0.398, 0.453, 0.586, 0.657, 0.388, 0.353, 0.618, 0.624, 0.507, 0.514, 0.504, 0.409, 0.273, 0.579, 0.304, 0.329, 0.477, 0.54, 0.4, 0.543, 0.57, 0.788, 6.39, 0.56, 0.431, 0.743, 0.506, 0.35, 0.412, 0.442, 0.413, 0.35, 0.327, 0.401, 0.396, 0.383, 0.421, 0.302, 0.321, 0.293, 0.325]\n",
      "\n",
      "Validation tx_exist_accuracy (all epochs): 0.784969687461853 \n",
      " [0.785, 0.799, 0.938, 0.879, 0.824, 1.0, 1.0, 0.836, 0.918, 0.999, 1.0, 1.0, 0.799, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.993, 1.0, 1.0, 0.792, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.948, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.848, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2071.00391, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2071.00391 to 153.41145, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 153.41145 to 84.25228, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 84.25228 to 28.27117, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 28.27117\n",
      "\n",
      "Epoch 00006: val_loss improved from 28.27117 to 11.04255, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 11.04255\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 11.04255\n",
      "\n",
      "Epoch 00009: val_loss improved from 11.04255 to 6.78044, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 6.78044\n",
      "\n",
      "Epoch 00011: val_loss improved from 6.78044 to 5.43028, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 5.43028\n",
      "\n",
      "Epoch 00021: val_loss improved from 5.43028 to 4.23055, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 4.23055\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 4.23055\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 4.23055\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 4.23055\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 4.23055\n",
      "\n",
      "Epoch 00027: val_loss improved from 4.23055 to 3.99901, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 3.99901\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 3.99901\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 3.99901\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 3.99901\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 3.99901\n",
      "\n",
      "Epoch 00033: val_loss improved from 3.99901 to 3.61098, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00034: val_loss improved from 3.61098 to 3.55976, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00035: val_loss improved from 3.55976 to 3.54323, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00036: val_loss improved from 3.54323 to 3.52948, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 3.52948\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 3.52948\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 3.52948\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 3.52948\n",
      "\n",
      "Epoch 00041: val_loss improved from 3.52948 to 3.19227, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 3.19227\n",
      "\n",
      "Epoch 00043: val_loss improved from 3.19227 to 2.95800, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 2.95800\n",
      "\n",
      "Epoch 00045: val_loss improved from 2.95800 to 2.75135, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 2.75135\n",
      "\n",
      "Epoch 00055: val_loss improved from 2.75135 to 2.63793, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00056: val_loss improved from 2.63793 to 2.51031, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00057: val_loss improved from 2.51031 to 2.43013, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 2.43013\n",
      "\n",
      "Epoch 00065: val_loss improved from 2.43013 to 2.29778, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 2.29778\n",
      "\n",
      "Epoch 00067: val_loss improved from 2.29778 to 2.18714, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00068: val_loss improved from 2.18714 to 2.03127, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 2.03127\n",
      "\n",
      "Epoch 00079: val_loss improved from 2.03127 to 1.96314, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 1.96314\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 1.96314\n",
      "\n",
      "Epoch 00082: val_loss improved from 1.96314 to 1.82676, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_1.h5\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 1.82676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00086: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 1.82676\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 1.82676\n",
      "\n",
      "\n",
      "Lambda: 0.001 , Time: 1:28:41\n",
      "\n",
      "*****Train Info*****\n",
      "\n",
      "Train loss (all epochs): 1.673628330230713 \n",
      " [455.757, 38.871, 23.133, 18.847, 14.515, 14.1, 10.618, 11.261, 8.809, 7.807, 8.606, 7.802, 7.368, 39.22, 10.76, 8.051, 7.523, 6.492, 5.991, 5.525, 5.209, 6.655, 8.042, 5.126, 4.482, 4.388, 4.153, 4.837, 18.44, 5.811, 5.007, 4.614, 4.025, 3.908, 3.617, 3.485, 3.344, 3.314, 6.625, 5.57, 4.143, 3.464, 3.069, 2.856, 2.773, 2.674, 2.652, 2.666, 2.549, 2.572, 8.871, 3.519, 3.018, 2.781, 2.631, 2.449, 2.369, 2.314, 2.298, 8.646, 3.307, 2.701, 2.532, 2.389, 2.264, 2.148, 2.081, 2.039, 1.998, 1.938, 1.861, 1.89, 6.772, 3.986, 2.712, 2.359, 2.239, 2.104, 1.992, 1.916, 1.862, 1.861, 1.802, 1.753, 1.674, 1.713, 13.253, 3.921, 3.034, 2.667, 2.447, 2.259, 2.113, 2.064, 1.973, 1.894, 1.833, 1.806, 7.281, 2.533]\n",
      "\n",
      "Train location_loss (all epochs): 0.7112447023391724 \n",
      " [432.66, 41.54, 24.384, 19.629, 14.824, 14.389, 10.384, 11.106, 8.256, 7.149, 8.067, 7.107, 6.605, 42.759, 9.944, 6.827, 6.293, 5.108, 4.611, 4.101, 3.765, 5.486, 7.002, 3.684, 2.992, 2.938, 2.725, 3.547, 18.749, 4.086, 3.242, 2.858, 2.258, 2.2, 1.951, 1.857, 1.757, 1.772, 5.555, 4.185, 2.521, 1.869, 1.498, 1.327, 1.314, 1.265, 1.3, 1.372, 1.286, 1.341, 8.042, 1.81, 1.384, 1.221, 1.152, 1.028, 1.016, 1.004, 1.04, 7.982, 1.76, 1.166, 1.075, 0.992, 0.923, 0.866, 0.859, 0.86, 0.86, 0.853, 0.814, 0.883, 6.299, 2.53, 1.222, 0.942, 0.91, 0.835, 0.775, 0.76, 0.756, 0.804, 0.781, 0.769, 0.711, 0.806, 13.554, 2.487, 1.56, 1.225, 1.057, 0.913, 0.819, 0.823, 0.762, 0.734, 0.723, 0.732, 6.68, 1.208]\n",
      "\n",
      "Train power_loss (all epochs): 0.37283995747566223 \n",
      " [869.245, 23.884, 11.854, 8.979, 6.143, 5.407, 4.34, 4.364, 3.856, 3.201, 3.285, 3.271, 2.986, 10.544, 3.547, 3.027, 2.453, 2.43, 1.928, 1.891, 1.89, 2.04, 2.28, 1.598, 1.47, 1.411, 1.289, 1.542, 4.495, 1.952, 1.587, 1.486, 1.265, 1.173, 0.988, 1.07, 1.052, 1.162, 2.134, 1.561, 1.413, 0.914, 0.83, 0.846, 0.813, 0.805, 0.795, 0.777, 0.705, 0.846, 3.238, 1.553, 1.115, 0.964, 0.784, 0.72, 0.635, 0.783, 0.797, 2.16, 0.896, 0.771, 0.634, 0.598, 0.593, 0.521, 0.48, 0.58, 0.646, 0.527, 0.505, 0.517, 2.252, 1.492, 0.966, 0.714, 0.555, 0.491, 0.511, 0.446, 0.434, 0.459, 0.439, 0.428, 0.481, 0.393, 3.439, 0.987, 0.818, 0.686, 0.546, 0.49, 0.411, 0.395, 0.518, 0.438, 0.373, 0.442, 1.792, 0.613]\n",
      "\n",
      "Train tx_exist_loss (all epochs): 0.0007963315001688898 \n",
      " [0.128, 0.013, 0.008, 0.007, 0.006, 0.006, 0.006, 0.005, 0.005, 0.004, 0.004, 0.004, 0.004, 0.008, 0.004, 0.004, 0.004, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.003, 0.002, 0.002, 0.002, 0.002, 0.005, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.002, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001, 0.001, 0.003, 0.001, 0.001, 0.001, 0.001, 0.001, 0.003, 0.001, 0.001, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.005, 0.002, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.001, 0.002, 0.001]\n",
      "\n",
      "Train location_location_mae2 (all epochs): 0.6371420621871948 \n",
      " [12.957, 4.781, 3.713, 3.306, 2.914, 2.818, 2.424, 2.471, 2.195, 2.028, 2.105, 1.983, 1.948, 3.893, 2.312, 1.946, 1.856, 1.708, 1.622, 1.524, 1.473, 1.59, 1.856, 1.446, 1.31, 1.298, 1.256, 1.328, 2.513, 1.5, 1.35, 1.268, 1.128, 1.122, 1.05, 1.031, 1.001, 1.007, 1.386, 1.422, 1.159, 1.03, 0.922, 0.867, 0.863, 0.849, 0.861, 0.883, 0.858, 0.873, 1.745, 1.004, 0.881, 0.828, 0.808, 0.765, 0.763, 0.758, 0.771, 1.746, 0.978, 0.805, 0.774, 0.746, 0.721, 0.701, 0.701, 0.699, 0.701, 0.696, 0.683, 0.713, 1.325, 1.136, 0.823, 0.724, 0.712, 0.679, 0.662, 0.658, 0.659, 0.677, 0.67, 0.665, 0.638, 0.677, 1.876, 1.139, 0.911, 0.81, 0.754, 0.703, 0.668, 0.673, 0.647, 0.637, 0.638, 0.641, 1.484, 0.807]\n",
      "\n",
      "Train power_mae (all epochs): 0.4396664500236511 \n",
      " [18.358, 3.689, 2.527, 2.189, 1.854, 1.719, 1.572, 1.528, 1.478, 1.358, 1.353, 1.328, 1.285, 2.018, 1.381, 1.287, 1.167, 1.166, 1.035, 1.037, 1.031, 1.04, 1.108, 0.945, 0.901, 0.88, 0.848, 0.899, 1.366, 1.033, 0.94, 0.906, 0.838, 0.808, 0.741, 0.777, 0.765, 0.785, 1.004, 0.944, 0.885, 0.728, 0.696, 0.692, 0.675, 0.674, 0.667, 0.657, 0.633, 0.695, 1.191, 0.918, 0.786, 0.732, 0.666, 0.64, 0.602, 0.645, 0.651, 1.042, 0.697, 0.649, 0.586, 0.569, 0.571, 0.533, 0.517, 0.552, 0.572, 0.525, 0.519, 0.518, 0.89, 0.852, 0.696, 0.604, 0.542, 0.513, 0.514, 0.481, 0.474, 0.49, 0.481, 0.472, 0.494, 0.455, 1.086, 0.704, 0.632, 0.58, 0.53, 0.501, 0.464, 0.456, 0.502, 0.472, 0.44, 0.467, 0.825, 0.544]\n",
      "\n",
      "Train tx_exist_accuracy (all epochs): 0.967519998550415 \n",
      " [0.968, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "*****Validation Info*****\n",
      "\n",
      "Validation loss (all epochs): 1.8267637491226196 \n",
      " [2071.004, 153.411, 84.252, 28.271, 95.512, 11.043, 519.997, 1064.004, 6.78, 7.648, 5.43, 81.651, 17.797, 16.425, 66.558, 6.523, 51.844, 61.543, 7.764, 5.616, 4.231, 317.089, 7.957, 4.401, 5.196, 4.4, 3.999, 384.605, 9.226, 4.805, 4.556, 4.385, 3.611, 3.56, 3.543, 3.529, 4.062, 3.788, 1032.168, 316.515, 3.192, 3.319, 2.958, 3.194, 2.751, 2.822, 2.965, 2.849, 6.56, 6.15, 675.797, 3.621, 3.116, 2.812, 2.638, 2.51, 2.43, 2.667, 2.852, 896.223, 2.92, 2.633, 2.696, 2.639, 2.298, 2.344, 2.187, 2.031, 2.22, 2.091, 2.254, 2.17, 1319.492, 2.724, 2.532, 2.575, 2.488, 2.185, 1.963, 2.797, 2.163, 1.827, 1.982, 1.958, 24.526, 3.935, 843.133, 3.411, 2.859, 2.541, 2.623, 2.339, 2.205, 2.166, 2.707, 2.152, 2.048, 2.066, 1153.613, 2.438]\n",
      "\n",
      "Validation location_loss (all epochs): 0.7967987656593323 \n",
      " [2211.018, 166.49, 95.306, 18.7, 105.938, 11.186, 576.354, 1136.593, 6.078, 7.079, 4.48, 85.533, 16.934, 15.949, 72.85, 4.393, 55.484, 69.658, 6.866, 4.14, 2.768, 368.696, 7.113, 2.904, 3.926, 3.025, 2.616, 441.015, 8.137, 3.119, 2.619, 2.627, 1.903, 1.889, 1.887, 1.992, 2.632, 2.395, 1056.881, 358.657, 1.551, 1.698, 1.402, 1.827, 1.37, 1.519, 1.679, 1.654, 1.884, 5.017, 776.006, 1.455, 1.409, 1.343, 1.182, 1.125, 1.15, 1.502, 1.404, 915.836, 1.425, 1.174, 1.368, 1.345, 1.043, 1.099, 1.036, 0.877, 1.172, 1.08, 1.231, 1.281, 1401.993, 1.267, 1.044, 1.302, 1.248, 0.982, 0.797, 1.816, 1.164, 0.826, 1.032, 0.969, 22.339, 0.999, 859.657, 2.014, 1.449, 1.167, 1.121, 1.073, 0.994, 0.954, 1.401, 1.052, 0.813, 1.054, 1252.996, 1.102]\n",
      "\n",
      "Validation power_loss (all epochs): 0.11490108072757721 \n",
      " [1904.619, 106.899, 19.979, 110.693, 41.488, 1.847, 287.024, 964.604, 2.043, 2.185, 1.903, 74.951, 19.455, 9.124, 26.685, 8.521, 27.512, 4.311, 0.639, 2.612, 0.728, 18.798, 0.547, 1.173, 0.899, 0.995, 0.903, 80.359, 1.424, 0.353, 2.629, 1.427, 0.457, 0.631, 1.076, 0.62, 1.056, 0.825, 1318.802, 97.564, 0.551, 1.267, 0.865, 0.347, 0.356, 0.366, 0.931, 0.294, 35.889, 5.441, 142.779, 6.079, 2.32, 0.618, 0.899, 0.834, 0.365, 0.278, 3.503, 1158.699, 0.315, 0.433, 0.149, 0.425, 0.201, 0.79, 0.325, 0.592, 0.431, 0.319, 1.068, 0.158, 1259.701, 0.158, 1.117, 0.174, 0.481, 0.392, 0.324, 0.524, 0.194, 0.115, 0.232, 0.928, 45.359, 21.051, 1105.803, 0.346, 0.339, 0.238, 2.063, 0.221, 0.121, 0.562, 2.656, 0.529, 1.968, 0.472, 870.285, 0.97]\n",
      "\n",
      "Validation tx_exist_loss (all epochs): 1.728533061395865e-05 \n",
      " [0.721, 0.168, 0.016, 0.423, 0.036, 0.019, 0.525, 0.821, 0.01, 0.006, 0.019, 0.099, 0.014, 0.056, 0.301, 0.005, 0.218, 0.144, 0.001, 0.004, 0.004, 0.088, 0.085, 0.025, 0.005, 0.011, 0.003, 0.161, 0.077, 0.002, 0.005, 0.001, 0.002, 0.013, 0.002, 0.011, 0.011, 0.002, 1.564, 0.457, 0.005, 0.001, 0.03, 0.003, 0.014, 0.004, 0.011, 0.092, 0.068, 0.002, 0.586, 0.0, 0.014, 0.002, 0.068, 0.001, 0.127, 0.142, 0.015, 2.484, 0.049, 0.002, 0.001, 0.0, 0.002, 0.003, 0.002, 0.003, 0.015, 0.007, 0.0, 0.003, 1.927, 0.047, 0.048, 0.061, 0.001, 0.002, 0.008, 0.001, 0.001, 0.006, 0.11, 0.043, 0.0, 0.04, 1.732, 0.007, 0.027, 0.001, 0.011, 0.002, 0.003, 0.001, 0.0, 0.002, 0.001, 0.0, 1.015, 0.0]\n",
      "\n",
      "Validation location_location_mae2 (all epochs): 0.5445640683174133 \n",
      " [38.624, 7.687, 6.697, 3.2, 5.241, 2.555, 12.496, 18.286, 1.621, 1.997, 1.589, 4.815, 2.954, 2.525, 4.262, 1.422, 3.144, 3.807, 2.006, 1.375, 1.143, 9.251, 2.015, 1.156, 1.399, 1.165, 1.079, 9.426, 1.976, 1.079, 1.044, 1.118, 0.836, 0.861, 0.862, 0.879, 1.121, 0.953, 16.362, 10.223, 0.755, 0.854, 0.746, 0.935, 0.74, 0.778, 0.838, 0.877, 0.912, 1.446, 13.681, 0.77, 0.768, 0.778, 0.663, 0.633, 0.674, 0.788, 0.729, 19.497, 0.719, 0.646, 0.725, 0.772, 0.614, 0.656, 0.63, 0.59, 0.661, 0.66, 0.674, 0.746, 19.634, 0.719, 0.638, 0.762, 0.759, 0.614, 0.545, 1.014, 0.722, 0.564, 0.657, 0.676, 2.162, 0.613, 15.283, 0.925, 0.779, 0.671, 0.633, 0.601, 0.581, 0.6, 0.811, 0.64, 0.566, 0.692, 16.84, 0.66]\n",
      "\n",
      "Validation power_mae (all epochs): 0.2531953454017639 \n",
      " [28.778, 6.006, 2.692, 6.017, 3.51, 0.977, 8.262, 15.881, 1.089, 1.086, 1.033, 4.592, 2.346, 2.011, 2.935, 1.832, 3.209, 1.37, 0.626, 1.216, 0.68, 3.35, 0.575, 0.908, 0.803, 0.766, 0.761, 5.742, 0.848, 0.476, 1.08, 1.031, 0.547, 0.592, 0.864, 0.59, 0.808, 0.672, 22.027, 5.816, 0.582, 0.986, 0.784, 0.459, 0.449, 0.502, 0.849, 0.427, 3.37, 2.011, 6.282, 1.673, 1.008, 0.65, 0.696, 0.658, 0.507, 0.408, 1.376, 17.104, 0.44, 0.534, 0.3, 0.476, 0.364, 0.677, 0.433, 0.627, 0.484, 0.419, 0.86, 0.285, 20.109, 0.301, 0.71, 0.315, 0.527, 0.455, 0.438, 0.561, 0.332, 0.255, 0.349, 0.724, 3.188, 2.303, 16.693, 0.436, 0.403, 0.344, 0.856, 0.341, 0.253, 0.512, 0.997, 0.483, 0.852, 0.506, 15.113, 0.744]\n",
      "\n",
      "Validation tx_exist_accuracy (all epochs): 0.7987474799156189 \n",
      " [0.799, 0.901, 0.997, 0.823, 0.994, 1.0, 0.799, 0.799, 1.0, 1.0, 1.0, 0.955, 0.998, 0.999, 0.799, 1.0, 0.843, 0.91, 1.0, 1.0, 1.0, 0.968, 0.998, 1.0, 1.0, 1.0, 1.0, 0.92, 0.989, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 0.998, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 0.966, 0.913, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.993, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 1547.89392, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 1547.89392 to 793.55017, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 793.55017 to 31.62316, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 31.62316\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 31.62316\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 31.62316\n",
      "\n",
      "Epoch 00007: val_loss improved from 31.62316 to 22.93087, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 22.93087\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 22.93087\n",
      "\n",
      "Epoch 00010: val_loss improved from 22.93087 to 17.68402, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 17.68402\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 17.68402\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 17.68402\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 17.68402\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 17.68402\n",
      "\n",
      "Epoch 00016: val_loss improved from 17.68402 to 10.47636, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 10.47636\n",
      "\n",
      "Epoch 00027: val_loss improved from 10.47636 to 9.25629, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00028: val_loss improved from 9.25629 to 7.81425, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00031: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 7.81425\n",
      "\n",
      "Epoch 00043: val_loss improved from 7.81425 to 6.69743, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 6.69743\n",
      "\n",
      "Epoch 00045: val_loss improved from 6.69743 to 6.36431, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 6.36431\n",
      "\n",
      "Epoch 00052: val_loss improved from 6.36431 to 6.34093, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00053: val_loss improved from 6.34093 to 6.16034, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 6.16034\n",
      "\n",
      "Epoch 00060: val_loss improved from 6.16034 to 6.06143, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 6.06143\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 6.06143\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 6.06143\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 6.06143\n",
      "\n",
      "Epoch 00065: val_loss improved from 6.06143 to 5.58180, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 5.58180\n",
      "\n",
      "Epoch 00073: val_loss improved from 5.58180 to 5.06568, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 5.06568\n",
      "\n",
      "Epoch 00075: val_loss improved from 5.06568 to 4.93885, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 4.93885\n",
      "\n",
      "Epoch 00087: val_loss improved from 4.93885 to 4.78519, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 4.78519\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 4.78519\n",
      "\n",
      "Epoch 00090: val_loss improved from 4.78519 to 4.40309, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_2.h5\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 4.40309\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 4.40309\n",
      "\n",
      "\n",
      "Lambda: 0.01 , Time: 1:28:44\n",
      "\n",
      "*****Train Info*****\n",
      "\n",
      "Train loss (all epochs): 4.318673133850098 \n",
      " [470.618, 46.886, 33.321, 26.772, 22.922, 21.361, 19.739, 17.354, 23.028, 20.707, 14.893, 14.086, 12.696, 13.64, 12.803, 11.299, 16.046, 13.264, 11.035, 13.217, 10.52, 11.35, 26.348, 13.163, 11.263, 10.872, 9.873, 9.109, 8.595, 18.628, 16.213, 11.597, 10.059, 9.496, 8.691, 8.459, 7.935, 8.065, 16.854, 9.076, 10.802, 7.927, 7.478, 7.068, 6.761, 13.231, 15.053, 9.2, 7.853, 7.249, 6.648, 7.183, 6.448, 10.719, 13.498, 8.378, 7.302, 6.881, 6.231, 5.893, 5.52, 14.392, 8.805, 7.122, 6.278, 5.995, 5.64, 12.604, 8.548, 6.455, 5.909, 5.411, 5.226, 5.036, 4.791, 4.519, 4.604, 17.095, 7.676, 6.409, 5.713, 5.335, 5.07, 11.258, 6.179, 5.287, 5.022, 4.637, 4.531, 4.319, 26.435, 11.223, 8.946, 7.679, 7.09, 6.493, 6.236, 6.081, 6.083, 8.849]\n",
      "\n",
      "Train location_loss (all epochs): 1.5202096700668335 \n",
      " [438.341, 39.08, 24.935, 17.984, 14.124, 12.892, 11.522, 9.295, 16.207, 13.029, 6.851, 6.68, 5.608, 7.183, 6.396, 5.035, 10.548, 6.473, 4.708, 7.436, 4.606, 5.731, 19.518, 5.231, 4.153, 4.499, 3.861, 3.542, 3.365, 14.625, 7.914, 4.255, 3.428, 3.474, 3.164, 3.309, 3.081, 3.532, 11.847, 3.249, 5.411, 2.777, 2.784, 2.737, 2.7, 10.123, 8.105, 3.06, 2.441, 2.39, 2.207, 3.034, 2.463, 7.664, 7.243, 2.787, 2.401, 2.447, 2.13, 2.097, 1.947, 10.223, 3.369, 2.353, 2.0, 2.127, 1.975, 9.245, 3.554, 2.013, 1.919, 1.726, 1.847, 1.872, 1.815, 1.672, 1.904, 12.991, 2.683, 2.074, 1.82, 1.737, 1.751, 7.676, 2.018, 1.6, 1.666, 1.52, 1.631, 1.578, 23.121, 4.347, 2.996, 2.387, 2.259, 2.063, 2.107, 2.168, 2.463, 5.093]\n",
      "\n",
      "Train power_loss (all epochs): 1.2364871501922607 \n",
      " [875.692, 26.79, 12.17, 8.72, 7.041, 6.424, 5.725, 4.894, 6.574, 5.28, 4.215, 4.274, 3.921, 3.854, 3.508, 3.489, 4.396, 3.568, 3.141, 3.446, 2.734, 3.574, 7.232, 2.948, 2.855, 2.226, 2.269, 2.001, 2.06, 4.991, 4.772, 2.653, 2.679, 2.707, 2.141, 1.961, 1.78, 1.798, 3.697, 2.013, 2.185, 1.612, 1.721, 1.555, 1.788, 3.043, 3.835, 2.225, 1.969, 1.831, 1.681, 1.989, 1.763, 2.512, 4.179, 2.459, 1.913, 1.819, 1.526, 1.52, 1.382, 4.176, 2.768, 2.111, 1.959, 1.549, 1.901, 3.262, 1.832, 1.443, 1.531, 1.551, 1.347, 1.286, 1.272, 1.346, 1.364, 4.772, 2.395, 1.924, 1.694, 1.834, 1.497, 3.28, 1.689, 1.252, 1.49, 1.306, 1.289, 1.236, 6.717, 4.48, 3.456, 2.648, 2.589, 1.923, 2.09, 1.678, 1.561, 2.11]\n",
      "\n",
      "Train tx_exist_loss (all epochs): 0.005027048289775848 \n",
      " [0.183, 0.067, 0.056, 0.049, 0.045, 0.039, 0.036, 0.033, 0.032, 0.031, 0.027, 0.026, 0.024, 0.023, 0.022, 0.021, 0.021, 0.019, 0.018, 0.018, 0.017, 0.016, 0.023, 0.016, 0.015, 0.014, 0.014, 0.013, 0.013, 0.015, 0.016, 0.013, 0.012, 0.012, 0.011, 0.011, 0.01, 0.01, 0.012, 0.01, 0.01, 0.009, 0.009, 0.009, 0.009, 0.01, 0.012, 0.01, 0.009, 0.008, 0.008, 0.008, 0.008, 0.009, 0.014, 0.01, 0.008, 0.008, 0.007, 0.007, 0.007, 0.012, 0.01, 0.008, 0.007, 0.007, 0.007, 0.008, 0.007, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.006, 0.012, 0.008, 0.006, 0.006, 0.006, 0.006, 0.008, 0.007, 0.006, 0.005, 0.005, 0.005, 0.005, 0.011, 0.01, 0.009, 0.008, 0.008, 0.007, 0.006, 0.006, 0.006, 0.007]\n",
      "\n",
      "Train location_location_mae2 (all epochs): 0.932375431060791 \n",
      " [12.8, 4.627, 3.71, 3.162, 2.818, 2.695, 2.571, 2.308, 2.632, 2.615, 1.978, 1.961, 1.802, 1.98, 1.908, 1.716, 2.144, 1.88, 1.664, 1.934, 1.635, 1.76, 2.761, 1.721, 1.549, 1.603, 1.504, 1.422, 1.4, 2.203, 2.002, 1.534, 1.391, 1.412, 1.348, 1.373, 1.331, 1.393, 2.201, 1.349, 1.577, 1.263, 1.265, 1.261, 1.246, 1.716, 1.946, 1.302, 1.174, 1.163, 1.129, 1.287, 1.191, 1.439, 1.816, 1.243, 1.165, 1.186, 1.11, 1.099, 1.059, 1.917, 1.305, 1.147, 1.064, 1.097, 1.061, 1.774, 1.319, 1.065, 1.049, 0.993, 1.028, 1.039, 1.024, 0.98, 1.049, 2.067, 1.216, 1.072, 1.014, 0.988, 0.995, 1.772, 1.053, 0.949, 0.972, 0.932, 0.963, 0.944, 2.449, 1.515, 1.277, 1.137, 1.113, 1.061, 1.069, 1.092, 1.121, 1.463]\n",
      "\n",
      "Train power_mae (all epochs): 0.7857033610343933 \n",
      " [18.439, 3.962, 2.536, 2.181, 1.919, 1.805, 1.725, 1.608, 1.775, 1.651, 1.519, 1.491, 1.448, 1.438, 1.377, 1.359, 1.431, 1.368, 1.269, 1.325, 1.175, 1.336, 1.785, 1.249, 1.224, 1.075, 1.09, 1.039, 1.043, 1.391, 1.523, 1.178, 1.162, 1.163, 1.06, 1.008, 0.973, 0.981, 1.334, 1.031, 1.053, 0.909, 0.931, 0.9, 0.95, 1.145, 1.342, 1.066, 1.001, 0.96, 0.928, 0.995, 0.932, 1.022, 1.374, 1.085, 0.982, 0.951, 0.895, 0.892, 0.848, 1.32, 1.156, 1.016, 0.982, 0.899, 0.959, 1.19, 0.962, 0.857, 0.883, 0.88, 0.817, 0.811, 0.802, 0.826, 0.83, 1.411, 1.07, 0.976, 0.918, 0.945, 0.863, 1.207, 0.925, 0.805, 0.861, 0.811, 0.812, 0.786, 1.524, 1.368, 1.23, 1.112, 1.091, 0.976, 0.997, 0.915, 0.874, 1.001]\n",
      "\n",
      "Train tx_exist_accuracy (all epochs): 0.9632400274276733 \n",
      " [0.963, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "*****Validation Info*****\n",
      "\n",
      "Validation loss (all epochs): 4.403092861175537 \n",
      " [1547.894, 793.55, 31.623, 1064.587, 330.371, 612.795, 22.931, 31.333, 1809.572, 17.684, 32.903, 75.96, 1209.141, 1465.013, 1354.08, 10.476, 1493.06, 1160.974, 14.225, 27.335, 12.842, 1150.168, 15.189, 11.584, 19.984, 10.644, 9.256, 7.814, 10.374, 1628.367, 12.594, 11.891, 9.216, 8.454, 9.317, 9.106, 9.751, 343.721, 9.314, 9.897, 9.142, 9.105, 6.697, 7.348, 6.364, 2193.078, 10.293, 8.236, 9.557, 7.277, 6.816, 6.341, 6.16, 874.77, 12.579, 7.745, 7.565, 6.329, 6.642, 6.061, 1008.309, 729.369, 7.556, 6.546, 5.582, 7.022, 5.65, 1008.43, 6.727, 6.24, 6.421, 5.669, 5.066, 5.588, 4.939, 8.643, 5.342, 217.424, 8.65, 6.403, 6.839, 5.123, 8.728, 17.841, 6.039, 5.006, 4.785, 5.107, 8.386, 4.403, 181.094, 2423.305, 8.22, 7.154, 7.163, 7.045, 2037.273, 5.499, 2188.792, 118.128]\n",
      "\n",
      "Validation location_loss (all epochs): 1.31371009349823 \n",
      " [1680.97, 874.234, 23.565, 1188.279, 375.367, 686.931, 15.526, 23.457, 1929.553, 6.722, 23.544, 79.596, 1348.95, 1594.9, 1466.368, 4.513, 1635.471, 1310.769, 4.797, 20.352, 7.388, 1216.482, 5.307, 3.105, 12.966, 4.164, 2.959, 2.428, 2.906, 1669.408, 3.676, 3.095, 2.991, 2.378, 4.092, 4.37, 5.3, 346.061, 2.99, 4.8, 2.928, 4.066, 1.957, 2.893, 1.964, 2279.081, 3.103, 1.894, 4.693, 2.801, 1.915, 2.126, 2.198, 741.425, 4.592, 1.451, 2.944, 2.067, 1.489, 2.142, 1112.021, 761.387, 2.698, 1.497, 1.494, 2.265, 2.094, 1063.67, 1.797, 1.966, 2.114, 1.913, 1.777, 2.439, 1.394, 2.27, 2.231, 189.913, 2.427, 2.206, 2.163, 1.527, 3.007, 6.43, 1.783, 1.54, 1.484, 2.079, 1.314, 1.789, 146.317, 2638.713, 2.895, 2.247, 2.459, 1.771, 2190.077, 1.545, 2366.082, 87.608]\n",
      "\n",
      "Validation power_loss (all epochs): 0.35171905159950256 \n",
      " [1081.232, 394.442, 8.256, 440.957, 13.209, 192.984, 5.489, 26.745, 1600.961, 29.812, 46.07, 4.896, 552.268, 1021.697, 1007.716, 0.911, 951.825, 397.946, 37.076, 34.432, 4.64, 1100.264, 16.438, 10.658, 18.936, 4.833, 6.561, 0.416, 25.043, 2003.321, 12.047, 19.831, 1.462, 4.062, 2.168, 0.764, 2.553, 446.252, 1.37, 1.044, 9.362, 5.127, 2.816, 4.665, 4.955, 2481.133, 7.039, 6.791, 2.742, 0.85, 7.666, 1.981, 2.893, 2384.798, 24.386, 11.448, 2.626, 1.246, 12.601, 3.875, 594.874, 759.852, 0.352, 6.519, 1.225, 11.901, 2.468, 984.753, 2.99, 2.419, 6.746, 3.932, 1.276, 2.983, 6.911, 38.27, 6.533, 502.584, 18.541, 3.356, 11.933, 2.597, 28.39, 76.858, 4.943, 0.933, 1.735, 2.504, 43.372, 0.91, 488.722, 1738.691, 0.771, 1.149, 3.518, 11.548, 1715.12, 2.421, 1736.256, 393.381]\n",
      "\n",
      "Validation tx_exist_loss (all epochs): 0.0011075371876358986 \n",
      " [0.322, 0.287, 0.09, 0.363, 0.095, 0.119, 0.047, 0.112, 0.715, 0.048, 0.179, 0.027, 0.211, 0.363, 0.625, 0.042, 0.841, 0.387, 0.048, 0.044, 0.013, 1.098, 0.086, 0.043, 0.027, 0.011, 0.019, 0.011, 0.019, 1.861, 0.054, 0.019, 0.021, 0.037, 0.014, 0.019, 0.011, 0.469, 0.058, 0.014, 0.009, 0.004, 0.005, 0.003, 0.026, 1.891, 0.101, 0.01, 0.018, 0.017, 0.005, 0.016, 0.01, 0.021, 0.054, 0.167, 0.018, 0.01, 0.01, 0.045, 0.265, 0.494, 0.014, 0.08, 0.005, 0.043, 0.001, 0.771, 0.04, 0.017, 0.027, 0.003, 0.009, 0.013, 0.007, 0.021, 0.011, 0.391, 0.061, 0.028, 0.017, 0.007, 0.024, 0.024, 0.056, 0.109, 0.008, 0.01, 0.008, 0.006, 0.251, 0.962, 0.055, 0.057, 0.017, 0.003, 0.574, 0.018, 0.804, 0.143]\n",
      "\n",
      "Validation location_location_mae2 (all epochs): 0.7705629467964172 \n",
      " [29.438, 17.084, 3.525, 17.962, 7.556, 10.443, 3.084, 2.812, 29.325, 1.747, 2.778, 4.379, 20.679, 26.004, 22.053, 1.61, 23.148, 18.735, 1.438, 1.614, 1.594, 22.03, 1.59, 1.26, 1.87, 1.384, 1.225, 1.067, 1.137, 34.046, 1.334, 1.201, 1.257, 1.013, 1.469, 1.573, 1.765, 10.907, 1.232, 1.412, 1.201, 1.463, 0.938, 1.227, 0.96, 38.066, 1.202, 0.892, 1.703, 1.212, 0.923, 0.986, 0.975, 17.855, 1.589, 0.794, 1.305, 0.99, 0.807, 0.931, 16.015, 11.737, 1.02, 0.789, 0.785, 1.016, 0.998, 16.697, 0.877, 1.006, 0.971, 0.935, 0.952, 1.119, 0.771, 1.004, 1.062, 7.037, 1.001, 1.08, 1.011, 0.791, 1.231, 1.543, 0.958, 0.858, 0.864, 0.985, 0.799, 0.89, 4.447, 43.03, 1.147, 0.97, 1.046, 0.873, 39.045, 0.812, 40.504, 5.116]\n",
      "\n",
      "Validation power_mae (all epochs): 0.4206128418445587 \n",
      " [23.156, 10.615, 2.166, 10.565, 2.242, 8.927, 1.512, 2.713, 22.887, 3.334, 3.563, 1.331, 11.233, 16.221, 15.873, 0.782, 16.375, 9.952, 3.683, 3.4, 1.364, 16.662, 2.39, 2.1, 2.966, 1.849, 1.88, 0.493, 2.914, 32.526, 2.38, 2.943, 1.026, 1.592, 1.012, 0.712, 1.25, 10.231, 0.961, 0.757, 1.993, 1.559, 1.235, 1.497, 1.467, 37.323, 1.892, 1.937, 1.394, 0.696, 2.556, 1.143, 1.44, 36.946, 3.04, 2.663, 1.281, 0.892, 1.882, 1.551, 11.498, 14.403, 0.421, 2.079, 0.853, 2.721, 1.331, 16.393, 1.369, 1.22, 2.353, 1.268, 0.933, 1.48, 2.065, 3.109, 2.045, 10.689, 2.405, 1.635, 2.567, 1.047, 2.876, 4.479, 1.579, 0.779, 1.071, 1.203, 4.078, 0.7, 10.588, 24.67, 0.721, 0.849, 1.32, 2.121, 24.316, 0.952, 24.905, 9.832]\n",
      "\n",
      "Validation tx_exist_accuracy (all epochs): 0.7982626557350159 \n",
      " [0.943, 0.808, 1.0, 0.799, 0.998, 0.951, 1.0, 0.964, 0.799, 1.0, 0.875, 0.997, 0.853, 0.799, 0.799, 1.0, 0.799, 0.799, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.798, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.991, 1.0, 0.995, 1.0, 1.0, 1.0, 1.0, 0.818, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.801, 0.799, 1.0, 1.0, 1.0, 1.0, 0.799, 1.0, 0.799, 0.917]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2620.72095, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2620.72095 to 131.20673, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 131.20673 to 92.49831, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00004: val_loss improved from 92.49831 to 71.61945, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00005: val_loss did not improve from 71.61945\n",
      "\n",
      "Epoch 00006: val_loss improved from 71.61945 to 49.71600, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 49.71600\n",
      "\n",
      "Epoch 00008: val_loss improved from 49.71600 to 44.81619, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00009: val_loss improved from 44.81619 to 36.14023, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 36.14023\n",
      "\n",
      "Epoch 00011: val_loss improved from 36.14023 to 34.95892, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 34.95892\n",
      "\n",
      "Epoch 00023: val_loss improved from 34.95892 to 31.50688, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 31.50688\n",
      "\n",
      "Epoch 00030: val_loss improved from 31.50688 to 30.71541, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00031: val_loss improved from 30.71541 to 27.52709, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 27.52709\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 27.52709\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 27.52709\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 27.52709\n",
      "\n",
      "Epoch 00036: val_loss improved from 27.52709 to 23.48334, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 23.48334\n",
      "\n",
      "Epoch 00047: val_loss improved from 23.48334 to 17.60969, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 17.60969\n",
      "\n",
      "Epoch 00072: val_loss improved from 17.60969 to 17.58513, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 17.58513\n",
      "\n",
      "Epoch 00085: val_loss improved from 17.58513 to 13.97438, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 13.97438\n",
      "\n",
      "Epoch 00098: val_loss improved from 13.97438 to 12.81233, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_3.h5\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 12.81233\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 12.81233\n",
      "\n",
      "\n",
      "Lambda: 0.1 , Time: 1:28:46\n",
      "\n",
      "*****Train Info*****\n",
      "\n",
      "Train loss (all epochs): 14.022080421447754 \n",
      " [555.071, 121.318, 88.461, 70.929, 58.945, 55.104, 49.769, 44.572, 42.878, 41.411, 38.632, 31.359, 66.936, 44.541, 35.957, 32.01, 30.794, 31.038, 35.067, 25.536, 27.739, 30.834, 30.438, 23.173, 38.47, 24.81, 23.045, 27.446, 39.276, 24.41, 24.665, 20.463, 35.753, 25.85, 21.069, 19.526, 41.539, 31.863, 25.334, 22.184, 21.925, 25.09, 17.846, 32.693, 27.808, 19.345, 17.158, 21.201, 28.192, 19.796, 17.083, 24.424, 22.681, 17.773, 16.061, 35.587, 23.718, 20.328, 19.106, 17.686, 30.149, 18.434, 16.447, 20.536, 17.575, 15.23, 54.021, 33.354, 25.962, 22.682, 20.546, 19.928, 19.849, 22.246, 16.82, 20.335, 23.696, 16.889, 17.587, 18.301, 15.253, 21.528, 17.786, 18.538, 14.695, 14.104, 34.059, 22.624, 18.287, 16.86, 21.826, 20.071, 15.598, 19.098, 16.489, 15.221, 20.531, 14.585, 14.022, 14.551]\n",
      "\n",
      "Train location_loss (all epochs): 4.33730411529541 \n",
      " [443.193, 48.501, 29.771, 22.626, 18.354, 18.703, 17.844, 14.176, 15.758, 15.478, 13.141, 9.491, 41.269, 14.277, 11.113, 10.095, 10.625, 12.674, 15.183, 7.892, 12.43, 13.539, 12.129, 8.062, 18.278, 8.318, 8.777, 12.205, 19.239, 7.254, 9.479, 7.211, 19.43, 9.095, 6.796, 7.2, 24.746, 9.211, 8.717, 7.362, 9.167, 10.332, 5.538, 17.63, 10.985, 5.961, 5.601, 10.556, 13.376, 6.33, 5.835, 13.467, 7.88, 5.98, 5.665, 18.821, 7.59, 6.247, 7.031, 6.866, 13.987, 5.574, 5.453, 9.73, 6.613, 5.399, 35.207, 9.58, 7.173, 6.521, 6.022, 6.582, 7.805, 8.196, 4.755, 9.739, 9.852, 4.918, 6.644, 6.844, 4.745, 10.532, 5.929, 7.267, 4.561, 4.971, 21.25, 6.264, 5.306, 5.465, 11.314, 6.113, 4.632, 8.852, 5.307, 5.697, 9.288, 4.337, 4.838, 5.87]\n",
      "\n",
      "Train power_loss (all epochs): 7.314911842346191 \n",
      " [851.38, 28.379, 15.588, 10.727, 9.625, 9.036, 9.843, 8.309, 8.653, 9.804, 8.381, 8.005, 12.954, 8.312, 7.992, 8.289, 8.257, 8.828, 9.213, 8.202, 9.02, 9.006, 9.232, 7.9, 10.864, 8.26, 8.599, 9.021, 9.935, 7.933, 8.644, 7.792, 10.385, 8.158, 8.036, 8.016, 11.744, 8.003, 8.241, 7.857, 8.573, 8.619, 7.638, 10.978, 8.817, 7.675, 7.866, 8.451, 9.361, 8.219, 7.931, 8.959, 8.095, 8.285, 7.92, 10.487, 8.315, 8.058, 8.074, 8.419, 8.97, 7.649, 7.85, 8.226, 7.828, 7.881, 13.033, 7.9, 8.094, 8.025, 7.958, 7.803, 8.093, 7.953, 7.774, 8.433, 8.083, 8.042, 8.06, 7.931, 7.941, 8.571, 7.725, 8.041, 7.695, 7.685, 10.548, 7.838, 7.788, 7.85, 8.26, 7.869, 7.799, 8.231, 7.596, 7.752, 8.144, 7.44, 7.315, 7.92]\n",
      "\n",
      "Train tx_exist_loss (all epochs): 0.03855002298951149 \n",
      " [0.371, 0.362, 0.419, 0.432, 0.425, 0.412, 0.395, 0.365, 0.323, 0.287, 0.252, 0.227, 0.234, 0.206, 0.187, 0.176, 0.165, 0.157, 0.151, 0.141, 0.136, 0.13, 0.127, 0.119, 0.126, 0.112, 0.108, 0.104, 0.106, 0.099, 0.096, 0.092, 0.093, 0.089, 0.085, 0.082, 0.09, 0.084, 0.078, 0.075, 0.074, 0.071, 0.069, 0.073, 0.068, 0.065, 0.064, 0.063, 0.063, 0.06, 0.059, 0.059, 0.057, 0.056, 0.055, 0.066, 0.058, 0.06, 0.055, 0.052, 0.063, 0.054, 0.05, 0.05, 0.047, 0.047, 0.082, 0.083, 0.065, 0.059, 0.055, 0.055, 0.054, 0.056, 0.05, 0.052, 0.056, 0.048, 0.049, 0.048, 0.045, 0.051, 0.046, 0.047, 0.043, 0.042, 0.06, 0.054, 0.045, 0.042, 0.045, 0.046, 0.042, 0.047, 0.043, 0.041, 0.046, 0.039, 0.039, 0.041]\n",
      "\n",
      "Train location_location_mae2 (all epochs): 1.5832499265670776 \n",
      " [13.148, 5.116, 4.053, 3.543, 3.204, 3.176, 3.027, 2.827, 2.93, 2.816, 2.628, 2.347, 3.93, 2.788, 2.506, 2.408, 2.425, 2.58, 2.711, 2.146, 2.472, 2.57, 2.53, 2.145, 2.875, 2.176, 2.249, 2.495, 2.959, 2.054, 2.235, 2.052, 2.957, 2.212, 1.997, 2.046, 3.062, 2.264, 2.163, 2.058, 2.131, 2.31, 1.797, 2.794, 2.304, 1.87, 1.811, 2.238, 2.469, 1.889, 1.814, 2.437, 2.052, 1.855, 1.816, 2.744, 2.016, 1.884, 1.987, 1.853, 2.494, 1.784, 1.783, 2.212, 1.905, 1.744, 3.45, 2.21, 1.953, 1.867, 1.835, 1.887, 1.911, 2.022, 1.655, 2.054, 2.142, 1.66, 1.84, 1.864, 1.643, 2.235, 1.775, 1.882, 1.611, 1.684, 2.619, 1.849, 1.714, 1.749, 2.048, 1.798, 1.621, 1.963, 1.706, 1.681, 2.081, 1.583, 1.655, 1.682]\n",
      "\n",
      "Train power_mae (all epochs): 1.827446460723877 \n",
      " [17.997, 4.224, 2.928, 2.374, 2.188, 2.1, 2.182, 1.994, 2.046, 2.111, 1.95, 1.932, 2.329, 1.93, 1.912, 1.935, 1.945, 2.021, 2.074, 1.932, 2.023, 2.015, 2.053, 1.901, 2.177, 1.955, 1.971, 2.013, 2.151, 1.91, 1.983, 1.891, 2.183, 1.95, 1.92, 1.906, 2.163, 1.924, 1.94, 1.904, 1.952, 1.981, 1.856, 2.198, 2.023, 1.872, 1.906, 1.971, 2.048, 1.926, 1.898, 2.009, 1.929, 1.935, 1.894, 2.131, 1.959, 1.941, 1.926, 1.917, 2.045, 1.857, 1.867, 1.936, 1.872, 1.853, 2.203, 1.893, 1.9, 1.911, 1.904, 1.88, 1.924, 1.919, 1.901, 1.955, 1.91, 1.905, 1.92, 1.903, 1.898, 1.978, 1.883, 1.9, 1.858, 1.866, 2.035, 1.881, 1.88, 1.902, 1.928, 1.911, 1.877, 1.947, 1.85, 1.873, 1.924, 1.837, 1.827, 1.897]\n",
      "\n",
      "Train tx_exist_accuracy (all epochs): 0.8023866415023804 \n",
      " [0.953, 0.937, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.813, 0.886, 0.96, 0.923, 0.982, 0.993, 0.995, 0.997, 0.998, 0.998, 0.999, 0.999, 0.999, 0.999, 1.0, 0.998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 0.998, 0.998, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 0.999, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "*****Validation Info*****\n",
      "\n",
      "Validation loss (all epochs): 12.81232738494873 \n",
      " [2620.721, 131.207, 92.498, 71.619, 130.725, 49.716, 2186.418, 44.816, 36.14, 448.538, 34.959, 35.907, 2222.404, 49.16, 40.584, 44.004, 455.757, 64.558, 39.247, 43.399, 1210.747, 786.066, 31.507, 479.366, 37.759, 38.334, 69.91, 33.797, 1092.191, 30.715, 27.527, 1076.147, 291.484, 35.392, 128.163, 23.483, 81.516, 103.083, 56.073, 31.05, 119.605, 27.589, 44.325, 720.234, 97.314, 289.712, 17.61, 984.885, 519.299, 29.657, 23.538, 2939.412, 23.527, 20.342, 21.109, 801.141, 1770.94, 23.59, 24.867, 594.006, 31.213, 23.536, 21.48, 25.262, 20.18, 28.873, 980.081, 904.785, 24.416, 1040.184, 122.989, 17.585, 3667.041, 22.367, 33.552, 1392.583, 21.276, 26.888, 1199.531, 21.19, 18.581, 957.468, 83.195, 62.285, 13.974, 14.299, 742.316, 54.309, 109.819, 25.093, 1057.565, 17.233, 16.303, 21.107, 15.404, 594.57, 15.552, 12.812, 24.988, 1148.458]\n",
      "\n",
      "Validation location_loss (all epochs): 3.1465911865234375 \n",
      " [2705.213, 66.934, 40.021, 27.674, 102.371, 10.693, 2391.621, 10.724, 6.474, 483.993, 10.591, 11.886, 2421.662, 19.443, 16.33, 18.893, 455.1, 22.422, 16.465, 10.043, 1207.676, 724.79, 11.323, 501.438, 11.108, 12.708, 40.074, 17.887, 1184.056, 15.222, 8.413, 1158.338, 287.013, 14.581, 100.661, 12.937, 42.48, 77.056, 36.024, 16.362, 103.671, 8.714, 25.386, 764.206, 82.735, 313.392, 6.9, 964.889, 543.108, 14.793, 13.646, 3285.989, 10.015, 6.5, 8.601, 829.49, 1906.379, 10.328, 9.134, 559.372, 16.644, 7.92, 4.93, 9.957, 6.776, 14.893, 964.94, 879.219, 6.107, 1138.021, 96.481, 4.379, 4057.153, 8.752, 20.936, 1396.833, 6.961, 15.138, 1244.649, 9.537, 7.488, 958.945, 78.181, 53.616, 3.992, 5.72, 791.759, 40.326, 111.184, 12.628, 1008.149, 4.316, 6.008, 6.692, 4.271, 585.266, 3.501, 3.147, 16.468, 1218.397]\n",
      "\n",
      "Validation power_loss (all epochs): 4.309126853942871 \n",
      " [2344.617, 55.843, 31.17, 20.214, 38.02, 52.037, 1193.609, 58.839, 23.317, 84.45, 18.902, 45.753, 1268.933, 47.073, 27.933, 66.566, 493.222, 251.72, 53.474, 183.711, 1644.926, 1501.085, 46.657, 380.751, 94.079, 118.935, 208.907, 39.97, 655.823, 23.623, 57.356, 785.019, 271.698, 70.402, 294.816, 6.232, 141.093, 186.521, 84.421, 30.889, 154.602, 65.189, 113.326, 518.042, 119.519, 108.824, 6.521, 1499.536, 422.575, 49.995, 12.029, 1271.354, 21.448, 37.075, 37.738, 766.862, 1348.795, 20.739, 53.056, 1065.414, 25.7, 50.211, 68.744, 47.3, 38.248, 62.1, 1299.368, 1365.326, 17.229, 573.259, 270.657, 8.028, 2031.406, 17.613, 44.249, 1909.501, 24.835, 22.38, 1301.364, 22.399, 25.915, 1288.882, 56.244, 60.371, 12.476, 4.309, 491.248, 57.925, 33.132, 25.892, 1829.061, 17.199, 8.565, 32.699, 14.803, 866.755, 13.699, 4.383, 18.7, 1022.595]\n",
      "\n",
      "Validation tx_exist_loss (all epochs): 0.01959013193845749 \n",
      " [0.558, 0.399, 0.447, 0.445, 0.413, 0.414, 0.499, 0.351, 0.33, 0.37, 0.218, 0.207, 0.533, 0.259, 0.199, 0.286, 0.409, 0.169, 0.171, 0.247, 0.619, 0.646, 0.162, 0.318, 0.223, 0.155, 0.101, 0.142, 0.363, 0.081, 0.163, 0.37, 0.354, 0.164, 0.238, 0.129, 0.143, 0.224, 0.192, 0.191, 0.156, 0.115, 0.225, 0.515, 0.214, 0.276, 0.09, 0.759, 0.42, 0.096, 0.048, 0.72, 0.077, 0.046, 0.103, 0.562, 0.696, 0.133, 0.136, 0.816, 0.185, 0.083, 0.07, 0.115, 0.081, 0.02, 0.648, 0.431, 0.13, 0.535, 0.134, 0.09, 1.431, 0.119, 0.097, 0.816, 0.108, 0.073, 0.699, 0.098, 0.073, 0.596, 0.095, 0.074, 0.075, 0.064, 0.205, 0.114, 0.107, 0.083, 0.907, 0.079, 0.054, 0.084, 0.073, 0.403, 0.098, 0.062, 0.111, 0.514]\n",
      "\n",
      "Validation location_location_mae2 (all epochs): 1.2061158418655396 \n",
      " [36.031, 5.866, 4.885, 4.26, 6.798, 2.38, 31.302, 2.404, 1.812, 12.952, 2.329, 2.551, 32.644, 3.267, 2.852, 3.374, 13.624, 2.981, 3.006, 2.213, 22.498, 17.367, 2.692, 14.134, 2.349, 2.64, 3.216, 3.129, 18.388, 3.003, 2.223, 19.692, 11.341, 3.215, 5.515, 2.83, 4.229, 6.59, 4.346, 3.418, 6.619, 2.099, 3.483, 14.996, 6.798, 10.79, 1.956, 19.789, 13.463, 2.707, 2.832, 33.696, 2.341, 1.802, 2.162, 16.735, 28.724, 2.365, 2.271, 16.791, 2.746, 2.026, 1.516, 2.318, 1.954, 2.363, 21.299, 21.436, 1.719, 22.291, 6.526, 1.47, 58.336, 2.076, 2.493, 27.987, 1.722, 2.37, 22.472, 1.837, 2.07, 22.714, 5.435, 4.495, 1.431, 1.766, 14.952, 4.251, 6.245, 2.21, 21.881, 1.437, 1.58, 1.779, 1.525, 15.822, 1.285, 1.206, 2.209, 17.595]\n",
      "\n",
      "Validation power_mae (all epochs): 1.4849848747253418 \n",
      " [35.882, 5.468, 3.465, 3.022, 4.596, 4.498, 19.75, 4.554, 2.791, 6.968, 4.103, 4.663, 17.735, 6.318, 4.317, 5.53, 13.655, 9.591, 4.237, 7.11, 25.504, 24.409, 4.539, 10.066, 8.197, 8.947, 8.389, 5.738, 14.117, 3.572, 6.523, 16.773, 9.48, 4.43, 9.403, 1.9, 8.048, 9.376, 5.97, 4.956, 9.573, 7.427, 8.765, 11.196, 8.199, 7.581, 2.212, 23.926, 13.548, 5.628, 2.46, 21.262, 4.212, 4.871, 3.569, 15.527, 18.873, 4.187, 6.26, 18.226, 3.245, 5.885, 6.304, 3.7, 4.011, 5.603, 18.454, 19.896, 3.339, 19.162, 8.686, 2.62, 30.184, 3.791, 3.812, 28.502, 3.152, 4.171, 19.907, 3.727, 4.363, 17.912, 4.46, 6.435, 3.238, 1.485, 11.049, 5.414, 3.242, 4.175, 27.34, 2.776, 2.42, 3.983, 3.695, 17.286, 2.3, 1.652, 3.436, 21.285]\n",
      "\n",
      "Validation tx_exist_accuracy (all epochs): 0.6988686919212341 \n",
      " [0.699, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 1.0, 0.999, 0.799, 0.802, 0.998, 0.799, 0.799, 0.946, 0.997, 0.82, 0.799, 0.799, 0.998, 0.799, 0.872, 0.999, 0.999, 0.998, 0.799, 1.0, 0.998, 0.799, 0.799, 0.997, 0.814, 1.0, 0.948, 0.872, 0.936, 0.942, 0.93, 1.0, 0.815, 0.799, 0.803, 0.799, 1.0, 0.799, 0.799, 1.0, 1.0, 0.799, 1.0, 1.0, 1.0, 0.799, 0.799, 1.0, 1.0, 0.799, 0.935, 1.0, 1.0, 0.999, 1.0, 1.0, 0.799, 0.799, 0.998, 0.705, 1.0, 0.996, 0.76, 0.999, 0.992, 0.785, 1.0, 0.999, 0.799, 0.989, 1.0, 0.799, 0.998, 0.984, 0.999, 1.0, 0.8, 1.0, 1.0, 0.998, 0.797, 1.0, 0.998, 0.998, 0.997, 0.799, 1.0, 1.0, 0.971, 0.785]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00001: val_loss improved from inf to 2502.86035, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00002: val_loss improved from 2502.86035 to 533.52252, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00003: val_loss improved from 533.52252 to 287.20352, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00004: val_loss did not improve from 287.20352\n",
      "\n",
      "Epoch 00005: val_loss improved from 287.20352 to 256.02954, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00006: val_loss did not improve from 256.02954\n",
      "\n",
      "Epoch 00007: val_loss did not improve from 256.02954\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 256.02954\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 256.02954\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 256.02954\n",
      "\n",
      "Epoch 00011: val_loss improved from 256.02954 to 251.97224, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 251.97224\n",
      "\n",
      "Epoch 00031: val_loss improved from 251.97224 to 114.83528, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00036: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00039: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00042: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00048: val_loss did not improve from 114.83528\n",
      "\n",
      "Epoch 00049: val_loss improved from 114.83528 to 97.45025, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00052: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00060: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00063: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 97.45025\n",
      "\n",
      "Epoch 00069: val_loss improved from 97.45025 to 61.24887, saving model to models/pictures_100_100/log/noisy_std_1/static_point_sensors/raw_power_min_max_norm/gray/900sensors/models/75000/best_model_lambda_4.h5\n",
      "\n",
      "Epoch 00070: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00071: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00080: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00082: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00089: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00094: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00097: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 61.24887\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 61.24887\n",
      "\n",
      "\n",
      "Lambda: 1 , Time: 1:28:42\n",
      "\n",
      "*****Train Info*****\n",
      "\n",
      "Train loss (all epochs): 43.329315185546875 \n",
      " [1050.45, 311.094, 217.137, 185.568, 138.331, 138.578, 108.023, 136.428, 106.478, 84.54, 79.846, 74.968, 76.257, 173.462, 96.157, 85.997, 78.067, 70.689, 78.247, 75.404, 67.603, 64.658, 59.906, 61.855, 55.662, 271.548, 133.027, 96.965, 88.381, 70.416, 69.982, 59.979, 61.359, 64.296, 214.896, 153.007, 105.787, 89.652, 77.383, 73.181, 65.586, 66.599, 59.816, 62.517, 55.836, 65.223, 53.336, 58.398, 53.419, 55.735, 61.333, 46.67, 385.477, 200.569, 146.896, 121.202, 103.9, 81.94, 79.047, 66.966, 63.523, 72.838, 57.917, 57.457, 59.017, 55.503, 55.189, 55.029, 51.363, 61.908, 54.281, 58.349, 54.829, 50.072, 46.711, 46.945, 47.131, 43.329, 187.611, 206.131, 127.949, 97.324, 81.74, 69.101, 69.587, 56.748, 104.181, 87.432, 61.872, 53.942, 48.521, 54.838, 47.463, 44.036, 134.141, 105.863, 72.401, 58.93, 55.008, 50.75]\n",
      "\n",
      "Train location_loss (all epochs): 15.054731369018555 \n",
      " [487.423, 61.16, 52.161, 51.468, 35.063, 45.441, 29.909, 52.871, 32.193, 24.719, 27.86, 24.592, 29.549, 76.495, 26.773, 28.694, 25.698, 24.121, 32.227, 24.125, 25.106, 23.092, 22.58, 23.059, 19.708, 142.114, 37.125, 27.271, 29.167, 22.196, 24.872, 20.256, 23.65, 24.111, 129.768, 47.245, 30.9, 28.95, 25.6, 26.035, 23.944, 25.056, 21.97, 24.77, 20.052, 26.642, 16.848, 23.021, 19.449, 22.531, 24.086, 15.366, 245.801, 83.803, 55.354, 44.511, 36.966, 25.508, 28.415, 20.894, 21.632, 30.024, 19.733, 20.755, 23.016, 20.133, 21.696, 19.962, 20.451, 26.773, 16.5, 25.158, 16.397, 18.673, 17.499, 16.805, 19.452, 15.055, 141.844, 88.59, 44.202, 30.828, 26.836, 22.207, 27.338, 18.443, 60.829, 33.577, 19.376, 18.252, 16.176, 22.939, 16.112, 15.573, 79.411, 24.945, 18.194, 16.192, 18.216, 16.74]\n",
      "\n",
      "Train power_loss (all epochs): 9.229068756103516 \n",
      " [798.516, 52.94, 28.049, 18.422, 13.01, 19.001, 12.864, 18.272, 13.639, 11.445, 12.625, 11.787, 13.393, 22.215, 12.292, 13.105, 11.907, 12.152, 14.249, 12.116, 12.173, 11.817, 11.262, 11.841, 11.18, 28.28, 11.844, 11.539, 12.506, 11.177, 12.324, 11.295, 12.302, 12.148, 28.948, 12.075, 11.221, 11.879, 11.243, 11.543, 11.672, 11.879, 11.113, 12.237, 10.594, 12.918, 10.242, 11.583, 10.707, 10.791, 12.383, 10.077, 39.015, 14.475, 12.709, 12.087, 12.144, 11.034, 12.462, 10.87, 11.127, 12.046, 10.443, 10.641, 11.721, 11.007, 11.812, 11.08, 11.355, 13.072, 9.943, 12.503, 10.047, 10.5, 10.461, 10.151, 10.25, 9.581, 42.517, 15.204, 12.153, 11.199, 10.963, 10.334, 10.929, 9.782, 14.852, 10.456, 9.706, 10.03, 9.589, 10.457, 9.77, 9.229, 25.717, 10.198, 9.742, 9.753, 10.801, 10.266]\n",
      "\n",
      "Train tx_exist_loss (all epochs): 0.4970785975456238 \n",
      " [0.582, 0.576, 0.55, 0.527, 0.513, 0.505, 0.5, 0.498, 0.498, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497, 0.497]\n",
      "\n",
      "Train location_location_mae2 (all epochs): 2.8770997524261475 \n",
      " [14.054, 5.835, 5.363, 5.178, 4.39, 4.796, 4.09, 4.883, 4.177, 3.728, 3.88, 3.692, 3.956, 5.639, 3.826, 3.921, 3.774, 3.646, 4.052, 3.648, 3.655, 3.592, 3.521, 3.566, 3.332, 7.512, 4.467, 3.853, 3.943, 3.502, 3.647, 3.354, 3.595, 3.617, 6.659, 4.904, 4.031, 3.898, 3.719, 3.705, 3.577, 3.61, 3.42, 3.597, 3.318, 3.664, 3.062, 3.497, 3.264, 3.458, 3.448, 2.957, 10.501, 6.219, 5.105, 4.595, 4.233, 3.646, 3.789, 3.331, 3.349, 3.828, 3.212, 3.292, 3.406, 3.279, 3.345, 3.206, 3.285, 3.596, 2.999, 3.48, 2.982, 3.154, 3.075, 2.996, 3.156, 2.877, 6.26, 6.437, 4.652, 3.928, 3.677, 3.406, 3.613, 3.121, 4.796, 3.96, 3.198, 3.136, 2.951, 3.36, 2.983, 2.931, 5.062, 3.627, 3.169, 2.983, 3.105, 3.018]\n",
      "\n",
      "Train power_mae (all epochs): 2.0154905319213867 \n",
      " [17.696, 6.053, 4.052, 2.865, 2.428, 2.886, 2.432, 2.828, 2.52, 2.314, 2.435, 2.362, 2.497, 3.114, 2.388, 2.465, 2.336, 2.367, 2.537, 2.353, 2.397, 2.36, 2.303, 2.35, 2.281, 3.335, 2.414, 2.391, 2.461, 2.3, 2.443, 2.303, 2.405, 2.415, 2.993, 2.389, 2.31, 2.378, 2.293, 2.308, 2.322, 2.32, 2.246, 2.344, 2.203, 2.396, 2.153, 2.282, 2.202, 2.228, 2.356, 2.116, 3.49, 2.446, 2.35, 2.333, 2.332, 2.258, 2.364, 2.278, 2.277, 2.325, 2.212, 2.223, 2.297, 2.244, 2.315, 2.239, 2.256, 2.407, 2.106, 2.336, 2.135, 2.14, 2.168, 2.137, 2.143, 2.075, 3.241, 2.481, 2.25, 2.198, 2.156, 2.129, 2.2, 2.103, 2.455, 2.151, 2.063, 2.115, 2.066, 2.17, 2.079, 2.015, 2.646, 2.082, 2.088, 2.099, 2.169, 2.17]\n",
      "\n",
      "Train tx_exist_accuracy (all epochs): 0.8023866415023804 \n",
      " [0.886, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802, 0.802]\n",
      "\n",
      "*****Validation Info*****\n",
      "\n",
      "Validation loss (all epochs): 61.24887466430664 \n",
      " [2502.86, 533.523, 287.204, 344.921, 256.03, 332.253, 402.029, 401.816, 379.819, 1154.757, 251.972, 1618.262, 1007.861, 1200.837, 1014.393, 755.373, 1389.257, 2758.752, 566.278, 609.614, 1798.458, 774.467, 1085.319, 1619.05, 424.136, 876.672, 2419.018, 676.797, 449.764, 611.735, 114.835, 421.511, 1011.847, 1804.884, 2265.54, 1128.344, 836.122, 5039.59, 275.451, 539.046, 255.595, 1547.982, 145.755, 1580.524, 1615.926, 554.468, 1458.677, 1587.058, 97.45, 167.206, 1128.629, 203.979, 2121.789, 12444.373, 7029.876, 6338.524, 5048.575, 670.59, 10358.642, 603.569, 2387.457, 1521.961, 1030.252, 1526.562, 2244.709, 1035.728, 1810.293, 549.229, 61.249, 1028.782, 822.223, 266.67, 158.747, 343.978, 1496.877, 1006.589, 1192.398, 196.176, 3941.047, 1590.449, 2416.442, 1134.982, 847.3, 1695.551, 1705.635, 1994.876, 1876.519, 1709.049, 603.671, 915.5, 790.456, 1551.174, 168.832, 933.966, 3623.719, 1083.283, 361.782, 678.655, 3510.081, 92.592]\n",
      "\n",
      "Validation location_loss (all epochs): 24.904449462890625 \n",
      " [2495.279, 350.602, 142.107, 241.902, 122.403, 214.415, 370.964, 336.164, 313.12, 1175.889, 210.28, 1702.289, 1014.624, 1227.122, 1021.014, 770.865, 1485.081, 3065.43, 225.27, 619.62, 1950.759, 781.456, 1104.965, 1730.952, 411.815, 837.63, 2600.54, 635.327, 389.917, 614.575, 59.387, 409.538, 1027.127, 1936.279, 2335.133, 1077.926, 814.932, 2311.839, 251.439, 537.955, 233.411, 1662.521, 121.339, 1693.978, 1718.509, 553.625, 1556.945, 1626.405, 51.47, 141.313, 1182.504, 167.014, 2132.574, 4780.905, 5249.382, 5043.392, 4098.106, 671.872, 4378.054, 612.547, 2481.652, 1412.304, 1040.956, 1619.265, 2452.45, 1059.706, 1923.47, 562.909, 24.904, 1107.383, 792.698, 203.284, 137.383, 308.741, 1590.353, 1026.627, 1204.156, 178.047, 4193.006, 1639.242, 2587.272, 1137.97, 826.923, 1813.732, 1860.265, 2193.885, 1983.214, 1842.971, 589.994, 913.438, 808.551, 1575.296, 148.309, 890.891, 3929.174, 1070.764, 314.989, 681.374, 3948.035, 55.147]\n",
      "\n",
      "Validation power_loss (all epochs): 32.4603385925293 \n",
      " [477.162, 380.009, 101.1, 179.246, 593.736, 578.334, 148.535, 289.757, 440.099, 983.526, 123.288, 1216.663, 892.049, 730.505, 807.66, 387.519, 758.594, 1061.272, 3057.353, 377.546, 931.078, 707.44, 1036.25, 1086.707, 389.876, 430.645, 1261.835, 731.756, 635.555, 426.913, 198.615, 337.109, 951.354, 1199.58, 1335.969, 1218.495, 737.25, 30149.244, 95.547, 368.318, 143.838, 942.065, 40.447, 1005.946, 1185.876, 395.692, 1002.296, 1681.35, 201.129, 124.983, 870.402, 308.328, 1613.348, 82705.25, 24781.426, 19754.006, 14987.172, 429.656, 65838.961, 376.265, 2369.104, 2780.903, 1062.347, 1124.711, 1208.659, 983.443, 1362.77, 377.303, 32.46, 370.229, 1144.441, 415.375, 82.727, 490.518, 1122.032, 1057.248, 1365.131, 171.229, 1934.187, 940.035, 1391.536, 1059.628, 925.26, 1085.482, 806.041, 925.515, 1218.172, 926.871, 625.375, 1024.61, 709.932, 1752.188, 111.247, 1479.584, 1660.038, 1085.222, 453.887, 595.95, 1174.125, 120.493]\n",
      "\n",
      "Validation tx_exist_loss (all epochs): 0.5021357536315918 \n",
      " [0.61, 0.567, 0.539, 0.522, 0.511, 0.506, 0.504, 0.503, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502, 0.502]\n",
      "\n",
      "Validation location_location_mae2 (all epochs): 3.6505391597747803 \n",
      " [37.42, 14.641, 9.692, 10.008, 7.739, 10.716, 10.797, 10.923, 14.812, 21.701, 9.46, 27.651, 22.532, 23.487, 21.096, 21.215, 24.808, 38.51, 10.6, 15.867, 28.068, 19.138, 24.214, 23.031, 13.845, 21.474, 41.8, 19.93, 14.743, 16.193, 6.009, 14.388, 19.811, 31.698, 41.738, 19.38, 19.469, 32.169, 8.831, 14.753, 10.411, 25.658, 6.651, 24.382, 33.646, 17.348, 31.323, 23.856, 5.121, 9.16, 19.308, 8.758, 40.073, 55.471, 60.556, 59.003, 49.002, 19.37, 51.929, 15.561, 35.692, 22.142, 19.936, 33.924, 34.803, 21.988, 29.999, 17.086, 3.651, 22.996, 18.547, 11.171, 9.07, 13.249, 21.246, 22.919, 24.801, 9.113, 59.944, 35.737, 38.811, 26.748, 19.693, 39.459, 34.379, 33.754, 39.665, 36.139, 14.768, 17.853, 13.945, 31.372, 7.458, 20.967, 48.886, 23.449, 12.292, 16.18, 47.193, 5.028]\n",
      "\n",
      "Validation power_mae (all epochs): 3.8013412952423096 \n",
      " [17.935, 12.142, 8.708, 11.794, 17.283, 16.626, 9.552, 15.775, 15.504, 15.566, 9.068, 17.303, 15.884, 15.969, 14.94, 14.5, 14.782, 20.707, 34.715, 12.652, 16.742, 15.067, 18.174, 16.266, 12.672, 17.734, 19.62, 18.83, 16.153, 13.096, 12.245, 10.887, 16.998, 18.919, 19.58, 24.949, 15.562, 127.965, 6.039, 16.35, 6.964, 14.842, 3.801, 17.456, 17.086, 13.803, 20.51, 30.178, 8.287, 9.071, 14.797, 11.178, 23.272, 267.06, 153.938, 134.77, 110.138, 12.367, 227.677, 13.95, 39.738, 40.206, 15.935, 21.602, 18.159, 16.65, 20.668, 12.989, 5.149, 11.32, 16.637, 11.335, 6.541, 14.138, 20.634, 16.205, 19.284, 6.966, 28.581, 22.954, 20.004, 19.607, 17.66, 28.526, 23.431, 21.197, 20.275, 24.221, 12.102, 16.516, 15.614, 25.33, 7.388, 21.443, 23.873, 17.635, 12.237, 14.346, 21.461, 8.799]\n",
      "\n",
      "Validation tx_exist_accuracy (all epochs): 0.7987474799156189 \n",
      " [0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799, 0.799]\n",
      "\n",
      "Trainig set size: 75000 , Time: 7:23:41 , best_lambda: 0 , min_ val_loss error: 0.499\n"
     ]
    }
   ],
   "source": [
    "# CNN: support batching\n",
    "TEST = False\n",
    "mini_batch = 16 if max(max_x, max_y) == 1000 else 256\n",
    "epochs = 200 if max(max_x, max_y) == 1000 else 100\n",
    "MAX_QUEUE_SIZE, WORKERS = 6, 1\n",
    "hyper_metric, mode = \"val_loss\", 'min'  # the metric that hyper parameters are tuned with\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.01, 0.1, 1]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3\n",
    "# lambda_vec = [0.01, 0.1, 1]\n",
    "# lambda_vec = [0.01]\n",
    "# MODEL_PATH = 'models/'\n",
    "average_diff_power, average_location_error = [],[] #[7.177, 8.088, 8.183], [3.438, 3.506, 2.662]\n",
    "best_lambda = []\n",
    "all_cnns = []\n",
    "\n",
    "for num_sample_idx, number_sample in enumerate(number_samples):\n",
    "#     if num_sample_idx < 3:\n",
    "#         continue\n",
    "#     if num_sample_idx == 0:\n",
    "    MODEL_PATH = '/'.join(image_dir.split('/')[:-1]) + '/models/' + str(number_sample)\n",
    "    if not os.path.exists(MODEL_PATH):\n",
    "        os.makedirs(MODEL_PATH)\n",
    "    MODEL_PATH += \"/best_model_lambda_\"\n",
    "    if True:\n",
    "        cnns = [cnn_model2(10, lamb, 0) for lamb in lambda_vec]\n",
    "        for cnn in cnns:\n",
    "#             cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae', fp_mean])\n",
    "            cnn.compile(loss={\"location\":location_loss, \"power\":\"mse\", \"tx_exist\":\"binary_crossentropy\"},\n",
    "                        optimizer='adam', \n",
    "#                         metrics=['mse', 'mae', location_mae2, power_mae2, total_mae2]\n",
    "                        metrics={\"location\":location_mae2, \"power\":\"mae\", \"tx_exist\": \"accuracy\"}, \n",
    "                        loss_weights={\"location\":0.85, \"power\":0.1, \"tx_exist\":0.05})\n",
    "        checkpointers = [ModelCheckpoint(filepath=MODEL_PATH + str(lamb_idx)+ '.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "                         for lamb_idx in range(len(lambda_vec))]\n",
    "    else:\n",
    "        cnns = []\n",
    "        cnns = [models.load_model(MODEL_PATH + str(lamb_idx) + '.h5', \n",
    "                                  custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                  'location_mae': location_mae,\n",
    "                                                 'power_mae': power_mae, 'total_mae': total_mae}) \n",
    "                for lamb_idx in range(len(lambda_vec))]\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator2(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used, \n",
    "                                         STATIC=STATIC_SENSORS, SENSOR_NUM=sensors_num, \n",
    "                                         pic_cell_size=pic_cell_size)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator2(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used,\n",
    "                                       STATIC=STATIC_SENSORS, SENSOR_NUM=sensors_num,\n",
    "                                       pic_cell_size=pic_cell_size)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    \n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "#     for lamb_idx, lamb in enumerate(lambda_vec[:len(lambda_vec) - num_sample_idx//2]):\n",
    "#         if num_sample_idx == 3 and lamb_idx < 4:\n",
    "#             continue\n",
    "        lambda_start = time.time()\n",
    "        cnns[lamb_idx].fit(train_generator, epochs=epochs, verbose=0,\n",
    "                           validation_data=val_generator, \n",
    "                           shuffle=True, callbacks=[checkpointers[lamb_idx]], \n",
    "                           workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                           use_multiprocessing=False)\n",
    "        \n",
    "        print(\"\\n\\nLambda:\", lamb, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        print(\"\\n*****Train Info*****\")\n",
    "        for metric in cnns[lamb_idx].metrics_names:\n",
    "            print(\"\\nTrain\", metric, \"(all epochs):\", min(cnns[lamb_idx].history.history[metric]), \"\\n\",\n",
    "                 [round(val, 3) for val in cnns[lamb_idx].history.history[metric]])\n",
    "#         print(\"\\nTrain Loss(all epochs):\", min(cnns[lamb_idx].history.history['loss']), '\\n', \n",
    "#               [round(val, 3) for val in cnns[lamb_idx].history.history['loss']])\n",
    "# #         print(\"\\nTrain Total Error(all epochs):\", min(cnns[lamb_idx].history.history['total_mae']), '\\n',\n",
    "# #               [round(val,3) for val in cnns[lamb_idx].history.history['total_mae']])\n",
    "#         print(\"\\nTrain Location Error(all epochs):\", min(cnns[lamb_idx].history.history['location_location_mae2']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['location_location_mae2']])\n",
    "#         print(\"\\nTrain Power Error(all epochs):\", min(cnns[lamb_idx].history.history['power_mae']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['power_mae']])\n",
    "        \n",
    "        print(\"\\n*****Validation Info*****\")\n",
    "        for metric in cnns[lamb_idx].metrics_names:\n",
    "            print(\"\\nValidation\", metric, \"(all epochs):\", min(cnns[lamb_idx].history.history[\"val_\" + metric]), \"\\n\",\n",
    "                 [round(val, 3) for val in cnns[lamb_idx].history.history[\"val_\" + metric]])\n",
    "#         print(\"\\nVal Loss(all epochs):\", min(cnns[lamb_idx].history.history['val_loss']), '\\n', \n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['val_loss']])\n",
    "# #         print(\"\\nVal Total Error(all epochs):\", min(cnns[lamb_idx].history.history['val_total_mae']), '\\n',\n",
    "# #               [round(val,3) for val in cnns[lamb_idx].history.history['val_total_mae']])\n",
    "#         print(\"\\nVal Location Error(all epochs):\", min(cnns[lamb_idx].history.history['val_location_location_mae2']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['val_location_location_mae2']])\n",
    "#         print(\"\\nVal Power Error(all epochs):\", min(cnns[lamb_idx].history.history['val_power_mae']), '\\n',\n",
    "#               [round(val,3) for val in cnns[lamb_idx].history.history['val_power_mae']])\n",
    "#     if num_sample_idx == 3:    \n",
    "#         models_min_mae = [8.27781, 8.23545, 8.20838, 7.74743]\n",
    "#         models_min_mae += [min(cnns[lamb_idx].history.history[hyper_metric]) for lamb_idx in range(4,lamb_idx+1)]\n",
    "#     else:\n",
    "    models_min_mae = [min(cnns[lam_idx].history.history[hyper_metric]) for\n",
    "                      lam_idx,_ in enumerate(lambda_vec)]\n",
    "    best_lamb_idx = models_min_mae.index(min(models_min_mae))\n",
    "    best_lambda.append(lambda_vec[best_lamb_idx])\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - \n",
    "                                                                                              number_start))),\n",
    "          \", best_lambda:\", lambda_vec[best_lamb_idx], \", min_\" , hyper_metric,\n",
    "          \"error:\", round(min(models_min_mae), 3))\n",
    "    all_cnns.append(cnns)\n",
    "    del cnns, train_generator, val_generator, checkpointers\n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        best_model = None\n",
    "        best_model = models.load_model(MODEL_PATH + str(best_lamb_idx) + '.h5', \n",
    "                                       custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                                       'fp_mae': fp_mae,\n",
    "                                                      'mae':'mae', 'mse':'mse'})\n",
    "        test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used,\n",
    "                                           pic_cell_size=pic_cell_size)\n",
    "\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "        test_res = best_model.evaluate(test_generator, verbose=1, \n",
    "                                       workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, use_multiprocessing=False)\n",
    "        \n",
    "        test_mae_idx, test_fp_mae_idx = [best_model.metrics_names.index(mtrc) \n",
    "                                         for mtrc in ['mae','fp_mae']]\n",
    "        test_mae, test_fp_mae = test_res[test_mae_idx], test_res[test_fp_mae_idx]\n",
    "        average_diff_power.append(round(test_mae, 3))\n",
    "        fp_mean_power.append(round(test_fp_mae, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    "                     dataset_name, max_dataset_name, average_diff_power_conserve, fp_mean_power_conserve],\n",
    "                    file=var_f)\n",
    "        var_f.close()\n",
    "        del best_model, test_generator\n",
    "#     prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_all_power_error, all_all_location_error = [], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.,   0., -90.,   0.,  nan,  nan,  nan,  nan,  nan,  nan,  nan,\n",
       "        nan,  nan,  nan,  nan,  nan,  nan,  nan,  nan])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[number_sample + val_size + 5][sensors_num:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "actual_locations:  [Point(x=31.0, y=61.0), Point(x=32.0, y=0.0), Point(x=90.0, y=3.0), Point(x=21.0, y=60.0)]\n",
      "actual_powers:  [-15.0, -10.0, -5.0, 0.0]\n",
      "tx_exists:  1.0\n",
      "predicted_locations: Point(x=21.16762351989746, y=59.80583190917969)\n",
      "predicted_power:  0.27223805\n",
      "tx_exists:  1.0\n",
      "predicted_locations: Point(x=90.85049438476562, y=3.5180389881134033)\n",
      "predicted_power:  -6.044528\n",
      "tx_exists:  1.0\n",
      "predicted_locations: Point(x=32.95085525512695, y=0.6779183149337769)\n",
      "predicted_power:  -13.376385\n",
      "tx_exists:  0.97886705\n",
      "predicted_locations: Point(x=44.729286193847656, y=65.13613891601562)\n",
      "predicted_power:  -44.218384\n",
      "tx_exists:  0.00015631363\n",
      "Test starts:  99750 , ends:  149999\n",
      "average_power_error:  8.478 , average_location_error:  4.19\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "alpha = 3.4\n",
    "if True:\n",
    "    if True:\n",
    "        best_model = models.load_model(MODEL_PATH + str(0) + '.h5', \n",
    "                                       custom_objects={'location_mae2':location_mae2, \n",
    "                                                       \"location_loss\": location_loss})\n",
    "        all_power_error, all_location_error, all_false_positive, all_false_negative = [], [], 0, 0\n",
    "        data_reg_copy = np.copy(data_reg)\n",
    "        for test_idx in range(number_sample + val_size + 1, number_sample + val_size + 2):\n",
    "#         for test_idx in range(3, 4):\n",
    "            test_offset = sensors_num + 3 if STATIC_SENSORS else int(data_reg_copy[test_idx][0]) * 3 + 4\n",
    "            tx_true_num = int(data_reg_copy[test_idx][test_offset])\n",
    "            tx_powers, tx_locations = [], []\n",
    "            for tx_idx in range(tx_true_num):\n",
    "                tx_x, tx_y = data_reg_copy[test_idx][test_offset+tx_idx*3+1], data_reg_copy[test_idx][test_offset+tx_idx*3+2]\n",
    "                tx_locations.append(Point(tx_x, tx_y))\n",
    "                tx_powers.append(data_reg_copy[test_idx][test_offset+tx_idx*3+3])\n",
    "#             print(tx_locations)\n",
    "#             print(tx_powers)\n",
    "            data_X = data_reg_copy[test_idx][:]\n",
    "#             print(data_X)\n",
    "            tx_pred_num, tx_pred_powers, tx_pred_locations = 0, [], []\n",
    "            power_error, location_error, false_positive, false_negative = 0, 0, 0, 0\n",
    "            STOPPED = False\n",
    "            print('actual_locations: ', tx_locations)\n",
    "            print('actual_powers: ', tx_powers)\n",
    "            while True:\n",
    "                #predicting\n",
    "                test_imm = create_image(data=data_X, slope=slope, style=style, \n",
    "                                        noise_floor=noise_floor,\n",
    "                                        ss_shape=ss_shape, \n",
    "                                        sensors_num=(sensors_num if STATIC_SENSORS else 0), \n",
    "                                        intensity_degradation=intensity_degradation, \n",
    "                                        max_ss_power=max_ss_power, ss_param=ss_param, pic_cell_size=pic_cell_size)\n",
    "#                 fig, ax = plt.subplots(figsize=(60,60))         # Sample figsize in inches\n",
    "#                 sns.heatmap(test_imm[0][0], ax=ax)\n",
    "#                 print(data_X[:100])\n",
    "#                 yy_p = np.empty((1, 1), dtype=float_memory_used)\n",
    "#                 yy_p[0,0] = -10.829\n",
    "#                 yy_loc = np.empty((1, 2), dtype=float_memory_used)\n",
    "#                 yy_loc[0,0] = 3.0\n",
    "#                 yy_loc[0,1] = 81.0\n",
    "                test_ans = best_model.predict(test_imm)\n",
    "#                 res = best_model.evaluate(test_imm, y={\"location\":yy_loc, \"power\":yy_p})\n",
    "#                 print(res)\n",
    "                tx_pred_x, tx_pred_y = test_ans[0][0][0]/pic_cell_size, test_ans[0][0][1]/pic_cell_size\n",
    "                tx_pred_p = test_ans[1][0][0]\n",
    "                tx_exists = test_ans[2][0][0]\n",
    "                print(\"tx_exists: \", tx_exists)\n",
    "                if tx_exists < 0.9:\n",
    "                    break\n",
    "                tx_pred_powers.append(tx_pred_p)\n",
    "                tx_pred_locations.append(Point(tx_pred_x, tx_pred_y))\n",
    "                print('predicted_locations:', tx_pred_locations[-1])\n",
    "                print('predicted_power: ', tx_pred_powers[-1])\n",
    "                tx_pred_num += 1\n",
    "                if len(tx_locations) == 0:#update false_positive\n",
    "                    false_positive += 1\n",
    "                else:\n",
    "                    min_dist, min_idx = float('inf'), -1\n",
    "                    for i in range(len(tx_locations)):\n",
    "                        dist_i = euclidian_distance(tx_locations[i], tx_pred_locations[-1])\n",
    "                        if dist_i < min_dist:\n",
    "                            min_dist = dist_i\n",
    "                            min_idx = i\n",
    "                    location_error += min_dist\n",
    "                    power_error += abs(tx_powers[min_idx] - tx_pred_powers[-1])\n",
    "                    tx_pred_p, tx_pred_x, tx_pred_y = tx_powers[min_idx], tx_locations[min_idx][0], tx_locations[min_idx][1]\n",
    "                    del tx_powers[min_idx], tx_locations[min_idx]\n",
    "                \n",
    "                #updating sensor values\n",
    "                STOPPED = True\n",
    "                if STATIC_SENSORS:\n",
    "                    #print('p l: ', tx_pred_x, tx_pred_y)\n",
    "                    #print(\"p p: \", tx_pred_p)\n",
    "                    for ss_idx in range(sensors_num):\n",
    "                        #print('ss#', ss_idx, ', location: ', sensors_location[ss_idx])\n",
    "                        #print('ss_power_before:', data_X[ss_idx])\n",
    "                        reduced_power = tx_pred_p\n",
    "                        if euclidian_distance(Point(tx_pred_x, tx_pred_y), \n",
    "                                              sensors_location[ss_idx])* cell_size> 1:\n",
    "                            reduced_power -= 10 * alpha * math.log10(euclidian_distance(\n",
    "                                Point(tx_pred_x, tx_pred_y),sensors_location[ss_idx]) * cell_size)\n",
    "#                         print('ss_received_power:', reduced_power)\n",
    "                        reduced_power += gauss(0, 1)\n",
    "                        data_X[ss_idx] = 10 ** (data_X[ss_idx]/10) - 10 ** (reduced_power/10)\n",
    "                        if data_X[ss_idx] <= 0:\n",
    "                            data_X[ss_idx] = noise_floor\n",
    "                        else:\n",
    "                            data_X[ss_idx] = max(noise_floor, 10 * math.log10(data_X[ss_idx]))\n",
    "#                         print('ss_power_after:', data_X[ss_idx])\n",
    "                        if data_X[ss_idx] > noise_floor + 20:\n",
    "                            STOPPED = False\n",
    "                else:\n",
    "                    ss_num = int(data_X[0])\n",
    "                    for ss_idx in range(ss_num):\n",
    "                        reduced_power = tx_pred_p\n",
    "                        if  euclidian_distance(Point(tx_pred_x, tx_pred_y),\n",
    "                                               Point(data_X[ss_idx*3+1], data_X[ss_idx*3+2])) * cell_size > 1:\n",
    "                            reduced_power -=10 * alpha * math.log10(euclidian_distance(\n",
    "                            Point(tx_pred_x, tx_pred_y), Point(data_X[ss_idx*3+1], data_X[ss_idx*3+2]))\n",
    "                                                                            * cell_size)\n",
    "                        reduced_power += gauss(0, 1)\n",
    "                        data_X[ss_idx*3+3] = 10 ** (data_X[ss_idx*3+3]/10) - 10 ** (reduced_power/10)\n",
    "                        if data_X[ss_idx*3+3] <= 0:\n",
    "                            data_X[ss_idx*3+3] = noise_floor\n",
    "                        else:\n",
    "                            data_X[ss_idx*3+3] = max(noise_floor, 10 * math.log10(data_X[ss_idx*3+3]))\n",
    "                        if data_X[ss_idx*3+3] > noise_floor:\n",
    "                            STOPPED = False\n",
    "            if tx_pred_num < tx_true_num:#update false negative\n",
    "                false_negative += (tx_true_num - tx_pred_num)\n",
    "            all_power_error.append(power_error / min(tx_true_num, tx_pred_num))\n",
    "            all_location_error.append(location_error / min(tx_true_num, tx_pred_num))\n",
    "            all_false_negative += false_negative\n",
    "            all_false_positive += false_positive\n",
    "            \n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        all_all_power_error.append(round(sum(all_power_error)/len(all_power_error), 3))\n",
    "        all_all_location_error.append(round(sum(all_location_error)/len(all_location_error),3))\n",
    "        print('average_power_error: ', all_all_power_error[-1], ', average_location_error: ', \n",
    "              all_all_location_error[-1])\n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "\n",
    "        \n",
    "#         var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "#                      dtime + \".dat\", \"wb\") # file for saving results\n",
    "#         pickle.dump([all_all_power_error, all_all_location_error, number_samples, best_lambda, \n",
    "#                      dataset_name, max_dataset_name],\n",
    "#                     file=var_f)\n",
    "#         var_f.close()\n",
    "        del best_model\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[0., 0.]], dtype=float32),\n",
       " array([[-88.191536]], dtype=float32),\n",
       " array([[0.14945157]], dtype=float32)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_ans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([21.   , 77.   , -3.488,  1.   , 21.   , 77.   , -3.488,    nan,\n",
       "          nan,    nan,    nan,    nan,    nan,    nan,    nan,    nan,\n",
       "          nan,    nan,    nan])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[number_sample + val_size + 2][500:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Point(x=170.38307, y=199.46127),\n",
       " Point(x=169.57475, y=194.1276),\n",
       " Point(x=173.07169, y=198.6157),\n",
       " Point(x=161.69841, y=195.14696),\n",
       " Point(x=172.01877, y=181.61957),\n",
       " Point(x=153.91132, y=164.04286),\n",
       " Point(x=87.492584, y=107.67233),\n",
       " Point(x=87.70111, y=108.093925),\n",
       " Point(x=87.94716, y=108.55117),\n",
       " Point(x=88.12611, y=108.973724),\n",
       " Point(x=88.396805, y=109.42558),\n",
       " Point(x=88.68089, y=109.84977),\n",
       " Point(x=88.96226, y=110.2121),\n",
       " Point(x=89.21187, y=110.678894),\n",
       " Point(x=89.37605, y=111.07044),\n",
       " Point(x=89.76044, y=111.55282),\n",
       " Point(x=90.08117, y=111.95813),\n",
       " Point(x=90.255554, y=112.36758),\n",
       " Point(x=90.54794, y=112.78554),\n",
       " Point(x=90.77384, y=113.072655),\n",
       " Point(x=91.04009, y=113.42287),\n",
       " Point(x=91.279366, y=113.70146),\n",
       " Point(x=91.53535, y=113.96938),\n",
       " Point(x=91.63726, y=114.14437),\n",
       " Point(x=91.68876, y=114.31183),\n",
       " Point(x=91.79541, y=114.50802),\n",
       " Point(x=91.90161, y=114.68584),\n",
       " Point(x=92.033165, y=114.88634),\n",
       " Point(x=92.19923, y=115.10028),\n",
       " Point(x=92.22237, y=115.21436),\n",
       " Point(x=92.31407, y=115.39863),\n",
       " Point(x=92.38584, y=115.58423),\n",
       " Point(x=92.50384, y=115.71365),\n",
       " Point(x=92.62624, y=115.994286),\n",
       " Point(x=92.72203, y=116.11373),\n",
       " Point(x=92.82591, y=116.32468),\n",
       " Point(x=92.899734, y=116.49206),\n",
       " Point(x=92.91759, y=116.67711),\n",
       " Point(x=92.95408, y=116.79416),\n",
       " Point(x=93.05212, y=116.993256),\n",
       " Point(x=93.15701, y=117.179146),\n",
       " Point(x=93.17302, y=117.406136),\n",
       " Point(x=93.23492, y=117.57379),\n",
       " Point(x=93.34648, y=117.78496),\n",
       " Point(x=93.35228, y=117.99506),\n",
       " Point(x=93.36697, y=118.16528),\n",
       " Point(x=93.38681, y=118.24456),\n",
       " Point(x=93.41435, y=118.3663),\n",
       " Point(x=93.53307, y=118.5716),\n",
       " Point(x=93.61844, y=118.748795),\n",
       " Point(x=93.59961, y=118.83881),\n",
       " Point(x=93.61427, y=118.9491),\n",
       " Point(x=93.65294, y=119.090256),\n",
       " Point(x=93.66486, y=119.23038),\n",
       " Point(x=93.70876, y=119.39277),\n",
       " Point(x=93.73967, y=119.43955),\n",
       " Point(x=93.79479, y=119.52486),\n",
       " Point(x=93.815315, y=119.63011),\n",
       " Point(x=93.785446, y=119.752335),\n",
       " Point(x=93.83311, y=119.78381),\n",
       " Point(x=93.81342, y=119.8694),\n",
       " Point(x=93.94833, y=120.04481),\n",
       " Point(x=93.90384, y=120.09116),\n",
       " Point(x=93.892204, y=120.141914),\n",
       " Point(x=93.862114, y=120.19196),\n",
       " Point(x=93.853645, y=120.18713),\n",
       " Point(x=93.83409, y=120.27631),\n",
       " Point(x=93.86155, y=120.369545),\n",
       " Point(x=93.85318, y=120.4298),\n",
       " Point(x=93.81529, y=120.51266),\n",
       " Point(x=93.843704, y=120.57228),\n",
       " Point(x=93.81935, y=120.63637),\n",
       " Point(x=93.7918, y=120.67444),\n",
       " Point(x=93.81121, y=120.74039),\n",
       " Point(x=93.75693, y=120.765854),\n",
       " Point(x=93.799965, y=120.84457),\n",
       " Point(x=93.72143, y=120.875015),\n",
       " Point(x=93.76402, y=120.96878),\n",
       " Point(x=93.76311, y=121.046906),\n",
       " Point(x=93.80981, y=121.16509),\n",
       " Point(x=93.76129, y=121.17835),\n",
       " Point(x=93.759056, y=121.26654),\n",
       " Point(x=93.7332, y=121.296326),\n",
       " Point(x=93.72902, y=121.35911),\n",
       " Point(x=93.69017, y=121.42554),\n",
       " Point(x=93.72959, y=121.467636),\n",
       " Point(x=93.68766, y=121.560745),\n",
       " Point(x=93.72021, y=121.62437),\n",
       " Point(x=93.68087, y=121.65437),\n",
       " Point(x=93.68719, y=121.753845),\n",
       " Point(x=93.68715, y=121.78742),\n",
       " Point(x=93.632034, y=121.83994),\n",
       " Point(x=93.64682, y=121.97817),\n",
       " Point(x=93.66577, y=122.05128),\n",
       " Point(x=93.64175, y=122.12353),\n",
       " Point(x=93.69425, y=122.168625),\n",
       " Point(x=93.7188, y=122.27661),\n",
       " Point(x=93.68893, y=122.35271),\n",
       " Point(x=93.69291, y=122.41458),\n",
       " Point(x=93.6545, y=122.45441),\n",
       " Point(x=93.67763, y=122.565765),\n",
       " Point(x=93.67801, y=122.58198),\n",
       " Point(x=93.711266, y=122.64624),\n",
       " Point(x=93.75882, y=122.79082),\n",
       " Point(x=93.75058, y=122.86965),\n",
       " Point(x=93.768326, y=122.92907),\n",
       " Point(x=93.70068, y=123.00699),\n",
       " Point(x=93.730194, y=123.103165),\n",
       " Point(x=93.71487, y=123.17044),\n",
       " Point(x=93.70956, y=123.240944),\n",
       " Point(x=93.71133, y=123.34029),\n",
       " Point(x=93.66254, y=123.38583),\n",
       " Point(x=93.742065, y=123.482086),\n",
       " Point(x=93.73108, y=123.54328),\n",
       " Point(x=93.70264, y=123.602776),\n",
       " Point(x=93.778725, y=123.694305),\n",
       " Point(x=93.79198, y=123.834625),\n",
       " Point(x=93.80254, y=123.94095),\n",
       " Point(x=93.79108, y=123.96045),\n",
       " Point(x=93.74117, y=123.97705),\n",
       " Point(x=93.698616, y=124.07485),\n",
       " Point(x=93.71673, y=124.114204),\n",
       " Point(x=93.69837, y=124.20214),\n",
       " Point(x=93.67957, y=124.27766),\n",
       " Point(x=93.64836, y=124.365456),\n",
       " Point(x=93.71745, y=124.48103),\n",
       " Point(x=93.788445, y=124.6247),\n",
       " Point(x=93.74618, y=124.68317),\n",
       " Point(x=93.72815, y=124.75006),\n",
       " Point(x=93.71289, y=124.85411),\n",
       " Point(x=93.72311, y=124.93891),\n",
       " Point(x=93.76943, y=125.0585),\n",
       " Point(x=93.71723, y=125.0625),\n",
       " Point(x=93.71617, y=125.16021),\n",
       " Point(x=93.718765, y=125.296524),\n",
       " Point(x=93.70928, y=125.33623),\n",
       " Point(x=93.72666, y=125.44627),\n",
       " Point(x=93.68229, y=125.507645),\n",
       " Point(x=93.67172, y=125.62913),\n",
       " Point(x=93.69009, y=125.70352),\n",
       " Point(x=93.67219, y=125.8108),\n",
       " Point(x=93.637695, y=125.87354),\n",
       " Point(x=93.70631, y=126.00495),\n",
       " Point(x=93.72302, y=126.09755),\n",
       " Point(x=93.6262, y=126.15439),\n",
       " Point(x=93.57502, y=126.20929),\n",
       " Point(x=93.671, y=126.29959),\n",
       " Point(x=93.64073, y=126.41619),\n",
       " Point(x=93.628136, y=126.48224),\n",
       " Point(x=93.61579, y=126.58898),\n",
       " Point(x=93.62132, y=126.675865),\n",
       " Point(x=93.67154, y=126.80961),\n",
       " Point(x=93.62226, y=126.87878),\n",
       " Point(x=93.63905, y=126.99796),\n",
       " Point(x=93.58057, y=127.03908),\n",
       " Point(x=93.57407, y=127.16439),\n",
       " Point(x=93.62041, y=127.23601),\n",
       " Point(x=93.61337, y=127.3067),\n",
       " Point(x=93.55667, y=127.40809),\n",
       " Point(x=93.514015, y=127.51237),\n",
       " Point(x=93.50201, y=127.61319),\n",
       " Point(x=93.48517, y=127.63724),\n",
       " Point(x=93.43397, y=127.721245),\n",
       " Point(x=93.42999, y=127.83338),\n",
       " Point(x=93.45868, y=127.96833),\n",
       " Point(x=93.39631, y=128.0466),\n",
       " Point(x=93.42772, y=128.13194),\n",
       " Point(x=93.43949, y=128.24129),\n",
       " Point(x=93.42222, y=128.27818),\n",
       " Point(x=93.37456, y=128.37952),\n",
       " Point(x=93.38838, y=128.48906),\n",
       " Point(x=93.395065, y=128.62143),\n",
       " Point(x=93.316216, y=128.6684),\n",
       " Point(x=93.36284, y=128.79333),\n",
       " Point(x=93.29929, y=128.8495),\n",
       " Point(x=93.312546, y=128.98837),\n",
       " Point(x=93.23845, y=129.05681),\n",
       " Point(x=93.20407, y=129.12358),\n",
       " Point(x=93.15946, y=129.2744),\n",
       " Point(x=93.23643, y=129.36635),\n",
       " Point(x=93.183426, y=129.51393),\n",
       " Point(x=93.18173, y=129.52328),\n",
       " Point(x=93.22574, y=129.56781),\n",
       " Point(x=93.22383, y=129.65857),\n",
       " Point(x=93.131096, y=129.71179),\n",
       " Point(x=93.10855, y=129.74878),\n",
       " Point(x=93.11973, y=129.82243),\n",
       " Point(x=93.163025, y=129.87904),\n",
       " Point(x=93.09092, y=129.94423),\n",
       " Point(x=93.08056, y=130.00192),\n",
       " Point(x=93.08296, y=130.06726),\n",
       " Point(x=93.085434, y=130.11032),\n",
       " Point(x=93.08657, y=130.13672),\n",
       " Point(x=93.01465, y=130.18152),\n",
       " Point(x=93.01897, y=130.21025),\n",
       " Point(x=92.96258, y=130.28154),\n",
       " Point(x=92.940674, y=130.32411),\n",
       " Point(x=92.91967, y=130.4486),\n",
       " Point(x=92.87498, y=130.44493),\n",
       " Point(x=92.899895, y=130.56488),\n",
       " Point(x=92.89844, y=130.61578),\n",
       " Point(x=92.91313, y=130.6441),\n",
       " Point(x=92.86839, y=130.78871),\n",
       " Point(x=92.83623, y=130.8109),\n",
       " Point(x=92.83847, y=130.856),\n",
       " Point(x=92.86923, y=130.93988),\n",
       " Point(x=92.86234, y=131.02272),\n",
       " Point(x=92.83837, y=131.06898),\n",
       " Point(x=92.78224, y=131.07185),\n",
       " Point(x=92.77783, y=131.11943),\n",
       " Point(x=92.81656, y=131.21199),\n",
       " Point(x=92.84082, y=131.26678),\n",
       " Point(x=92.75813, y=131.34431),\n",
       " Point(x=92.74588, y=131.34729),\n",
       " Point(x=92.77045, y=131.43056),\n",
       " Point(x=92.7345, y=131.42728),\n",
       " Point(x=92.72572, y=131.51488),\n",
       " Point(x=92.7238, y=131.56277),\n",
       " Point(x=92.74305, y=131.68571),\n",
       " Point(x=92.750626, y=131.69485),\n",
       " Point(x=92.662476, y=131.72937),\n",
       " Point(x=92.678925, y=131.75522),\n",
       " Point(x=92.671524, y=131.79282),\n",
       " Point(x=92.70368, y=131.88634),\n",
       " Point(x=92.679436, y=131.90982),\n",
       " Point(x=92.70057, y=131.91931),\n",
       " Point(x=92.59611, y=131.93639),\n",
       " Point(x=92.624146, y=131.9807),\n",
       " Point(x=92.604935, y=132.00243),\n",
       " Point(x=92.57507, y=131.98952),\n",
       " Point(x=92.55832, y=131.99493),\n",
       " Point(x=92.54027, y=132.1099),\n",
       " Point(x=92.517044, y=132.16052),\n",
       " Point(x=92.50811, y=132.183),\n",
       " Point(x=92.46356, y=132.16856),\n",
       " Point(x=92.56405, y=132.30338),\n",
       " Point(x=92.58888, y=132.38033),\n",
       " Point(x=92.53836, y=132.45735),\n",
       " Point(x=92.52817, y=132.45978),\n",
       " Point(x=92.57217, y=132.52547),\n",
       " Point(x=92.56867, y=132.52745),\n",
       " Point(x=92.59761, y=132.68416),\n",
       " Point(x=92.53754, y=132.6809),\n",
       " Point(x=92.54532, y=132.72519),\n",
       " Point(x=92.52236, y=132.75926),\n",
       " Point(x=92.523476, y=132.81216),\n",
       " Point(x=92.45189, y=132.84644),\n",
       " Point(x=92.45441, y=132.97818),\n",
       " Point(x=92.455536, y=132.9957),\n",
       " Point(x=92.37488, y=132.99992),\n",
       " Point(x=92.435555, y=133.09666),\n",
       " Point(x=92.39018, y=133.08188),\n",
       " Point(x=92.43959, y=133.19888),\n",
       " Point(x=92.42178, y=133.2735),\n",
       " Point(x=92.39101, y=133.32805),\n",
       " Point(x=92.372314, y=133.43913),\n",
       " Point(x=92.394615, y=133.47598),\n",
       " Point(x=92.33879, y=133.55518),\n",
       " Point(x=92.31784, y=133.64876),\n",
       " Point(x=92.31028, y=133.72185),\n",
       " Point(x=92.31515, y=133.8175),\n",
       " Point(x=92.32234, y=133.89134),\n",
       " Point(x=92.24523, y=133.90706),\n",
       " Point(x=92.16675, y=133.95052),\n",
       " Point(x=92.179276, y=134.00198),\n",
       " Point(x=92.15741, y=134.03319),\n",
       " Point(x=92.19712, y=134.21013),\n",
       " Point(x=92.1347, y=134.23969),\n",
       " Point(x=92.083496, y=134.35422),\n",
       " Point(x=92.028625, y=134.42142),\n",
       " Point(x=92.03115, y=134.49956),\n",
       " Point(x=91.925354, y=134.52486),\n",
       " Point(x=91.899315, y=134.58597),\n",
       " Point(x=91.84642, y=134.67886),\n",
       " Point(x=91.85466, y=134.82579),\n",
       " Point(x=91.80857, y=134.9156),\n",
       " Point(x=91.7346, y=134.96024),\n",
       " Point(x=91.68501, y=135.04979),\n",
       " Point(x=91.607666, y=135.09282),\n",
       " Point(x=91.5685, y=135.15016),\n",
       " Point(x=91.541084, y=135.27036),\n",
       " Point(x=91.46533, y=135.27884),\n",
       " Point(x=91.38522, y=135.27176),\n",
       " Point(x=91.3551, y=135.33897),\n",
       " Point(x=91.277954, y=135.37332),\n",
       " Point(x=91.16676, y=135.32504),\n",
       " Point(x=91.07734, y=135.36267),\n",
       " Point(x=91.082504, y=135.37059),\n",
       " Point(x=91.11438, y=135.38463),\n",
       " Point(x=91.13411, y=135.43842),\n",
       " Point(x=91.10183, y=135.39307),\n",
       " Point(x=91.07622, y=135.34785),\n",
       " Point(x=91.07995, y=135.3507),\n",
       " Point(x=91.00087, y=135.27664),\n",
       " Point(x=90.99035, y=135.28905),\n",
       " Point(x=90.97359, y=135.28432),\n",
       " Point(x=90.93818, y=135.25755),\n",
       " Point(x=90.95776, y=135.27629),\n",
       " Point(x=90.93079, y=135.2713),\n",
       " Point(x=90.92841, y=135.2266),\n",
       " Point(x=90.95063, y=135.24545),\n",
       " Point(x=90.92667, y=135.23793),\n",
       " Point(x=90.94094, y=135.29141),\n",
       " Point(x=90.966896, y=135.28477),\n",
       " Point(x=90.957, y=135.24292),\n",
       " Point(x=90.90076, y=135.16876),\n",
       " Point(x=90.91854, y=135.18243),\n",
       " Point(x=90.858444, y=135.13708),\n",
       " Point(x=90.843, y=135.16824),\n",
       " Point(x=90.860756, y=135.1431),\n",
       " Point(x=90.830185, y=135.13133),\n",
       " Point(x=90.82165, y=135.10901),\n",
       " Point(x=90.80331, y=135.08592),\n",
       " Point(x=90.75235, y=135.09796),\n",
       " Point(x=90.70585, y=135.05927),\n",
       " Point(x=90.745605, y=135.08023),\n",
       " Point(x=90.72398, y=135.06567),\n",
       " Point(x=90.703995, y=134.99034),\n",
       " Point(x=90.66097, y=134.96017),\n",
       " Point(x=90.68434, y=134.95909),\n",
       " Point(x=90.725494, y=134.9856),\n",
       " Point(x=90.65076, y=134.94804),\n",
       " Point(x=90.60627, y=134.90782),\n",
       " Point(x=90.559685, y=134.87753),\n",
       " Point(x=90.50998, y=134.86552),\n",
       " Point(x=90.511665, y=134.85464),\n",
       " Point(x=90.45414, y=134.81433),\n",
       " Point(x=90.42962, y=134.76523),\n",
       " Point(x=90.475235, y=134.78069),\n",
       " Point(x=90.42899, y=134.74808),\n",
       " Point(x=90.37794, y=134.73494),\n",
       " Point(x=90.36154, y=134.71875),\n",
       " Point(x=90.339005, y=134.6529),\n",
       " Point(x=90.35681, y=134.64737),\n",
       " Point(x=90.30083, y=134.59212),\n",
       " Point(x=90.2382, y=134.5643),\n",
       " Point(x=90.220276, y=134.53406),\n",
       " Point(x=90.24906, y=134.55),\n",
       " Point(x=90.22388, y=134.51712),\n",
       " Point(x=90.19949, y=134.47559),\n",
       " Point(x=90.10877, y=134.4394),\n",
       " Point(x=90.14629, y=134.43033),\n",
       " Point(x=90.12065, y=134.41731),\n",
       " Point(x=90.09756, y=134.40353),\n",
       " Point(x=90.05266, y=134.33517),\n",
       " Point(x=90.03461, y=134.33168),\n",
       " Point(x=89.988235, y=134.25607),\n",
       " Point(x=89.95592, y=134.26779),\n",
       " Point(x=89.88634, y=134.24734),\n",
       " Point(x=89.8829, y=134.17719),\n",
       " Point(x=89.86722, y=134.17235),\n",
       " Point(x=89.82641, y=134.09828),\n",
       " Point(x=89.78951, y=134.07617),\n",
       " Point(x=89.81662, y=134.04948),\n",
       " Point(x=89.82705, y=133.96747),\n",
       " Point(x=89.79676, y=133.95801),\n",
       " Point(x=89.76127, y=133.96614),\n",
       " Point(x=89.76043, y=133.99728),\n",
       " Point(x=89.731186, y=133.93788),\n",
       " Point(x=89.58424, y=133.82274),\n",
       " Point(x=89.636696, y=133.82646),\n",
       " Point(x=89.596115, y=133.78526),\n",
       " Point(x=89.624695, y=133.75749),\n",
       " Point(x=89.5242, y=133.80264),\n",
       " Point(x=89.51682, y=133.73987),\n",
       " Point(x=89.48164, y=133.74344),\n",
       " Point(x=89.50944, y=133.67746),\n",
       " Point(x=89.465836, y=133.62053),\n",
       " Point(x=89.404335, y=133.56558),\n",
       " Point(x=89.46455, y=133.587),\n",
       " Point(x=89.43533, y=133.60283),\n",
       " Point(x=89.33612, y=133.52637),\n",
       " Point(x=89.32046, y=133.46896),\n",
       " Point(x=89.289185, y=133.4522),\n",
       " Point(x=89.2786, y=133.39748),\n",
       " Point(x=89.19711, y=133.35483),\n",
       " Point(x=89.22122, y=133.37326),\n",
       " Point(x=89.171074, y=133.29738),\n",
       " Point(x=89.12834, y=133.28615),\n",
       " Point(x=89.07052, y=133.27466),\n",
       " Point(x=88.96817, y=133.22992),\n",
       " Point(x=88.965775, y=133.2178),\n",
       " Point(x=88.95894, y=133.25401),\n",
       " Point(x=88.94181, y=133.23947),\n",
       " Point(x=88.923355, y=133.20901),\n",
       " Point(x=88.90721, y=133.21109),\n",
       " Point(x=88.87247, y=133.15396),\n",
       " Point(x=88.81182, y=133.13574),\n",
       " Point(x=88.83038, y=133.14575),\n",
       " Point(x=88.85227, y=133.20244),\n",
       " Point(x=88.80474, y=133.18187),\n",
       " Point(x=88.73538, y=133.17757),\n",
       " Point(x=88.67314, y=133.12961),\n",
       " Point(x=88.61403, y=133.10281),\n",
       " Point(x=88.62256, y=133.09392),\n",
       " Point(x=88.666275, y=133.16693),\n",
       " Point(x=88.68646, y=133.18552),\n",
       " Point(x=88.62327, y=133.14688),\n",
       " Point(x=88.60671, y=133.13611),\n",
       " Point(x=88.54226, y=133.09795),\n",
       " Point(x=88.53045, y=133.12257),\n",
       " Point(x=88.55511, y=133.13062),\n",
       " Point(x=88.517586, y=133.1283),\n",
       " Point(x=88.50587, y=133.17764),\n",
       " Point(x=88.51015, y=133.19493),\n",
       " Point(x=88.4702, y=133.19025),\n",
       " Point(x=88.4764, y=133.1738),\n",
       " Point(x=88.42807, y=133.17067),\n",
       " Point(x=88.44956, y=133.27785),\n",
       " Point(x=88.4556, y=133.28578),\n",
       " Point(x=88.428734, y=133.25333),\n",
       " Point(x=88.4653, y=133.26857),\n",
       " Point(x=88.466064, y=133.31685),\n",
       " Point(x=88.42136, y=133.35033),\n",
       " Point(x=88.42132, y=133.41443),\n",
       " Point(x=88.40679, y=133.4284),\n",
       " Point(x=88.444016, y=133.51921),\n",
       " Point(x=88.434845, y=133.4909),\n",
       " Point(x=88.42936, y=133.4837),\n",
       " Point(x=88.42991, y=133.50323),\n",
       " Point(x=88.41046, y=133.51967),\n",
       " Point(x=88.489105, y=133.62024),\n",
       " Point(x=88.47731, y=133.61064),\n",
       " Point(x=88.480354, y=133.67183),\n",
       " Point(x=88.50198, y=133.6867),\n",
       " Point(x=88.50972, y=133.70657),\n",
       " Point(x=88.43089, y=133.75546),\n",
       " Point(x=88.41541, y=133.79578),\n",
       " Point(x=88.458565, y=133.78609),\n",
       " Point(x=88.437, y=133.84576),\n",
       " Point(x=88.47825, y=133.91422),\n",
       " Point(x=88.48226, y=133.93372),\n",
       " Point(x=88.49554, y=133.95743),\n",
       " Point(x=88.51768, y=134.02324),\n",
       " Point(x=88.51217, y=134.05989),\n",
       " Point(x=88.51928, y=134.0406),\n",
       " Point(x=88.5561, y=134.11948),\n",
       " Point(x=88.52915, y=134.13638),\n",
       " Point(x=88.488914, y=134.14421),\n",
       " Point(x=88.52838, y=134.15714),\n",
       " Point(x=88.52814, y=134.19179),\n",
       " Point(x=88.54799, y=134.21837),\n",
       " Point(x=88.580795, y=134.25165),\n",
       " Point(x=88.575584, y=134.25735),\n",
       " Point(x=88.54196, y=134.22847),\n",
       " Point(x=88.5531, y=134.19012),\n",
       " Point(x=88.57734, y=134.23311),\n",
       " Point(x=88.56001, y=134.25732),\n",
       " Point(x=88.57328, y=134.24437),\n",
       " Point(x=88.55966, y=134.29356),\n",
       " Point(x=88.578804, y=134.30159),\n",
       " Point(x=88.58192, y=134.28343),\n",
       " Point(x=88.5632, y=134.26413),\n",
       " Point(x=88.54579, y=134.23035),\n",
       " Point(x=88.56377, y=134.30528),\n",
       " Point(x=88.57934, y=134.30553),\n",
       " Point(x=88.57373, y=134.28787),\n",
       " Point(x=88.574776, y=134.26935),\n",
       " Point(x=88.58682, y=134.26285),\n",
       " Point(x=88.56814, y=134.22194),\n",
       " Point(x=88.55591, y=134.22353),\n",
       " Point(x=88.572586, y=134.26051),\n",
       " Point(x=88.5828, y=134.22935),\n",
       " Point(x=88.62026, y=134.22322),\n",
       " Point(x=88.64294, y=134.27107),\n",
       " Point(x=88.62616, y=134.24193),\n",
       " Point(x=88.61621, y=134.22134),\n",
       " Point(x=88.61911, y=134.30455),\n",
       " Point(x=88.61815, y=134.28915),\n",
       " Point(x=88.646065, y=134.30257),\n",
       " Point(x=88.626724, y=134.2587),\n",
       " Point(x=88.64788, y=134.25294),\n",
       " Point(x=88.657906, y=134.22719),\n",
       " Point(x=88.64218, y=134.2373),\n",
       " Point(x=88.639534, y=134.2147),\n",
       " Point(x=88.649895, y=134.20874),\n",
       " Point(x=88.6793, y=134.26283),\n",
       " Point(x=88.70964, y=134.25922),\n",
       " Point(x=88.70633, y=134.22719),\n",
       " Point(x=88.6953, y=134.22504),\n",
       " Point(x=88.71004, y=134.26007),\n",
       " Point(x=88.65919, y=134.28905),\n",
       " Point(x=88.6377, y=134.29568),\n",
       " Point(x=88.6634, y=134.29059),\n",
       " Point(x=88.66974, y=134.29123),\n",
       " Point(x=88.67092, y=134.33098),\n",
       " Point(x=88.65867, y=134.27548),\n",
       " Point(x=88.66893, y=134.30894),\n",
       " Point(x=88.68787, y=134.32248),\n",
       " Point(x=88.70299, y=134.32214),\n",
       " Point(x=88.69124, y=134.27835),\n",
       " Point(x=88.66821, y=134.27602),\n",
       " Point(x=88.7223, y=134.27748),\n",
       " Point(x=88.72917, y=134.29431),\n",
       " Point(x=88.70848, y=134.26614),\n",
       " Point(x=88.68187, y=134.26099),\n",
       " Point(x=88.7361, y=134.27097),\n",
       " Point(x=88.705986, y=134.29848),\n",
       " Point(x=88.69338, y=134.32262),\n",
       " Point(x=88.69641, y=134.30814),\n",
       " Point(x=88.72783, y=134.36433),\n",
       " Point(x=88.74254, y=134.36064),\n",
       " Point(x=88.72064, y=134.33217),\n",
       " Point(x=88.69762, y=134.29465),\n",
       " Point(x=88.72958, y=134.27364),\n",
       " Point(x=88.742004, y=134.26709),\n",
       " Point(x=88.7149, y=134.26817),\n",
       " Point(x=88.70598, y=134.22914),\n",
       " Point(x=88.70551, y=134.24266),\n",
       " Point(x=88.62136, y=134.24449),\n",
       " Point(x=88.63688, y=134.23155),\n",
       " Point(x=88.61684, y=134.22229),\n",
       " Point(x=88.67128, y=134.25333),\n",
       " Point(x=88.68328, y=134.2751),\n",
       " Point(x=88.66423, y=134.24197),\n",
       " Point(x=88.64631, y=134.2756),\n",
       " Point(x=88.586174, y=134.23941),\n",
       " Point(x=88.62929, y=134.21964),\n",
       " Point(x=88.6292, y=134.19832),\n",
       " Point(x=88.63768, y=134.22522),\n",
       " Point(x=88.63632, y=134.18613),\n",
       " Point(x=88.59914, y=134.18767),\n",
       " Point(x=88.57967, y=134.19417),\n",
       " Point(x=88.59848, y=134.20895),\n",
       " Point(x=88.60432, y=134.20847),\n",
       " Point(x=88.62343, y=134.26631),\n",
       " Point(x=88.615234, y=134.25868),\n",
       " Point(x=88.60257, y=134.20934),\n",
       " Point(x=88.57002, y=134.15895),\n",
       " Point(x=88.558136, y=134.15886),\n",
       " Point(x=88.597275, y=134.14775),\n",
       " Point(x=88.61945, y=134.1476),\n",
       " Point(x=88.58372, y=134.1794),\n",
       " Point(x=88.52176, y=134.18217),\n",
       " Point(x=88.56, y=134.19653),\n",
       " Point(x=88.555435, y=134.23692),\n",
       " Point(x=88.49271, y=134.20612),\n",
       " Point(x=88.507904, y=134.17886),\n",
       " Point(x=88.519035, y=134.1891),\n",
       " Point(x=88.547905, y=134.19977),\n",
       " Point(x=88.55078, y=134.1767),\n",
       " Point(x=88.56411, y=134.1883),\n",
       " Point(x=88.584785, y=134.21542),\n",
       " Point(x=88.62159, y=134.22624),\n",
       " Point(x=88.62842, y=134.20424),\n",
       " Point(x=88.66955, y=134.2726),\n",
       " Point(x=88.67655, y=134.28937),\n",
       " Point(x=88.68334, y=134.32693),\n",
       " Point(x=88.6742, y=134.3345),\n",
       " Point(x=88.64172, y=134.32005),\n",
       " Point(x=88.67084, y=134.33826),\n",
       " Point(x=88.69955, y=134.35161),\n",
       " Point(x=88.68672, y=134.35074),\n",
       " Point(x=88.696526, y=134.3862),\n",
       " Point(x=88.71445, y=134.37323),\n",
       " Point(x=88.72913, y=134.3351),\n",
       " Point(x=88.7067, y=134.33434),\n",
       " Point(x=88.78904, y=134.41309),\n",
       " Point(x=88.81922, y=134.42688),\n",
       " Point(x=88.81408, y=134.4239),\n",
       " Point(x=88.817154, y=134.3916),\n",
       " Point(x=88.82145, y=134.37576),\n",
       " Point(x=88.80658, y=134.38173),\n",
       " Point(x=88.83682, y=134.39093),\n",
       " Point(x=88.85719, y=134.42598),\n",
       " Point(x=88.89768, y=134.4549),\n",
       " Point(x=88.86191, y=134.46915),\n",
       " Point(x=88.85527, y=134.46297),\n",
       " Point(x=88.856735, y=134.47127),\n",
       " Point(x=88.8553, y=134.49483),\n",
       " Point(x=88.9118, y=134.52106),\n",
       " Point(x=88.9216, y=134.48158),\n",
       " Point(x=88.97467, y=134.54005),\n",
       " Point(x=88.9296, y=134.49132),\n",
       " Point(x=88.94743, y=134.51881),\n",
       " Point(x=88.96227, y=134.52234),\n",
       " Point(x=89.00264, y=134.54185),\n",
       " Point(x=89.01955, y=134.53685),\n",
       " Point(x=89.081795, y=134.5123),\n",
       " Point(x=89.08863, y=134.50793),\n",
       " Point(x=89.07781, y=134.53412),\n",
       " Point(x=89.08685, y=134.51418),\n",
       " Point(x=89.10833, y=134.5208),\n",
       " Point(x=89.14714, y=134.55054),\n",
       " Point(x=89.192276, y=134.56146),\n",
       " Point(x=89.215164, y=134.60869),\n",
       " Point(x=89.21118, y=134.597),\n",
       " Point(x=89.20245, y=134.56712),\n",
       " Point(x=89.23763, y=134.61688),\n",
       " Point(x=89.22623, y=134.58829),\n",
       " Point(x=89.26654, y=134.58601),\n",
       " Point(x=89.28843, y=134.62976),\n",
       " Point(x=89.33319, y=134.67459),\n",
       " Point(x=89.353584, y=134.67151),\n",
       " Point(x=89.37196, y=134.71455),\n",
       " Point(x=89.36343, y=134.72874),\n",
       " Point(x=89.383934, y=134.77985),\n",
       " Point(x=89.36545, y=134.76622),\n",
       " Point(x=89.375946, y=134.79608),\n",
       " Point(x=89.39461, y=134.78893),\n",
       " Point(x=89.39531, y=134.79802),\n",
       " Point(x=89.36401, y=134.79662),\n",
       " Point(x=89.37338, y=134.89381),\n",
       " Point(x=89.3507, y=134.86812),\n",
       " Point(x=89.38462, y=134.89622),\n",
       " Point(x=89.38382, y=134.91512),\n",
       " Point(x=89.4075, y=134.92569),\n",
       " Point(x=89.40177, y=134.93349),\n",
       " Point(x=89.40574, y=134.97498),\n",
       " Point(x=89.42987, y=135.02596),\n",
       " Point(x=89.42445, y=135.02766),\n",
       " Point(x=89.43634, y=135.04086),\n",
       " Point(x=89.44603, y=135.02786),\n",
       " Point(x=89.41515, y=135.00272),\n",
       " Point(x=89.448135, y=135.04092),\n",
       " Point(x=89.48527, y=135.08711),\n",
       " Point(x=89.504616, y=135.15222),\n",
       " Point(x=89.52356, y=135.14336),\n",
       " Point(x=89.53759, y=135.12537),\n",
       " Point(x=89.56618, y=135.15799),\n",
       " Point(x=89.490555, y=135.1771),\n",
       " Point(x=89.522385, y=135.20107),\n",
       " Point(x=89.538414, y=135.28014),\n",
       " Point(x=89.5304, y=135.30635),\n",
       " Point(x=89.5718, y=135.34294),\n",
       " Point(x=89.557434, y=135.32341),\n",
       " Point(x=89.6026, y=135.34575),\n",
       " Point(x=89.61316, y=135.38202),\n",
       " Point(x=89.605156, y=135.4013),\n",
       " Point(x=89.58122, y=135.4197),\n",
       " Point(x=89.590935, y=135.41443),\n",
       " Point(x=89.64863, y=135.46423),\n",
       " Point(x=89.63583, y=135.46207),\n",
       " Point(x=89.63561, y=135.45392),\n",
       " Point(x=89.62628, y=135.46127),\n",
       " Point(x=89.66344, y=135.55373),\n",
       " Point(x=89.66857, y=135.5537),\n",
       " Point(x=89.63075, y=135.50745),\n",
       " Point(x=89.67427, y=135.55826),\n",
       " Point(x=89.64865, y=135.59924),\n",
       " Point(x=89.67168, y=135.64294),\n",
       " Point(x=89.66545, y=135.62665),\n",
       " Point(x=89.66119, y=135.6617),\n",
       " Point(x=89.65247, y=135.65051),\n",
       " Point(x=89.707115, y=135.69206),\n",
       " Point(x=89.68008, y=135.68309),\n",
       " Point(x=89.68724, y=135.67972),\n",
       " Point(x=89.72917, y=135.73402),\n",
       " Point(x=89.72738, y=135.75476),\n",
       " Point(x=89.73485, y=135.79636),\n",
       " Point(x=89.72843, y=135.77179),\n",
       " Point(x=89.71035, y=135.8033),\n",
       " Point(x=89.706245, y=135.85481),\n",
       " Point(x=89.76597, y=135.87808),\n",
       " Point(x=89.80324, y=135.88094),\n",
       " Point(x=89.81835, y=135.87502),\n",
       " Point(x=89.81428, y=135.89822),\n",
       " Point(x=89.80269, y=135.9205),\n",
       " Point(x=89.76586, y=135.96333),\n",
       " Point(x=89.80264, y=136.00273),\n",
       " Point(x=89.83596, y=136.01826),\n",
       " Point(x=89.82474, y=136.01257),\n",
       " Point(x=89.82466, y=136.03189),\n",
       " Point(x=89.792, y=136.01923),\n",
       " Point(x=89.83293, y=136.0714),\n",
       " Point(x=89.81821, y=136.09348),\n",
       " Point(x=89.83757, y=136.11626),\n",
       " Point(x=89.87741, y=136.11415),\n",
       " Point(x=89.88658, y=136.12932),\n",
       " Point(x=89.90738, y=136.18347),\n",
       " Point(x=89.89842, y=136.17448),\n",
       " Point(x=89.875656, y=136.1627),\n",
       " Point(x=89.87999, y=136.186),\n",
       " Point(x=89.86544, y=136.1662),\n",
       " Point(x=89.94039, y=136.14806),\n",
       " Point(x=89.89367, y=136.13054),\n",
       " Point(x=89.91455, y=136.21126),\n",
       " Point(x=89.90083, y=136.26859),\n",
       " Point(x=89.916626, y=136.28625),\n",
       " Point(x=89.92851, y=136.2742),\n",
       " Point(x=89.9285, y=136.32632),\n",
       " Point(x=89.89783, y=136.31981),\n",
       " Point(x=89.92531, y=136.33759),\n",
       " Point(x=89.955734, y=136.38469),\n",
       " Point(x=89.934784, y=136.40878),\n",
       " Point(x=89.92159, y=136.43036),\n",
       " Point(x=89.91448, y=136.43037),\n",
       " Point(x=89.913864, y=136.43053),\n",
       " Point(x=89.92702, y=136.46754),\n",
       " Point(x=89.96781, y=136.51205),\n",
       " Point(x=89.98317, y=136.5522),\n",
       " Point(x=90.032555, y=136.57939),\n",
       " Point(x=90.0331, y=136.5631),\n",
       " Point(x=90.02469, y=136.56056),\n",
       " Point(x=90.01579, y=136.5765),\n",
       " Point(x=90.00548, y=136.5678),\n",
       " Point(x=89.99575, y=136.56401),\n",
       " Point(x=90.00624, y=136.63055),\n",
       " Point(x=90.04811, y=136.6649),\n",
       " Point(x=90.0281, y=136.658),\n",
       " Point(x=90.0222, y=136.65726),\n",
       " Point(x=89.98849, y=136.62994),\n",
       " Point(x=90.01968, y=136.67258),\n",
       " Point(x=90.02774, y=136.652),\n",
       " Point(x=90.06039, y=136.73274),\n",
       " Point(x=90.123566, y=136.75359),\n",
       " Point(x=90.12103, y=136.74178),\n",
       " Point(x=90.116455, y=136.71436),\n",
       " Point(x=90.092155, y=136.76437),\n",
       " Point(x=90.129074, y=136.81194),\n",
       " Point(x=90.13877, y=136.79527),\n",
       " Point(x=90.191246, y=136.84668),\n",
       " Point(x=90.18878, y=136.85939),\n",
       " Point(x=90.17424, y=136.87518),\n",
       " Point(x=90.15155, y=136.87663),\n",
       " Point(x=90.15619, y=136.89081),\n",
       " Point(x=90.116165, y=136.89685),\n",
       " Point(x=90.09587, y=136.8955),\n",
       " Point(x=90.17227, y=136.97124),\n",
       " Point(x=90.16456, y=137.0215),\n",
       " Point(x=90.176796, y=137.0453),\n",
       " Point(x=90.17319, y=137.04092),\n",
       " Point(x=90.160446, y=137.0593),\n",
       " Point(x=90.16105, y=137.05922),\n",
       " Point(x=90.15776, y=137.05573),\n",
       " Point(x=90.17499, y=137.08969),\n",
       " Point(x=90.19746, y=137.13124),\n",
       " Point(x=90.22939, y=137.1426),\n",
       " Point(x=90.24079, y=137.14452),\n",
       " Point(x=90.21971, y=137.13567),\n",
       " Point(x=90.20891, y=137.14847),\n",
       " Point(x=90.212814, y=137.14394),\n",
       " Point(x=90.243484, y=137.17892),\n",
       " Point(x=90.27535, y=137.24477),\n",
       " Point(x=90.29054, y=137.25385),\n",
       " Point(x=90.28934, y=137.25336),\n",
       " Point(x=90.29084, y=137.25615),\n",
       " Point(x=90.311806, y=137.3084),\n",
       " Point(x=90.32505, y=137.2913),\n",
       " Point(x=90.31166, y=137.30525),\n",
       " Point(x=90.358665, y=137.35934),\n",
       " Point(x=90.3702, y=137.38028),\n",
       " Point(x=90.35474, y=137.37288),\n",
       " Point(x=90.32757, y=137.36053),\n",
       " Point(x=90.329544, y=137.35898),\n",
       " Point(x=90.341415, y=137.40709),\n",
       " Point(x=90.39316, y=137.43427),\n",
       " Point(x=90.39077, y=137.51477),\n",
       " Point(x=90.37885, y=137.49881),\n",
       " Point(x=90.38601, y=137.52243),\n",
       " Point(x=90.350044, y=137.49445),\n",
       " Point(x=90.37694, y=137.53102),\n",
       " Point(x=90.36244, y=137.53864),\n",
       " Point(x=90.36066, y=137.52997),\n",
       " Point(x=90.35385, y=137.56294),\n",
       " Point(x=90.37083, y=137.57802),\n",
       " Point(x=90.45101, y=137.62704),\n",
       " Point(x=90.43763, y=137.63464),\n",
       " Point(x=90.44205, y=137.66666),\n",
       " Point(x=90.43498, y=137.6691),\n",
       " Point(x=90.463936, y=137.69977),\n",
       " Point(x=90.456825, y=137.6852),\n",
       " Point(x=90.4792, y=137.75671),\n",
       " Point(x=90.476616, y=137.75613),\n",
       " Point(x=90.474785, y=137.72183),\n",
       " Point(x=90.48813, y=137.755),\n",
       " Point(x=90.51104, y=137.78712),\n",
       " Point(x=90.50788, y=137.79306),\n",
       " Point(x=90.520226, y=137.77632),\n",
       " Point(x=90.50261, y=137.82997),\n",
       " Point(x=90.50465, y=137.85982),\n",
       " Point(x=90.52988, y=137.8936),\n",
       " Point(x=90.49472, y=137.86594),\n",
       " Point(x=90.50806, y=137.8933),\n",
       " Point(x=90.52614, y=137.89818),\n",
       " Point(x=90.54646, y=137.9225),\n",
       " Point(x=90.54567, y=137.95735),\n",
       " Point(x=90.53892, y=137.97719),\n",
       " Point(x=90.54735, y=138.00418),\n",
       " Point(x=90.57069, y=137.99701),\n",
       " Point(x=90.58922, y=138.00487),\n",
       " Point(x=90.61826, y=138.02443),\n",
       " Point(x=90.60427, y=138.03636),\n",
       " Point(x=90.58739, y=138.0539),\n",
       " Point(x=90.62482, y=138.14905),\n",
       " Point(x=90.59722, y=138.13661),\n",
       " Point(x=90.59194, y=138.1394),\n",
       " Point(x=90.60732, y=138.15796),\n",
       " Point(x=90.62857, y=138.1698),\n",
       " Point(x=90.65466, y=138.1835),\n",
       " Point(x=90.70461, y=138.20448),\n",
       " Point(x=90.67481, y=138.23842),\n",
       " Point(x=90.6929, y=138.25746),\n",
       " Point(x=90.66046, y=138.23041),\n",
       " Point(x=90.67858, y=138.24605),\n",
       " Point(x=90.70475, y=138.28845),\n",
       " Point(x=90.728874, y=138.31068),\n",
       " Point(x=90.756516, y=138.30975),\n",
       " Point(x=90.68809, y=138.27184),\n",
       " Point(x=90.73296, y=138.33029),\n",
       " Point(x=90.690735, y=138.32436),\n",
       " Point(x=90.69145, y=138.3325),\n",
       " Point(x=90.67575, y=138.31973),\n",
       " Point(x=90.66063, y=138.33746),\n",
       " Point(x=90.6876, y=138.3705),\n",
       " Point(x=90.71905, y=138.39136),\n",
       " Point(x=90.75134, y=138.40102),\n",
       " Point(x=90.75182, y=138.42444),\n",
       " Point(x=90.738945, y=138.41614),\n",
       " Point(x=90.76019, y=138.43419),\n",
       " Point(x=90.74843, y=138.43394),\n",
       " Point(x=90.794556, y=138.47116),\n",
       " Point(x=90.80547, y=138.47379),\n",
       " Point(x=90.800186, y=138.47392),\n",
       " Point(x=90.82262, y=138.51529),\n",
       " Point(x=90.82473, y=138.53874),\n",
       " Point(x=90.830696, y=138.57872),\n",
       " Point(x=90.84771, y=138.63121),\n",
       " Point(x=90.85095, y=138.64983),\n",
       " Point(x=90.82158, y=138.65364),\n",
       " Point(x=90.84756, y=138.6709),\n",
       " Point(x=90.8304, y=138.67822),\n",
       " Point(x=90.8566, y=138.72978),\n",
       " Point(x=90.842514, y=138.73267),\n",
       " Point(x=90.88811, y=138.74332),\n",
       " Point(x=90.87976, y=138.74026),\n",
       " Point(x=90.879326, y=138.7516),\n",
       " Point(x=90.89787, y=138.74649),\n",
       " Point(x=90.92653, y=138.77684),\n",
       " Point(x=90.96532, y=138.80328),\n",
       " Point(x=90.953125, y=138.84767),\n",
       " Point(x=90.93576, y=138.85095),\n",
       " Point(x=90.89142, y=138.82071),\n",
       " Point(x=90.91658, y=138.83374),\n",
       " Point(x=90.92288, y=138.85732),\n",
       " Point(x=90.89413, y=138.828),\n",
       " Point(x=90.88124, y=138.85252),\n",
       " Point(x=90.86755, y=138.86124),\n",
       " Point(x=90.886566, y=138.9301),\n",
       " Point(x=90.91375, y=138.93867),\n",
       " Point(x=90.89867, y=138.91908),\n",
       " Point(x=90.91006, y=138.91817),\n",
       " Point(x=90.91888, y=138.94965),\n",
       " Point(x=90.89508, y=138.94543),\n",
       " Point(x=90.95069, y=138.94933),\n",
       " Point(x=90.94573, y=138.99074),\n",
       " Point(x=90.95785, y=139.02676),\n",
       " Point(x=90.96466, y=139.04187),\n",
       " Point(x=90.96137, y=139.01654),\n",
       " Point(x=91.00995, y=139.0032),\n",
       " Point(x=91.010765, y=139.05711),\n",
       " Point(x=91.01512, y=139.0801),\n",
       " Point(x=91.07649, y=139.1106),\n",
       " Point(x=91.098564, y=139.16292),\n",
       " Point(x=91.10173, y=139.18863),\n",
       " Point(x=91.079765, y=139.18205),\n",
       " Point(x=91.07366, y=139.16075),\n",
       " Point(x=91.06003, y=139.16101),\n",
       " Point(x=91.0686, y=139.1912),\n",
       " Point(x=91.06303, y=139.2206),\n",
       " Point(x=91.09605, y=139.2291),\n",
       " Point(x=91.09385, y=139.23369),\n",
       " Point(x=91.13219, y=139.2633),\n",
       " Point(x=91.11152, y=139.26247),\n",
       " Point(x=91.105576, y=139.25018),\n",
       " Point(x=91.122665, y=139.24686),\n",
       " Point(x=91.10124, y=139.23636),\n",
       " Point(x=91.107086, y=139.27823),\n",
       " Point(x=91.0253, y=139.27092),\n",
       " Point(x=91.06506, y=139.32523),\n",
       " Point(x=91.089615, y=139.35097),\n",
       " Point(x=91.13494, y=139.3906),\n",
       " Point(x=91.13517, y=139.40323),\n",
       " Point(x=91.15571, y=139.40984),\n",
       " Point(x=91.15504, y=139.39386),\n",
       " Point(x=91.154785, y=139.3745),\n",
       " Point(x=91.16992, y=139.40668),\n",
       " Point(x=91.1896, y=139.45917),\n",
       " Point(x=91.206726, y=139.45961),\n",
       " Point(x=91.21808, y=139.4893),\n",
       " Point(x=91.22454, y=139.50882),\n",
       " Point(x=91.21327, y=139.52498),\n",
       " Point(x=91.256996, y=139.5801),\n",
       " Point(x=91.2597, y=139.59846),\n",
       " Point(x=91.25383, y=139.58575),\n",
       " Point(x=91.24551, y=139.58363),\n",
       " Point(x=91.24668, y=139.60326),\n",
       " Point(x=91.29996, y=139.63043),\n",
       " Point(x=91.31901, y=139.63268),\n",
       " Point(x=91.27283, y=139.62177),\n",
       " Point(x=91.27043, y=139.65166),\n",
       " Point(x=91.29405, y=139.73448),\n",
       " Point(x=91.28369, y=139.72957),\n",
       " Point(x=91.30436, y=139.77083),\n",
       " Point(x=91.31427, y=139.77354),\n",
       " Point(x=91.292015, y=139.75209),\n",
       " Point(x=91.334656, y=139.81163),\n",
       " Point(x=91.36022, y=139.803),\n",
       " Point(x=91.30536, y=139.7872),\n",
       " Point(x=91.33705, y=139.78458),\n",
       " Point(x=91.32728, y=139.82303),\n",
       " Point(x=91.37003, y=139.85017),\n",
       " Point(x=91.37804, y=139.85344),\n",
       " Point(x=91.40265, y=139.84926),\n",
       " Point(x=91.39365, y=139.8638),\n",
       " Point(x=91.41572, y=139.91023),\n",
       " Point(x=91.40683, y=139.91676),\n",
       " Point(x=91.39654, y=139.92595),\n",
       " Point(x=91.435265, y=139.98622),\n",
       " Point(x=91.42615, y=139.99104),\n",
       " Point(x=91.433975, y=139.98769),\n",
       " Point(x=91.49273, y=140.03368),\n",
       " Point(x=91.50044, y=140.05289),\n",
       " Point(x=91.552376, y=140.07347),\n",
       " Point(x=91.576385, y=140.0811),\n",
       " Point(x=91.56973, y=140.09875),\n",
       " Point(x=91.559296, y=140.09283),\n",
       " Point(x=91.59447, y=140.13644),\n",
       " Point(x=91.592674, y=140.15363),\n",
       " Point(x=91.58402, y=140.16795),\n",
       " Point(x=91.60177, y=140.18657),\n",
       " Point(x=91.60715, y=140.18237),\n",
       " Point(x=91.62817, y=140.21301),\n",
       " Point(x=91.624725, y=140.19696),\n",
       " Point(x=91.66965, y=140.26567),\n",
       " Point(x=91.65936, y=140.25124),\n",
       " Point(x=91.64945, y=140.28078),\n",
       " Point(x=91.62374, y=140.25925),\n",
       " Point(x=91.65409, y=140.31244),\n",
       " Point(x=91.67817, y=140.33012),\n",
       " Point(x=91.725006, y=140.36046),\n",
       " Point(x=91.724266, y=140.34709),\n",
       " Point(x=91.710655, y=140.30711),\n",
       " Point(x=91.71415, y=140.35373),\n",
       " Point(x=91.73928, y=140.40988),\n",
       " Point(x=91.7081, y=140.37007),\n",
       " Point(x=91.7094, y=140.37311),\n",
       " Point(x=91.76254, y=140.42502),\n",
       " Point(x=91.74398, y=140.43416),\n",
       " Point(x=91.72107, y=140.42581),\n",
       " Point(x=91.73382, y=140.42316),\n",
       " Point(x=91.763855, y=140.46347),\n",
       " Point(x=91.72858, y=140.45589),\n",
       " Point(x=91.74242, y=140.51126),\n",
       " Point(x=91.75684, y=140.54053),\n",
       " Point(x=91.764114, y=140.55222),\n",
       " Point(x=91.82718, y=140.59871),\n",
       " Point(x=91.79989, y=140.60316),\n",
       " Point(x=91.82112, y=140.60097),\n",
       " Point(x=91.84744, y=140.61667),\n",
       " Point(x=91.84474, y=140.6144),\n",
       " Point(x=91.842476, y=140.62057),\n",
       " Point(x=91.88772, y=140.63353),\n",
       " Point(x=91.872215, y=140.6453),\n",
       " Point(x=91.89212, y=140.63513),\n",
       " Point(x=91.93069, y=140.6722),\n",
       " Point(x=91.93024, y=140.66818),\n",
       " Point(x=91.95031, y=140.70383),\n",
       " Point(x=91.98174, y=140.73503),\n",
       " Point(x=91.94809, y=140.7664),\n",
       " Point(x=91.96482, y=140.81148),\n",
       " Point(x=91.95386, y=140.807),\n",
       " Point(x=91.96438, y=140.80797),\n",
       " Point(x=91.93611, y=140.78441),\n",
       " Point(x=91.954346, y=140.82094),\n",
       " Point(x=91.97051, y=140.82779),\n",
       " Point(x=91.953125, y=140.81253),\n",
       " Point(x=91.97637, y=140.84192),\n",
       " Point(x=91.99758, y=140.88383),\n",
       " Point(x=91.97769, y=140.89691),\n",
       " Point(x=91.971306, y=140.90782),\n",
       " Point(x=91.97858, y=140.90483),\n",
       " Point(x=91.985344, y=140.90567),\n",
       " Point(x=91.99914, y=140.9118),\n",
       " Point(x=92.01874, y=140.92235),\n",
       " Point(x=92.07367, y=141.00922),\n",
       " Point(x=92.08745, y=141.0126),\n",
       " Point(x=92.08817, y=141.05237),\n",
       " Point(x=92.10067, y=141.07607),\n",
       " Point(x=92.10155, y=141.09195),\n",
       " Point(x=92.104126, y=141.09145),\n",
       " Point(x=92.093094, y=141.10051),\n",
       " Point(x=92.12421, y=141.14056),\n",
       " Point(x=92.1339, y=141.14964),\n",
       " Point(x=92.144485, y=141.15823),\n",
       " Point(x=92.168106, y=141.15952),\n",
       " Point(x=92.13685, y=141.20172),\n",
       " Point(x=92.155716, y=141.18762),\n",
       " Point(x=92.16104, y=141.2182),\n",
       " Point(x=92.172485, y=141.21402),\n",
       " Point(x=92.17366, y=141.21196),\n",
       " Point(x=92.19973, y=141.229),\n",
       " Point(x=92.23596, y=141.23024),\n",
       " Point(x=92.24075, y=141.2384),\n",
       " Point(x=92.23784, y=141.26529),\n",
       " Point(x=92.27654, y=141.29355),\n",
       " Point(x=92.26932, y=141.29578),\n",
       " Point(x=92.25961, y=141.29814),\n",
       " Point(x=92.25821, y=141.32965),\n",
       " Point(x=92.23544, y=141.28152),\n",
       " ...]"
      ]
     },
     "execution_count": 164,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tx_pred_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = models.load_model(MODEL_PATH + str(1) + '.h5', \n",
    "                               custom_objects={ 'location_mae2': location_mae2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.metrics_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 78.   , 199.   ,  -6.78 ,   2.   ,   9.   , 117.   ,  -9.169,\n",
       "        78.   , 199.   ,  -6.78 ,     nan,     nan,     nan,     nan,\n",
       "           nan,     nan,     nan,     nan,     nan])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_reg[0][1600:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4096, 8192]\n",
      "[4.649, 3.681]\n",
      "[2.898, 2.232]\n",
      "[]\n",
      "[]\n",
      "[1, 1]\n"
     ]
    }
   ],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "# print(best_lambda)\n",
    "print(average_diff_power_conserve)\n",
    "print(fp_mean_power_conserve)\n",
    "print(best_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-9dec59e8bf11>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_cnns\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_cnns[0][0].history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = all_cnns[0][4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number_samples: 8192 , New samples: 8192\n",
      "Validation size: 2704 , starts: 8192 , ends: 10895\n",
      "\n",
      "Epoch 00061: val_mae improved from inf to 4.03655, saving model to ML/data/pictures_100_100/splat/pu_circle_su_circle_10/raw_power_min_max_norm/color/log_5/pus/models/8192/best_model_lambda_0new.h5\n",
      "\n",
      "Epoch 00062: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00063: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00064: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00065: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00066: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00067: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00068: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00069: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00070: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00071: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00072: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00073: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00074: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00075: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00076: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00077: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00078: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00079: val_mae did not improve from 4.03655\n",
      "\n",
      "Epoch 00080: val_mae did not improve from 4.03655\n",
      "Train Error(all epochs): 2.7838029861450195 \n",
      " [3.146, 3.051, 3.077, 3.16, 3.025, 3.048, 3.075, 3.047, 2.933, 2.916, 2.954, 2.952, 2.843, 2.839, 2.855, 2.931, 2.881, 2.853, 2.784, 2.87]\n",
      "Train FP Error(all epochs): 1.395856499671936 \n",
      " [1.565, 1.509, 1.525, 1.594, 1.505, 1.514, 1.543, 1.499, 1.472, 1.458, 1.471, 1.466, 1.421, 1.429, 1.414, 1.447, 1.427, 1.423, 1.396, 1.423]\n",
      "Val Error(all epochs): 4.036548614501953 \n",
      " [4.037, 5.122, 4.407, 4.095, 5.374, 4.484, 4.181, 4.058, 5.237, 6.0, 4.326, 4.051, 4.319, 4.129, 4.174, 4.58, 4.328, 4.15, 4.372, 4.068]\n",
      "Val FP Error(all epochs): 0.6289845108985901 \n",
      " [1.809, 0.989, 1.929, 2.832, 0.842, 1.47, 1.915, 2.469, 0.917, 0.629, 1.431, 2.745, 1.351, 1.903, 1.793, 1.255, 1.595, 1.794, 1.674, 2.165]\n"
     ]
    }
   ],
   "source": [
    "if True:\n",
    "    checkpointers = ModelCheckpoint(filepath=MODEL_PATH + str(0)+ 'new.h5',\n",
    "                                         verbose=1, save_best_only=True, \n",
    "                                         monitor=hyper_metric,\n",
    "                                         mode=mode)\n",
    "    number_start = time.time()\n",
    "    train_generator = DataBatchGenerator(dataset=data_reg[prev_sample:number_sample], batch_size=mini_batch,\n",
    "                                         start_idx=prev_sample, number_image_channels=number_image_channels,\n",
    "                                         max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "    \n",
    "\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_generator = DataBatchGenerator(dataset=data_reg[number_sample:number_sample+val_size], \n",
    "                                       batch_size=mini_batch,\n",
    "                                       start_idx=number_sample,\n",
    "                                       number_image_channels=number_image_channels,\n",
    "                                       max_x=max_x, max_y=max_y, \n",
    "                                       float_memory_used=float_memory_used)\n",
    "  \n",
    "    print('number_samples:', number_sample, \", New samples:\", number_sample - prev_sample)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "    best_model.fit(train_generator, epochs=80, verbose=0,\n",
    "                   validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "                   workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "                   use_multiprocessing=False, initial_epoch=60)\n",
    "    print(\"Train Error(all epochs):\", min(best_model.history.history['mae']), '\\n',\n",
    "          [round(val, 3) for val in best_model.history.history['mae']])\n",
    "    print(\"Train FP Error(all epochs):\", min(best_model.history.history['fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['fp_mae']])\n",
    "    print(\"Val Error(all epochs):\", min(best_model.history.history['val_mae']), '\\n', \n",
    "          [round(val,3) for val in best_model.history.history['val_mae']])\n",
    "    print(\"Val FP Error(all epochs):\", min(best_model.history.history['val_fp_mae']), '\\n',\n",
    "          [round(val,3) for val in best_model.history.history['val_fp_mae']])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test starts:  10896 , ends:  34263\n",
      "92/92 [==============================] - 22s 234ms/step - loss: 38.6681 - mse: 27.7563 - mae: 4.0863 - fp_mae: 1.7810\n"
     ]
    }
   ],
   "source": [
    "best_best_model = models.load_model(MODEL_PATH + str(0) + 'new.h5', \n",
    "                               custom_objects={ 'loss': custom_loss(fp_penalty_coef, fn_penalty_coef), \n",
    "                                               'fp_mae': fp_mae,\n",
    "                                               'mae':'mae', 'mse':'mse'})\n",
    "test_generator = DataBatchGenerator(dataset=data_reg[number_sample + val_size:], \n",
    "                                            batch_size=mini_batch,\n",
    "                                            start_idx=number_sample + val_size, \n",
    "                                            number_image_channels=number_image_channels,\n",
    "                                            max_x=max_x, max_y=max_y, float_memory_used=float_memory_used)\n",
    "\n",
    "print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "time.sleep(1)\n",
    "test_res = best_best_model.evaluate(test_generator, verbose=1, \n",
    "                                    workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE,\n",
    "                                    use_multiprocessing=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[496.0564270019531, 56.21223449707031, 5.682240009307861, 1.5113146305084229]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [105.50102233886719,\n",
       "  100.78263854980469,\n",
       "  99.25178527832031,\n",
       "  104.09859466552734,\n",
       "  104.50546264648438],\n",
       " 'mse': [44.08177185058594,\n",
       "  41.80543518066406,\n",
       "  41.379180908203125,\n",
       "  44.375244140625,\n",
       "  44.21040725708008],\n",
       " 'mae': [5.269191265106201,\n",
       "  5.1021318435668945,\n",
       "  5.071562767028809,\n",
       "  5.278663635253906,\n",
       "  5.268550872802734],\n",
       " 'fp_mae': [0.19422784447669983,\n",
       "  0.1903233528137207,\n",
       "  0.19018331170082092,\n",
       "  0.2034783661365509,\n",
       "  0.19720497727394104],\n",
       " 'val_loss': [396.6565246582031,\n",
       "  401.5822448730469,\n",
       "  435.2545166015625,\n",
       "  567.9744262695312,\n",
       "  345.01153564453125],\n",
       " 'val_mse': [120.43984985351562,\n",
       "  76.076171875,\n",
       "  161.7263946533203,\n",
       "  71.23866271972656,\n",
       "  77.79705047607422],\n",
       " 'val_mae': [9.087770462036133,\n",
       "  6.748907089233398,\n",
       "  10.751054763793945,\n",
       "  6.494345188140869,\n",
       "  6.804019927978516],\n",
       " 'val_fp_mae': [0.6868877410888672,\n",
       "  0.9997519850730896,\n",
       "  0.640586793422699,\n",
       "  1.5676021575927734,\n",
       "  0.9557649493217468]}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fac19dcf810>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.fit(train_generator, epochs=15, verbose=0,\n",
    "               validation_data=val_generator, shuffle=True, callbacks=[checkpointers], \n",
    "               workers=WORKERS, max_queue_size=MAX_QUEUE_SIZE, \n",
    "               use_multiprocessing=False, initial_epoch=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_min_mae = [8.27781, 8.23545, 8.20838]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power = [8.166, 7.844, 7.592]\n",
    "fp_mean_power = [4.56, 4.42, 4.37]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN: support batching\n",
    "TEST = True\n",
    "mini_batch, epochs = 16, 30\n",
    "batch_size = (batch_size // mini_batch) * mini_batch\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10]  #, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    number_start = time.time()\n",
    "    current_sample = number_sample - prev_sample\n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "#     val_samples = [batch_size] * (val_size//batch_size) + ([val_size%batch_size] if \n",
    "#                                                                val_size%batch_size else [])\n",
    "    \n",
    "    print('number_samples:', number_sample)\n",
    "    print(\"Train batches:\", train_samples)\n",
    "    for i, train_sample in enumerate(train_samples):\n",
    "        print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "                      \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "    print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", number_sample + val_size - 1)\n",
    "#     print(\"Validation Batches:\", val_samples)\n",
    "#     for i, val_sample in enumerate(val_samples):\n",
    "#         print(\"Validation batch#:\", i, \", batch size:\", val_sample, \", starts:\", number_sample + i * batch_size,\n",
    "#                       \", ends:\", number_sample + i * batch_size + val_sample - 1)\n",
    "        \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        lambda_start = time.time()\n",
    "        \n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "#             if lamb_idx == 0:\n",
    "#                 print(\"Train batch#:\", i, \", batch size:\", train_sample, \", starts:\", prev_sample + i * batch_size,\n",
    "#                       \", ends:\", prev_sample + i * batch_size + train_sample - 1)\n",
    "            x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "            y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                x_train[(image_num - prev_sample) % batch_size] = read_image(image_num)\n",
    "                y_train[(image_num - prev_sample) % batch_size] = np.asarray(data_reg[image_num][-1], \n",
    "                                                                             dtype=float_memory_used)\n",
    "            cnns[lamb_idx].fit(x_train, y_train, epochs=epochs, verbose=2, batch_size=mini_batch,\n",
    "                               validation_split=0.2, \n",
    "                               shuffle=True)\n",
    "            del x_train, y_train\n",
    "#         if lamb_idx == 0:\n",
    "#             print(\"Validation size:\", val_size, \", starts:\", number_sample, \", ends:\", \n",
    "#                   number_sample + val_size - 1)\n",
    "        print(\"\\nLambda:\", lamb)\n",
    "        print(\"Train Error(all epochs): \", cnns[lamb_idx].history.history['mae'])\n",
    "        \n",
    "        # validating\n",
    "        val_mae, val_fp_mae = 0.0, 0.0\n",
    "#         for i, val_sample in enumerate(val_samples):\n",
    "#             x_val = np.empty((val_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "#             for image_num in range(val_sample):\n",
    "#                 x_val[image_num] = read_image(image_num + number_sample + i * batch_size)\n",
    "#             yp_val = cnns[lamb_idx].predict(x_val)\n",
    "        for image_num in range(val_size):\n",
    "            val_y = data_reg[image_num + number_sample][-1]\n",
    "            image = read_image(image_num + number_sample)\n",
    "            val_yp = cnns[lamb_idx].predict(image)[0][0]\n",
    "#             for image_num in range(val_sample):\n",
    "#                 val_yp = yp_val[image_num][0]\n",
    "#                 val_y = data_reg[image_num + number_sample + i * batch_size][-1]\n",
    "            val_mae += abs(val_y - val_yp)\n",
    "            if val_yp > val_y:\n",
    "                val_fp_mae += abs(val_yp - val_y)\n",
    "        val_mae /= val_size\n",
    "        val_fp_mae /= val_size\n",
    "        print(\"Val Error:\", round(val_mae, 3), \", Time:\", str(datetime.timedelta(seconds=int(time.time() - lambda_start))))\n",
    "        if val_mae < min_error:\n",
    "            min_error = val_mae\n",
    "            best_model = cnns[lamb_idx]\n",
    "            best_lam = lamb\n",
    "            best_lam_idx = lamb_idx\n",
    "    print(\"\\nTrainig set size:\", number_sample, \", Time:\", str(datetime.timedelta(seconds=int(time.time() - number_start)))\n",
    "          ,\", best_lambda:\", best_lam, \", min_error:\", round(min_error, 3))\n",
    "    \n",
    "    \n",
    "    if TEST:\n",
    "        # evaluating test images\n",
    "        sum_mae, sum_fp_mae = 0, 0\n",
    "        test_size = 0\n",
    "\n",
    "        y_test_p = np.empty((data_reg.shape[0] - (number_sample + val_size)), dtype=float_memory_used)\n",
    "    #     test_size = data_reg.shape[0] - (number_sample + val_size)\n",
    "    #     test_samples = [batch_size] * (test_size//batch_size) + ([test_size%batch_size] if \n",
    "    #                                                              test_size%batch_size else [])\n",
    "        print(\"Test starts: \", number_sample + val_size, \", ends: \", data_reg.shape[0] - 1)\n",
    "        time.sleep(1)\n",
    "    #     for i, test_sample in tqdm.tqdm(enumerate(test_samples)):\n",
    "    #         x_test = np.empty((test_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             x_test[image_num] = read_image(number_sample + val_size + i * batch_size)\n",
    "    #         yp_test = cnns[best_lam_idx].predict(x_test)\n",
    "    #         for image_num in range(test_sample):\n",
    "    #             test_y = data_reg[number_sample + val_size + i * batch_size][-1]\n",
    "    #             test_yp = yp_test[image_num][0]\n",
    "    #             sum_mae += abs(test_yp - test_y)\n",
    "    #             if test_yp > test_y:\n",
    "    #                 sum_fp_mae += abs(test_yp - test_y)\n",
    "\n",
    "        for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "            test_size += 1\n",
    "            test_image = read_image(test_num)\n",
    "            test_y = data_reg[test_num][-1]\n",
    "            test_yp = best_model.predict(test_image)[0][0]\n",
    "            y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "            sum_mae += abs(test_yp - test_y)\n",
    "            if test_yp > test_y:\n",
    "                sum_fp_mae += abs(test_yp - test_y)\n",
    "        fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "        average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "        print('average_error: ', average_diff_power[-1], ', fp_average_error: ', \n",
    "              fp_mean_power[-1])\n",
    "        print(\"\\n\\n\")\n",
    "        var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                     dtime + \".dat\", \"wb\") # file for saving results\n",
    "        pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "        var_f.close()\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnns[1].weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# CNN: support batching\n",
    "prev_sample = 0\n",
    "# number_samples = [120, 200, 700]\n",
    "lambda_vec = [0, 0.001]  #0.003, 0.01, 0.03, 0.1, 0.3, 1, 3, 10\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "cnns = [cnn_model(10, lamb, 0) for lamb in lambda_vec]\n",
    "for cnn in cnns:\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "for number_sample in number_samples:\n",
    "    current_sample = number_sample - prev_sample\n",
    "    print(\"prev: \", prev_sample, \", now: \", number_sample, \", size\", current_sample) \n",
    "    train_samples = [batch_size] * (current_sample//batch_size) + ([current_sample%batch_size] if \n",
    "                                                                    current_sample%batch_size else [])\n",
    "    print(train_samples)\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb_idx, lamb in enumerate(lambda_vec):\n",
    "        print(\"Lambda:\", lamb)\n",
    "#         cnn = cnn_model(10, lamb, 0)\n",
    "#         cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "        # training on all batches\n",
    "                                    \n",
    "        for i, train_sample in enumerate(train_samples):\n",
    "            for image_num in range(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample):\n",
    "                print(prev_sample + i * batch_size, prev_sample + i * batch_size + train_sample)\n",
    "                print((prev_sample + i * batch_size - prev_sample) % batch_size, \n",
    "                      (prev_sample + i * batch_size + train_sample - prev_sample)% batch_size)\n",
    "                break\n",
    "\n",
    "        \n",
    "        # validating\n",
    "        print(\"validating\")\n",
    "        val_size = math.ceil(number_sample * validation_size)\n",
    "        for image_num in range(val_size):\n",
    "            print(number_sample, val_size + number_sample)\n",
    "            break\n",
    "     \n",
    "    print(\"Test\") \n",
    "    \n",
    "    # evaluating test images\n",
    "\n",
    "    \n",
    "    for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "        print(number_sample + val_size, data_reg.shape[0])\n",
    "        break\n",
    "    prev_sample = number_sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + 'best_cnn_4000samples' + intensity_degradation + '_' + str(slope) + '_' + \n",
    "                 dtime + \".dat\", \"wb\") # file for saving results\n",
    "pickle.dump(best_model, file=var_f)\n",
    "var_f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## use self-training\n",
    "unlabeled_train_samples = [batch_size] * (len(y_test_p)//batch_size) + ([len(y_test_p)%batch_size] if len(y_test_p)%batch_size else [])\n",
    "labeled_train_samples = [batch_size] * (number_sample//batch_size) + ([number_sample%batch_size] if number_sample%batch_size else [])   \n",
    "min_min_error = float('inf')\n",
    "best_best_model, best_best_lam = None, None\n",
    "for lamb in tqdm.tqdm(lambda_vec):\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(10, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "        \n",
    "    # training on all batches\n",
    "    # training on all batches\n",
    "    for i, train_sample in tqdm.tqdm(enumerate(labeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size, i * batch_size + train_sample):\n",
    "            x_train[image_num % batch_size] = read_image(image_num)\n",
    "            y_train[image_num % batch_size] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=6, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "            \n",
    "    for i, train_sample in tqdm.tqdm(enumerate(unlabeled_train_samples)):\n",
    "        x_train = np.empty((train_sample, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "        y_train = np.empty((train_sample), dtype=float_memory_used)\n",
    "        for image_num in range(i * batch_size + number_sample + val_size, i * batch_size + number_sample + val_size + train_sample):\n",
    "            x_train[(image_num-number_sample - val_size) % batch_size] = read_image(image_num)\n",
    "            y_train[(image_num-number_sample - val_size) % batch_size] = np.asarray(y_test_p[image_num-(number_sample + val_size)], dtype=float_memory_used)\n",
    "        cnn.fit(x_train, y_train, epochs=3, verbose=0, batch_size=1, validation_split=0.0)\n",
    "        del x_train, y_train\n",
    "        \n",
    "    # validating\n",
    "    val_size = math.ceil(number_sample * validation_size)\n",
    "    val_mae, val_fp_mae = 0.0, 0.0\n",
    "    for image_num in range(val_size):\n",
    "        val_y = data_reg[image_num + number_sample][-1]\n",
    "        image = read_image(image_num + number_sample)\n",
    "        val_yp = cnn.predict(image)[0][0]\n",
    "        val_mae += abs(val_y - val_yp)\n",
    "        if val_yp > val_y:\n",
    "            val_fp_mae += abs(val_yp - val_y)\n",
    "    val_mae /= val_size\n",
    "    val_fp_mae /= val_size\n",
    "    print(val_mae)\n",
    "    if val_mae < min_min_error:\n",
    "        min_min_error = val_mae\n",
    "        best_best_model = cnn\n",
    "        best_best_lam = lamb\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "    \n",
    "for test_num in tqdm.tqdm(range(number_sample + val_size, data_reg.shape[0])):\n",
    "    test_size += 1\n",
    "    test_image = read_image(test_num)\n",
    "    test_y = data_reg[test_num][-1]\n",
    "    test_yp = best_best_model.predict(test_image)[0][0]\n",
    "#     y_test_p[test_num - (number_sample + val_size)] = test_yp\n",
    "    sum_mae += abs(test_yp - test_y)\n",
    "    if test_yp > test_y:\n",
    "        sum_fp_mae += abs(test_yp - test_y)\n",
    "fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', \n",
    "      fp_mean_power[-1])\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6.285, 6.366, 6.45, 6.454, 6.382, 6.26, 6.49, 6.224, 6.052, 5.87, 4.915, 4.836"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN\n",
    "prev_sample = 0\n",
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1]\n",
    "max_train_samples = math.ceil(number_samples[-1] * (1 + validation_size))\n",
    "x_train = np.empty((max_train_samples, number_image_channels, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train1 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "# x_train2 = np.empty((max_train_samples, 1, max_x, max_y), dtype=float_memory_used)\n",
    "y_train = np.empty((max_train_samples), dtype=float_memory_used)\n",
    "average_diff_power, fp_mean_power = [], []\n",
    "for number_sample in number_samples:\n",
    "    sample = math.ceil(number_sample * (1 + validation_size))\n",
    "    for image_num in range(prev_sample, sample):\n",
    "        prev_sample = sample\n",
    "        if style == \"image_intensity\":\n",
    "            image = plt.imread(image_dir + '/image' + str(image_num)+'.png')\n",
    "            image = np.swapaxes(image, 0, 2)\n",
    "            x_train[image_num] = np.array(image[:number_image_channels], dtype=float_memory_used).reshape(1, number_image_channels, max_x, max_y)\n",
    "            del image\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            x_train[image_num] = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             image = np.load(image_dir + '/image' + str(image_num)+'.npy')\n",
    "#             x_train1[image_num][0] = image[0][0]\n",
    "#             x_train2[image_num][0] = image[0][1]\n",
    "        y_train[image_num] = np.asarray(data_reg[image_num][-1], dtype=float_memory_used)\n",
    "        if image_num + 1 % 100 == 0:\n",
    "            print(image_num)\n",
    "#     cnn = cnn_model(7, 0, 0)\n",
    "#     cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#     cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#             (validation_size + 1))\n",
    "    \n",
    "    min_error = float('inf')\n",
    "    best_model, best_lam = None, None\n",
    "    for lamb in lambda_vec:\n",
    "        print(\"Lambda:\", lamb)\n",
    "        cnn = cnn_model(10, lamb, 0)\n",
    "        cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "#         cnn.fit([x_train1[:sample], x_train2[:sample]], y_train[:sample], epochs=6, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "#                 (validation_size + 1))\n",
    "        cnn.fit(x_train[:sample], y_train[:sample], epochs=6, verbose=0, batch_size=1, validation_split=validation_size/\n",
    "                (validation_size + 1))\n",
    "        if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "            min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "            best_model = cnn\n",
    "            best_lam = lamb\n",
    "    print(\"best_lambda, \", best_lam, \"min_error\", min_error)    \n",
    "    # evaluating test images\n",
    "    sum_mae, sum_fp_mae = 0, 0\n",
    "    test_size = 0\n",
    "#     for test_num in range(max_train_samples, data_reg.shape[0]):\n",
    "    for test_num in range(sample, data_reg.shape[0]):\n",
    "        test_size += 1\n",
    "        if style == \"image_intensity\":\n",
    "            test_image = plt.imread(image_dir + '/image' + str(test_num) + '.png')\n",
    "            test_image = np.swapaxes(test_image, 0, 2)\n",
    "            test_image = np.array(test_image[:number_image_channels]).reshape(1, number_image_channels, max_x, max_y)\n",
    "        elif  style == \"raw_power_min_max_norm\" or style == \"raw_power_zscore_norm\":\n",
    "            test_image = np.load(image_dir + '/image' + str(test_num)+'.npy')\n",
    "        test_y = data_reg[test_num][-1]\n",
    "        test_yp = best_model.predict(test_image)[0][0]\n",
    "        sum_mae += abs(test_yp - test_y)\n",
    "        if test_yp > test_y:\n",
    "            sum_fp_mae += abs(test_yp - test_y)\n",
    "        if test_num % 500 == 0:\n",
    "            print('test: ', test_num)\n",
    "    fp_mean_power.append(round(sum_fp_mae/ test_size, 3))\n",
    "    average_diff_power.append(round(sum_mae / test_size, 3))\n",
    "    print('number_samples: ', number_sample, ', average_error: ', average_diff_power[-1], ' fp_average_error: ', fp_mean_power[-1])\n",
    "    print(\"\\n\")\n",
    "    var_f = open('/'.join(image_dir.split('/')[:-1]) +  '/' + intensity_degradation + '_' + str(slope) + '_' + dtime + \".dat\", \"wb\") # file for saving results\n",
    "    pickle.dump([average_diff_power, fp_mean_power, number_samples], file=var_f)\n",
    "    var_f.close()       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power[8], average_diff_power[9] = average_diff_power[9], average_diff_power[8]\n",
    "# fp_mean_power = fp_mean_power[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee = Input(shape=(number_image_channels, max_x, max_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shapee[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(1, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cnn.history.history['val_mean_absolute_error'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = cnn_model(10, 0, 0)\n",
    "cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_vec = [0, 0.001, 0.003, 0.01, 0.03, 0.1, 0.3, 1, 3]\n",
    "min_error = float('inf')\n",
    "best_model, best_lam = None, None\n",
    "for lamb in lambda_vec:\n",
    "    print(\"Lambda:\", lamb)\n",
    "    cnn = cnn_model(15, lamb, 0)\n",
    "    cnn.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse','mae'])\n",
    "    cnn.fit(x_train[:sample], y_train[:sample], epochs=5, verbose=1, batch_size=1, validation_split=validation_size/\n",
    "            (validation_size + 1))\n",
    "    if cnn.history.history['val_mean_absolute_error'][-1] < min_error:\n",
    "        min_error = cnn.history.history['val_mean_absolute_error'][-1]\n",
    "        best_model = cnn\n",
    "        best_lam = lamb\n",
    "    print(\"\\n\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_lam)\n",
    "print(best_model.history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# just run to dispaly the image. First change return line from create_image\n",
    "aa = np.swapaxes(np.append(np.array(x_train[50]), np.zeros((2,max_x, max_y), dtype=float_memory_used), axis=0), 0, 2)\n",
    "plt.imshow(aa)\n",
    "# plt.imsave('image.png', aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use this cell to read saved variables\n",
    "var_ff = open('ML/data/pictures_1000_1000/log_201912_0705_37.txt', 'rb')\n",
    "[average_diff_power_1, fp_mean_power_1, number_samples_1] = pickle.load(var_ff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_diff_power[-1]*(data_reg.shape[0] - max_train_samples)/(300-sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_fp_mae/200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp_mean_power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.load(image_dir + '/image10.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/'.join(image_dir.split('/')[:-1]) + '/log_5__202006_2714_19.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('/home/shahrokh/projects/research/MLSpectrumAllocation/ML/data/pictures_1000_1000/log/noisy_std_1/' +\n",
    "            'pu_circle_su_circle_30/raw_power_min_max_norm/color/log_4/pus/log_4__202005_0512_10.dat', 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[average_diff_power, fp_mean_power, number_samples, best_lambda, \n",
    " dataset_name, max_dataset_name, average_power_conserve, \n",
    " fp_mean_power_conserve] = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(number_samples)\n",
    "print(average_diff_power)\n",
    "print(fp_mean_power)\n",
    "print(best_lambda)\n",
    "print(average_power_conserve)\n",
    "print(fp_mean_power_conserve)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(fp1, fp2, fp3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(samples1, samples2, samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(samples3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
